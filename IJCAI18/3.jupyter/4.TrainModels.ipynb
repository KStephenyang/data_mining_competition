{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.导包、导数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.sparse import csc_matrix\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "pd.set_option('display.max_colwidth',1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# 取最后一天的作为验证\n",
    "data = pd.read_pickle('data/round1_train2')\n",
    "feature_train = data[data['f16'] <= 1537718400]\n",
    "feature_train.drop('label',axis = 1,inplace = True)\n",
    "feature_val = data[data['f16'] > 1537718400]\n",
    "feature_val.drop('label',axis = 1,inplace = True)\n",
    "\n",
    "label_train = data[data['f16'] <= 1537718400]['label']\n",
    "label_val = data[data['f16'] > 1537718400]['label']\n",
    "\n",
    "data = pd.read_pickle('data/round1_test2')\n",
    "feature_test = data\n",
    "\n",
    "data_index = pd.DataFrame(np.genfromtxt('data/round1_test.txt',dtype = np.str, delimiter=' ',skip_header=1))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.训练SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.957631066\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm,datasets\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "#调用SVC()\n",
    "# 参数见：http://blog.csdn.net/xiaodongxiexie/article/details/70667101\n",
    "clf = svm.SVC(\n",
    "            C=0.001, \n",
    "            kernel='rbf', \n",
    "            degree=3, \n",
    "            gamma='auto', \n",
    "            coef0=0.0, \n",
    "            probability=False, \n",
    "            shrinking=True, \n",
    "            tol=0.001, \n",
    "            cache_size=200, \n",
    "            class_weight='balanced', \n",
    "            verbose=True, \n",
    "            max_iter=1000, \n",
    "            decision_function_shape='ovr', \n",
    "            random_state=None\n",
    ")\n",
    "#载入鸢尾花数据集\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "#fit()训练\n",
    "clf.fit(feature_train,label_train)\n",
    "\n",
    "#predict()预测\n",
    "label_val_svm = clf.predict(feature_val)\n",
    "print(log_loss(label_val,label_val_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.训练逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1711058309\n",
      "1.1711058309\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "feature_train_now = StandardScaler().fit_transform(feature_train)\n",
    "\n",
    "\n",
    "clf_l1_LR= LogisticRegression(\n",
    "#    tol = 0.0001,C = 0.00005, penalty = 'l1',max_iter = 500,class_weight = 'balanced',verbose = 2\n",
    ") \n",
    "clf_l1_LR.fit(feature_train_now,label_train)\n",
    "label_val_lr = clf_l1_LR.predict(feature_val)  \n",
    "print(log_loss(label_val,label_val_lr))\n",
    "\n",
    "\n",
    "clf_l2_LR= LogisticRegression(\n",
    "#    tol = 0.0001,C = 0.0001, penalty = 'l2',max_iter = 500,class_weight = 'balanced',verbose = 2\n",
    ") \n",
    "clf_l2_LR.fit(feature_train_now,label_train)\n",
    "label_val_lr = clf_l2_LR.predict(feature_val)\n",
    "print(log_loss(label_val,label_val_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.训练xgboost\n",
    "这里要重点讲一下 Xgboost 的调参。通常认为对它性能影响较大的参数有：\n",
    "\n",
    "- eta：每次迭代完成后更新权重时的步长。越小训练越慢。\n",
    "- num_round：总共迭代的次数。\n",
    "- subsample：训练每棵树时用来训练的数据占全部的比例。用于防止 Overfitting。\n",
    "- colsample_bytree：训练每棵树时用来训练的特征的比例，类似 RandomForestClassifier 的 max_features。\n",
    "- max_depth：每棵树的最大深度限制。与 Random Forest 不同，Gradient Boosting 如果不对深度加以限制，最终是会 Overfit 的。\n",
    "- early_stopping_rounds：用于控制在 Out Of Sample 的验证集上连续多少个迭代的分数都没有提高后就提前终止训练。用于防止 Overfitting。\n",
    "\n",
    "一般的调参步骤是：\n",
    "\n",
    "- 将训练数据的一部分划出来作为验证集。\n",
    "- 先将 eta 设得比较高（比如 0.1），num_round 设为 300 ~ 500。\n",
    "- 用 Grid Search 对其他参数进行搜索\n",
    "- 逐步将 eta 降低，找到最佳值。\n",
    "- 以验证集为 watchlist，用找到的最佳参数组合重新在训练集上训练。注意观察算法的输出，看每次迭代后在验证集上分数的变化情况，从而得到最佳的 early_stopping_rounds。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['f0', 'f1', 'f2:2_22731265849056483', 'f2:2_509660095530134768',\n",
       "       'f2:2_1968056100269760729', 'f2:2_2011981573061447208',\n",
       "       'f2:2_2436715285093487584', 'f2:2_2642175453151805566',\n",
       "       'f2:2_3203673979138763595', 'f2:2_4879721024980945592',\n",
       "       'f2:2_5755694407684602296', 'f2:2_5799347067982556520',\n",
       "       'f2:2_7258015885215914736', 'f2:2_8277336076276184272',\n",
       "       'f2:2_8710739180200009128', 'f2:3_-1', 'f2:3_6233669177166538628',\n",
       "       'f2:3_8868887661186419229', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10',\n",
       "       'f11:woman', 'f11:man', 'f11:family', 'f11:other', 'f12', 'f13:2002.0',\n",
       "       'f13:2003.0', 'f13:2004.0', 'f13:2005.0', 'f14', 'f15', 'f16',\n",
       "       'f16:day', 'f16:hour', 'f16:minute', 'f16:second', 'f16:dayofweek',\n",
       "       'f16:dayofyear', 'f17', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25',\n",
       "       'f19', 'f1', 'f4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = xgb.DMatrix(feature_train,label = label_train)\n",
    "val = xgb.DMatrix(feature_val,label = label_val)\n",
    "test = xgb.DMatrix(feature_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "\n",
    "\n",
    "param = {\n",
    "    # 1.General Parameters\n",
    "    'booster': 'gbtree', # 提升计算的模型，可以是：gbtree, gblinear or dart\n",
    "    'silent': 0,  # 0为打印信息，1为缄默\n",
    "    # 'nthread': 4, # XGBoost运行时的线程数\n",
    "    # 'num_pbuffer': # 预测缓冲区的大小，通常设置为训练实例的数量。缓冲区用于保存最后一次提升步骤的预测结果。\n",
    "    # 'num_feature': # 特征值数量\n",
    "    \n",
    "    # 2.Booster Parameters\n",
    "    'eta': 0.05,  # 学习率，默认0.3，取值范围为：[0,1]，典型值为0.01-0.2，越小越保守\n",
    "    'gamma': 0.1,  # 节点分裂所需的最小损失函数下降值，和损失函数息息相关。默认0，典型值0.1、0.2，越大越保守，\n",
    "    'max_depth': 5,  # 树的最大深度，默认6，典型值为3-10，越大越易过拟合。\n",
    "    'min_child_weight':1, # 这个参数非常影响结果，最小叶子节点样本权重和。如果一个叶子节点的样本权重和小于min_child_weight则拆分过程结束。在现行回归模型中，这个参数是指建立每个模型所需要的最小样本数。默认1，越大算法越易欠拟合\n",
    "    'max_delta_step ':0, # 限制每棵树权重改变的最大步长。越大越保守。通常，这个参数不需要设置。但是当各类别的样本十分不平衡时，它对逻辑回归是很有帮助的。\n",
    "    'subsample':0.8, # 用于训练模型的子样本占整个样本集合的比例，行采样，典型值0.5-1，越小越保守。\n",
    "    'colsample_bytree ': 0.8,  # 在建立树时对特征采样的比例，列采样。典型值0.5-1\n",
    "\n",
    "    'lambda':10, # 权重的L2正则化项，默认是1\n",
    "    'alpha':1,# 权重的L1正则化项，默认是1\n",
    "    'lambda_bias': 0,  # 在偏置上的L2正则。缺省值为0（在L1上没有偏置项的正则，因为L1时偏置不重要）\n",
    "    # 'scale_pos_weight':0.5, # 各类样本十分不平衡时，把这个参数设置为一个正数，可以使算法更快收敛。默认是1\n",
    "\n",
    "    # 3.Task Parameters\n",
    "    # 'tree_method': 'approx',\n",
    "    'objective': 'binary:logistic',  # 使用的模型，分类的数目\n",
    "    'eval_metric':'logloss', # 校验数据所需要的评价指标，不同的目标函数将会有缺省的评价指标\n",
    "    'base_score':0.5, # 对于所有样本预测为正样本的全局偏置（初始分数）。如果迭代次数够多，改变这个参数对结果不会有影响。\n",
    "    'seed':0 # 随机数的种子\n",
    "}\n",
    "num_boost_round = 500  # 迭代的次数，弱分类器的数量\n",
    "\n",
    "watchlist = [(train, 'train'),(val, 'eval')]  # 看板，每次迭代都可以在控制台打印出训练集与测试集的损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.648047\teval-logloss:0.647781\n",
      "Multiple eval metrics have been passed: 'eval-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-logloss hasn't improved in 50 rounds.\n",
      "[1]\ttrain-logloss:0.607243\teval-logloss:0.606727\n",
      "[2]\ttrain-logloss:0.570172\teval-logloss:0.56945\n",
      "[3]\ttrain-logloss:0.536356\teval-logloss:0.535411\n",
      "[4]\ttrain-logloss:0.505402\teval-logloss:0.504244\n",
      "[5]\ttrain-logloss:0.476974\teval-logloss:0.475646\n",
      "[6]\ttrain-logloss:0.450839\teval-logloss:0.449325\n",
      "[7]\ttrain-logloss:0.426746\teval-logloss:0.425043\n",
      "[8]\ttrain-logloss:0.404455\teval-logloss:0.402585\n",
      "[9]\ttrain-logloss:0.383832\teval-logloss:0.381817\n",
      "[10]\ttrain-logloss:0.364703\teval-logloss:0.362523\n",
      "[11]\ttrain-logloss:0.346939\teval-logloss:0.344619\n",
      "[12]\ttrain-logloss:0.330436\teval-logloss:0.327957\n",
      "[13]\ttrain-logloss:0.315063\teval-logloss:0.312448\n",
      "[14]\ttrain-logloss:0.300738\teval-logloss:0.297979\n",
      "[15]\ttrain-logloss:0.287372\teval-logloss:0.284478\n",
      "[16]\ttrain-logloss:0.2749\teval-logloss:0.27187\n",
      "[17]\ttrain-logloss:0.263248\teval-logloss:0.260089\n",
      "[18]\ttrain-logloss:0.252354\teval-logloss:0.249058\n",
      "[19]\ttrain-logloss:0.242173\teval-logloss:0.238756\n",
      "[20]\ttrain-logloss:0.232642\teval-logloss:0.229115\n",
      "[21]\ttrain-logloss:0.223702\teval-logloss:0.220052\n",
      "[22]\ttrain-logloss:0.21532\teval-logloss:0.211569\n",
      "[23]\ttrain-logloss:0.207487\teval-logloss:0.203635\n",
      "[24]\ttrain-logloss:0.200137\teval-logloss:0.196172\n",
      "[25]\ttrain-logloss:0.193243\teval-logloss:0.189167\n",
      "[26]\ttrain-logloss:0.186786\teval-logloss:0.182597\n",
      "[27]\ttrain-logloss:0.180726\teval-logloss:0.176429\n",
      "[28]\ttrain-logloss:0.175024\teval-logloss:0.17063\n",
      "[29]\ttrain-logloss:0.169689\teval-logloss:0.165187\n",
      "[30]\ttrain-logloss:0.16468\teval-logloss:0.160081\n",
      "[31]\ttrain-logloss:0.159971\teval-logloss:0.155268\n",
      "[32]\ttrain-logloss:0.15555\teval-logloss:0.150778\n",
      "[33]\ttrain-logloss:0.151398\teval-logloss:0.14653\n",
      "[34]\ttrain-logloss:0.147497\teval-logloss:0.142527\n",
      "[35]\ttrain-logloss:0.143847\teval-logloss:0.138781\n",
      "[36]\ttrain-logloss:0.140411\teval-logloss:0.135239\n",
      "[37]\ttrain-logloss:0.137191\teval-logloss:0.131926\n",
      "[38]\ttrain-logloss:0.134162\teval-logloss:0.128817\n",
      "[39]\ttrain-logloss:0.13131\teval-logloss:0.125878\n",
      "[40]\ttrain-logloss:0.128655\teval-logloss:0.123141\n",
      "[41]\ttrain-logloss:0.126154\teval-logloss:0.12057\n",
      "[42]\ttrain-logloss:0.123807\teval-logloss:0.118149\n",
      "[43]\ttrain-logloss:0.121614\teval-logloss:0.115941\n",
      "[44]\ttrain-logloss:0.119551\teval-logloss:0.113787\n",
      "[45]\ttrain-logloss:0.117615\teval-logloss:0.111765\n",
      "[46]\ttrain-logloss:0.115802\teval-logloss:0.109877\n",
      "[47]\ttrain-logloss:0.114096\teval-logloss:0.108105\n",
      "[48]\ttrain-logloss:0.112505\teval-logloss:0.106453\n",
      "[49]\ttrain-logloss:0.111012\teval-logloss:0.10488\n",
      "[50]\ttrain-logloss:0.109617\teval-logloss:0.103413\n",
      "[51]\ttrain-logloss:0.108302\teval-logloss:0.102036\n",
      "[52]\ttrain-logloss:0.107077\teval-logloss:0.100731\n",
      "[53]\ttrain-logloss:0.105929\teval-logloss:0.099509\n",
      "[54]\ttrain-logloss:0.104857\teval-logloss:0.09844\n",
      "[55]\ttrain-logloss:0.103857\teval-logloss:0.097361\n",
      "[56]\ttrain-logloss:0.102919\teval-logloss:0.096364\n",
      "[57]\ttrain-logloss:0.102044\teval-logloss:0.095468\n",
      "[58]\ttrain-logloss:0.101225\teval-logloss:0.094593\n",
      "[59]\ttrain-logloss:0.100451\teval-logloss:0.093771\n",
      "[60]\ttrain-logloss:0.099725\teval-logloss:0.093\n",
      "[61]\ttrain-logloss:0.099054\teval-logloss:0.092272\n",
      "[62]\ttrain-logloss:0.098426\teval-logloss:0.091596\n",
      "[63]\ttrain-logloss:0.097841\teval-logloss:0.090963\n",
      "[64]\ttrain-logloss:0.097289\teval-logloss:0.090373\n",
      "[65]\ttrain-logloss:0.096781\teval-logloss:0.08984\n",
      "[66]\ttrain-logloss:0.096303\teval-logloss:0.08934\n",
      "[67]\ttrain-logloss:0.095858\teval-logloss:0.088863\n",
      "[68]\ttrain-logloss:0.095437\teval-logloss:0.088419\n",
      "[69]\ttrain-logloss:0.095048\teval-logloss:0.088004\n",
      "[70]\ttrain-logloss:0.094684\teval-logloss:0.087626\n",
      "[71]\ttrain-logloss:0.094348\teval-logloss:0.087274\n",
      "[72]\ttrain-logloss:0.094033\teval-logloss:0.086938\n",
      "[73]\ttrain-logloss:0.093732\teval-logloss:0.086625\n",
      "[74]\ttrain-logloss:0.093457\teval-logloss:0.086313\n",
      "[75]\ttrain-logloss:0.093196\teval-logloss:0.086026\n",
      "[76]\ttrain-logloss:0.092959\teval-logloss:0.085775\n",
      "[77]\ttrain-logloss:0.092729\teval-logloss:0.085549\n",
      "[78]\ttrain-logloss:0.092521\teval-logloss:0.085331\n",
      "[79]\ttrain-logloss:0.092322\teval-logloss:0.085124\n",
      "[80]\ttrain-logloss:0.092134\teval-logloss:0.084934\n",
      "[81]\ttrain-logloss:0.091953\teval-logloss:0.084757\n",
      "[82]\ttrain-logloss:0.091788\teval-logloss:0.084578\n",
      "[83]\ttrain-logloss:0.09163\teval-logloss:0.084423\n",
      "[84]\ttrain-logloss:0.091489\teval-logloss:0.084276\n",
      "[85]\ttrain-logloss:0.091353\teval-logloss:0.08413\n",
      "[86]\ttrain-logloss:0.091222\teval-logloss:0.083998\n",
      "[87]\ttrain-logloss:0.091101\teval-logloss:0.083855\n",
      "[88]\ttrain-logloss:0.090988\teval-logloss:0.083724\n",
      "[89]\ttrain-logloss:0.090878\teval-logloss:0.083622\n",
      "[90]\ttrain-logloss:0.090777\teval-logloss:0.083538\n",
      "[91]\ttrain-logloss:0.090679\teval-logloss:0.083458\n",
      "[92]\ttrain-logloss:0.090588\teval-logloss:0.083378\n",
      "[93]\ttrain-logloss:0.090501\teval-logloss:0.083297\n",
      "[94]\ttrain-logloss:0.090422\teval-logloss:0.083224\n",
      "[95]\ttrain-logloss:0.090345\teval-logloss:0.083152\n",
      "[96]\ttrain-logloss:0.090273\teval-logloss:0.083089\n",
      "[97]\ttrain-logloss:0.090201\teval-logloss:0.083018\n",
      "[98]\ttrain-logloss:0.090138\teval-logloss:0.082953\n",
      "[99]\ttrain-logloss:0.090074\teval-logloss:0.082892\n",
      "[100]\ttrain-logloss:0.090008\teval-logloss:0.082828\n",
      "[101]\ttrain-logloss:0.089945\teval-logloss:0.082787\n",
      "[102]\ttrain-logloss:0.089897\teval-logloss:0.082746\n",
      "[103]\ttrain-logloss:0.08984\teval-logloss:0.082705\n",
      "[104]\ttrain-logloss:0.089786\teval-logloss:0.082666\n",
      "[105]\ttrain-logloss:0.089738\teval-logloss:0.082635\n",
      "[106]\ttrain-logloss:0.089689\teval-logloss:0.082593\n",
      "[107]\ttrain-logloss:0.089646\teval-logloss:0.082561\n",
      "[108]\ttrain-logloss:0.089603\teval-logloss:0.082525\n",
      "[109]\ttrain-logloss:0.089556\teval-logloss:0.082488\n",
      "[110]\ttrain-logloss:0.089514\teval-logloss:0.082459\n",
      "[111]\ttrain-logloss:0.089474\teval-logloss:0.082435\n",
      "[112]\ttrain-logloss:0.089426\teval-logloss:0.082403\n",
      "[113]\ttrain-logloss:0.089387\teval-logloss:0.082355\n",
      "[114]\ttrain-logloss:0.089346\teval-logloss:0.082338\n",
      "[115]\ttrain-logloss:0.089313\teval-logloss:0.082326\n",
      "[116]\ttrain-logloss:0.089279\teval-logloss:0.082296\n",
      "[117]\ttrain-logloss:0.089246\teval-logloss:0.082277\n",
      "[118]\ttrain-logloss:0.089207\teval-logloss:0.082263\n",
      "[119]\ttrain-logloss:0.08918\teval-logloss:0.082246\n",
      "[120]\ttrain-logloss:0.089154\teval-logloss:0.08223\n",
      "[121]\ttrain-logloss:0.08912\teval-logloss:0.082217\n",
      "[122]\ttrain-logloss:0.089088\teval-logloss:0.08219\n",
      "[123]\ttrain-logloss:0.089055\teval-logloss:0.082188\n",
      "[124]\ttrain-logloss:0.089025\teval-logloss:0.082165\n",
      "[125]\ttrain-logloss:0.088994\teval-logloss:0.082163\n",
      "[126]\ttrain-logloss:0.088968\teval-logloss:0.082159\n",
      "[127]\ttrain-logloss:0.088937\teval-logloss:0.082148\n",
      "[128]\ttrain-logloss:0.088913\teval-logloss:0.082134\n",
      "[129]\ttrain-logloss:0.088881\teval-logloss:0.082131\n",
      "[130]\ttrain-logloss:0.088852\teval-logloss:0.082113\n",
      "[131]\ttrain-logloss:0.088823\teval-logloss:0.082107\n",
      "[132]\ttrain-logloss:0.088801\teval-logloss:0.082105\n",
      "[133]\ttrain-logloss:0.088777\teval-logloss:0.082096\n",
      "[134]\ttrain-logloss:0.088748\teval-logloss:0.08209\n",
      "[135]\ttrain-logloss:0.088722\teval-logloss:0.08209\n",
      "[136]\ttrain-logloss:0.088694\teval-logloss:0.082077\n",
      "[137]\ttrain-logloss:0.08867\teval-logloss:0.082066\n",
      "[138]\ttrain-logloss:0.088645\teval-logloss:0.08206\n",
      "[139]\ttrain-logloss:0.088622\teval-logloss:0.082055\n",
      "[140]\ttrain-logloss:0.088598\teval-logloss:0.082047\n",
      "[141]\ttrain-logloss:0.08857\teval-logloss:0.082024\n",
      "[142]\ttrain-logloss:0.08855\teval-logloss:0.082025\n",
      "[143]\ttrain-logloss:0.088523\teval-logloss:0.082016\n",
      "[144]\ttrain-logloss:0.088504\teval-logloss:0.082016\n",
      "[145]\ttrain-logloss:0.088478\teval-logloss:0.082012\n",
      "[146]\ttrain-logloss:0.088461\teval-logloss:0.082008\n",
      "[147]\ttrain-logloss:0.08844\teval-logloss:0.082003\n",
      "[148]\ttrain-logloss:0.088422\teval-logloss:0.081997\n",
      "[149]\ttrain-logloss:0.088406\teval-logloss:0.081995\n",
      "[150]\ttrain-logloss:0.088389\teval-logloss:0.081991\n",
      "[151]\ttrain-logloss:0.088369\teval-logloss:0.081988\n",
      "[152]\ttrain-logloss:0.088344\teval-logloss:0.081987\n",
      "[153]\ttrain-logloss:0.088324\teval-logloss:0.081979\n",
      "[154]\ttrain-logloss:0.088298\teval-logloss:0.081973\n",
      "[155]\ttrain-logloss:0.088274\teval-logloss:0.081982\n",
      "[156]\ttrain-logloss:0.08825\teval-logloss:0.081978\n",
      "[157]\ttrain-logloss:0.08823\teval-logloss:0.081966\n",
      "[158]\ttrain-logloss:0.088213\teval-logloss:0.081966\n",
      "[159]\ttrain-logloss:0.088188\teval-logloss:0.081954\n",
      "[160]\ttrain-logloss:0.088174\teval-logloss:0.081953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[161]\ttrain-logloss:0.08816\teval-logloss:0.081947\n",
      "[162]\ttrain-logloss:0.088135\teval-logloss:0.08195\n",
      "[163]\ttrain-logloss:0.088123\teval-logloss:0.081946\n",
      "[164]\ttrain-logloss:0.088106\teval-logloss:0.081943\n",
      "[165]\ttrain-logloss:0.088085\teval-logloss:0.081942\n",
      "[166]\ttrain-logloss:0.08806\teval-logloss:0.081948\n",
      "[167]\ttrain-logloss:0.08804\teval-logloss:0.081948\n",
      "[168]\ttrain-logloss:0.088017\teval-logloss:0.08195\n",
      "[169]\ttrain-logloss:0.088\teval-logloss:0.081947\n",
      "[170]\ttrain-logloss:0.087985\teval-logloss:0.081942\n",
      "[171]\ttrain-logloss:0.087971\teval-logloss:0.081946\n",
      "[172]\ttrain-logloss:0.087956\teval-logloss:0.081944\n",
      "[173]\ttrain-logloss:0.087935\teval-logloss:0.081954\n",
      "[174]\ttrain-logloss:0.087912\teval-logloss:0.081959\n",
      "[175]\ttrain-logloss:0.087894\teval-logloss:0.081963\n",
      "[176]\ttrain-logloss:0.087878\teval-logloss:0.081965\n",
      "[177]\ttrain-logloss:0.087863\teval-logloss:0.081959\n",
      "[178]\ttrain-logloss:0.087848\teval-logloss:0.081967\n",
      "[179]\ttrain-logloss:0.087836\teval-logloss:0.081961\n",
      "[180]\ttrain-logloss:0.087809\teval-logloss:0.081962\n",
      "[181]\ttrain-logloss:0.087787\teval-logloss:0.08196\n",
      "[182]\ttrain-logloss:0.087769\teval-logloss:0.081963\n",
      "[183]\ttrain-logloss:0.087746\teval-logloss:0.081962\n",
      "[184]\ttrain-logloss:0.087732\teval-logloss:0.081969\n",
      "[185]\ttrain-logloss:0.087703\teval-logloss:0.081976\n",
      "[186]\ttrain-logloss:0.087682\teval-logloss:0.081973\n",
      "[187]\ttrain-logloss:0.087661\teval-logloss:0.081974\n",
      "[188]\ttrain-logloss:0.087641\teval-logloss:0.081971\n",
      "[189]\ttrain-logloss:0.087618\teval-logloss:0.081971\n",
      "[190]\ttrain-logloss:0.087596\teval-logloss:0.081966\n",
      "[191]\ttrain-logloss:0.087583\teval-logloss:0.08196\n",
      "[192]\ttrain-logloss:0.087572\teval-logloss:0.081961\n",
      "[193]\ttrain-logloss:0.087546\teval-logloss:0.081961\n",
      "[194]\ttrain-logloss:0.087533\teval-logloss:0.081963\n",
      "[195]\ttrain-logloss:0.087519\teval-logloss:0.081965\n",
      "[196]\ttrain-logloss:0.087509\teval-logloss:0.08196\n",
      "[197]\ttrain-logloss:0.087496\teval-logloss:0.081962\n",
      "[198]\ttrain-logloss:0.087474\teval-logloss:0.081965\n",
      "[199]\ttrain-logloss:0.087458\teval-logloss:0.081965\n",
      "[200]\ttrain-logloss:0.08744\teval-logloss:0.081968\n",
      "[201]\ttrain-logloss:0.087416\teval-logloss:0.08197\n",
      "[202]\ttrain-logloss:0.087402\teval-logloss:0.081969\n",
      "[203]\ttrain-logloss:0.087376\teval-logloss:0.081969\n",
      "[204]\ttrain-logloss:0.087356\teval-logloss:0.081975\n",
      "[205]\ttrain-logloss:0.087339\teval-logloss:0.081977\n",
      "[206]\ttrain-logloss:0.087322\teval-logloss:0.081974\n",
      "[207]\ttrain-logloss:0.087304\teval-logloss:0.081974\n",
      "[208]\ttrain-logloss:0.087289\teval-logloss:0.081975\n",
      "[209]\ttrain-logloss:0.087272\teval-logloss:0.081974\n",
      "[210]\ttrain-logloss:0.087263\teval-logloss:0.081975\n",
      "[211]\ttrain-logloss:0.08724\teval-logloss:0.081975\n",
      "[212]\ttrain-logloss:0.087228\teval-logloss:0.081974\n",
      "[213]\ttrain-logloss:0.087214\teval-logloss:0.081972\n",
      "[214]\ttrain-logloss:0.0872\teval-logloss:0.081968\n",
      "[215]\ttrain-logloss:0.087183\teval-logloss:0.081967\n",
      "Stopping. Best iteration:\n",
      "[165]\ttrain-logloss:0.088085\teval-logloss:0.081942\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# obj、feval、early_stopping_rounds、evals_result、verbose_eval、xgb_model\n",
    "\n",
    "bst = xgb.train(param, train, num_boost_round, evals=watchlist, obj=None, feval=None, maximize=False,\n",
    "      early_stopping_rounds=50, evals_result=None, verbose_eval=True, xgb_model=None, callbacks=None,learning_rates=None)\n",
    "bst.save_model('data/model_xgboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importance = bst.get_fscore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importance = sorted(importance,key=lambda s: s[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f 2 : 2 _ 7 2 5 8 0 1 5 8 8 5 2 1 5 9 1 4 7 3 6',\n",
       " 'f 2 4',\n",
       " 'f 2 3',\n",
       " 'f 6',\n",
       " 'f 0',\n",
       " 'f 1 5',\n",
       " 'f 4',\n",
       " 'f 1 0',\n",
       " 'f 2 5',\n",
       " 'f 1 9',\n",
       " 'f 2 2',\n",
       " 'f 2 1',\n",
       " 'f 1',\n",
       " 'f 1 6 : h o u r',\n",
       " 'f 1 6',\n",
       " 'f 1 6 : m i n u t e',\n",
       " 'f 1 6 : s e c o n d',\n",
       " 'f 2 0',\n",
       " 'f 2 : 2 _ 2 6 4 2 1 7 5 4 5 3 1 5 1 8 0 5 5 6 6',\n",
       " 'f 1 7',\n",
       " 'f 1 4',\n",
       " 'f 7',\n",
       " 'f 1 2',\n",
       " 'f 5',\n",
       " 'f 2 : 2 _ 1 9 6 8 0 5 6 1 0 0 2 6 9 7 6 0 7 2 9',\n",
       " 'f 9',\n",
       " 'f 2 : 3 _ 8 8 6 8 8 8 7 6 6 1 1 8 6 4 1 9 2 2 9',\n",
       " 'f 2 : 2 _ 2 4 3 6 7 1 5 2 8 5 0 9 3 4 8 7 5 8 4',\n",
       " 'f 8',\n",
       " 'f 1 1 : m a n',\n",
       " 'f 2 : 2 _ 3 2 0 3 6 7 3 9 7 9 1 3 8 7 6 3 5 9 5',\n",
       " 'f 1 1 : f a m i l y',\n",
       " 'f 2 : 2 _ 5 7 9 9 3 4 7 0 6 7 9 8 2 5 5 6 5 2 0',\n",
       " 'f 2 : 2 _ 4 8 7 9 7 2 1 0 2 4 9 8 0 9 4 5 5 9 2',\n",
       " 'f 2 : 2 _ 8 7 1 0 7 3 9 1 8 0 2 0 0 0 0 9 1 2 8',\n",
       " 'f 2 : 2 _ 5 0 9 6 6 0 0 9 5 5 3 0 1 3 4 7 6 8',\n",
       " 'f 1 3 : 2 0 0 4 . 0',\n",
       " 'f 2 : 2 _ 8 2 7 7 3 3 6 0 7 6 2 7 6 1 8 4 2 7 2',\n",
       " 'f 1 3 : 2 0 0 5 . 0',\n",
       " 'f 1 3 : 2 0 0 2 . 0',\n",
       " 'f 2 : 2 _ 2 0 1 1 9 8 1 5 7 3 0 6 1 4 4 7 2 0 8',\n",
       " 'f 1 1 : w o m a n',\n",
       " 'f 2 : 2 _ 5 7 5 5 6 9 4 4 0 7 6 8 4 6 0 2 2 9 6',\n",
       " 'f 1 1 : o t h e r',\n",
       " 'f 1 3 : 2 0 0 3 . 0']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 预测\n",
    "label_test_xgb = bst.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4.保存\n",
    "np.savetxt('data/round1_result_xgb.txt', np.c_[data_index, label_test_xgb], delimiter=',', header='instance_id predicted_score',comments='', fmt='%s %f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.581931454242\n"
     ]
    }
   ],
   "source": [
    "clf = AdaBoostClassifier() #迭代100次  \n",
    "clf.fit(feature_train,label_train)\n",
    "label_val_lr = clf.predict(feature_val)  \n",
    "print(log_loss(label_val,label_val_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.训练Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.584341164082\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier()\n",
    "clf.fit(feature_train , label_train)\n",
    "label_val_rf = clf.predict(feature_val)    \n",
    "print(log_loss(label_val,label_val_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.lightbgm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's binary_logloss: 0.0855802\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.0855103\n",
      "[3]\tvalid_0's binary_logloss: 0.08545\n",
      "[4]\tvalid_0's binary_logloss: 0.0853836\n",
      "[5]\tvalid_0's binary_logloss: 0.0853265\n",
      "[6]\tvalid_0's binary_logloss: 0.0852707\n",
      "[7]\tvalid_0's binary_logloss: 0.085216\n",
      "[8]\tvalid_0's binary_logloss: 0.0851641\n",
      "[9]\tvalid_0's binary_logloss: 0.0851081\n",
      "[10]\tvalid_0's binary_logloss: 0.085059\n",
      "[11]\tvalid_0's binary_logloss: 0.085007\n",
      "[12]\tvalid_0's binary_logloss: 0.0849601\n",
      "[13]\tvalid_0's binary_logloss: 0.0849131\n",
      "[14]\tvalid_0's binary_logloss: 0.0848708\n",
      "[15]\tvalid_0's binary_logloss: 0.0848308\n",
      "[16]\tvalid_0's binary_logloss: 0.0847922\n",
      "[17]\tvalid_0's binary_logloss: 0.084749\n",
      "[18]\tvalid_0's binary_logloss: 0.0847106\n",
      "[19]\tvalid_0's binary_logloss: 0.0846788\n",
      "[20]\tvalid_0's binary_logloss: 0.0846407\n",
      "[21]\tvalid_0's binary_logloss: 0.0846072\n",
      "[22]\tvalid_0's binary_logloss: 0.0845702\n",
      "[23]\tvalid_0's binary_logloss: 0.084538\n",
      "[24]\tvalid_0's binary_logloss: 0.0845012\n",
      "[25]\tvalid_0's binary_logloss: 0.0844666\n",
      "[26]\tvalid_0's binary_logloss: 0.0844361\n",
      "[27]\tvalid_0's binary_logloss: 0.084404\n",
      "[28]\tvalid_0's binary_logloss: 0.0843779\n",
      "[29]\tvalid_0's binary_logloss: 0.0843501\n",
      "[30]\tvalid_0's binary_logloss: 0.0843198\n",
      "[31]\tvalid_0's binary_logloss: 0.0842878\n",
      "[32]\tvalid_0's binary_logloss: 0.0842602\n",
      "[33]\tvalid_0's binary_logloss: 0.0842329\n",
      "[34]\tvalid_0's binary_logloss: 0.0842017\n",
      "[35]\tvalid_0's binary_logloss: 0.0841766\n",
      "[36]\tvalid_0's binary_logloss: 0.0841441\n",
      "[37]\tvalid_0's binary_logloss: 0.0841202\n",
      "[38]\tvalid_0's binary_logloss: 0.0840986\n",
      "[39]\tvalid_0's binary_logloss: 0.0840751\n",
      "[40]\tvalid_0's binary_logloss: 0.084048\n",
      "[41]\tvalid_0's binary_logloss: 0.0840287\n",
      "[42]\tvalid_0's binary_logloss: 0.0840033\n",
      "[43]\tvalid_0's binary_logloss: 0.0839798\n",
      "[44]\tvalid_0's binary_logloss: 0.0839545\n",
      "[45]\tvalid_0's binary_logloss: 0.0839365\n",
      "[46]\tvalid_0's binary_logloss: 0.0839146\n",
      "[47]\tvalid_0's binary_logloss: 0.0838938\n",
      "[48]\tvalid_0's binary_logloss: 0.0838736\n",
      "[49]\tvalid_0's binary_logloss: 0.0838506\n",
      "[50]\tvalid_0's binary_logloss: 0.0838296\n",
      "[51]\tvalid_0's binary_logloss: 0.0838116\n",
      "[52]\tvalid_0's binary_logloss: 0.0837913\n",
      "[53]\tvalid_0's binary_logloss: 0.0837639\n",
      "[54]\tvalid_0's binary_logloss: 0.0837424\n",
      "[55]\tvalid_0's binary_logloss: 0.0837214\n",
      "[56]\tvalid_0's binary_logloss: 0.0837008\n",
      "[57]\tvalid_0's binary_logloss: 0.0836842\n",
      "[58]\tvalid_0's binary_logloss: 0.0836601\n",
      "[59]\tvalid_0's binary_logloss: 0.0836403\n",
      "[60]\tvalid_0's binary_logloss: 0.0836199\n",
      "[61]\tvalid_0's binary_logloss: 0.083603\n",
      "[62]\tvalid_0's binary_logloss: 0.0835817\n",
      "[63]\tvalid_0's binary_logloss: 0.0835614\n",
      "[64]\tvalid_0's binary_logloss: 0.0835458\n",
      "[65]\tvalid_0's binary_logloss: 0.0835279\n",
      "[66]\tvalid_0's binary_logloss: 0.0835109\n",
      "[67]\tvalid_0's binary_logloss: 0.0834911\n",
      "[68]\tvalid_0's binary_logloss: 0.0834771\n",
      "[69]\tvalid_0's binary_logloss: 0.0834626\n",
      "[70]\tvalid_0's binary_logloss: 0.0834459\n",
      "[71]\tvalid_0's binary_logloss: 0.0834297\n",
      "[72]\tvalid_0's binary_logloss: 0.0834157\n",
      "[73]\tvalid_0's binary_logloss: 0.0833989\n",
      "[74]\tvalid_0's binary_logloss: 0.0833864\n",
      "[75]\tvalid_0's binary_logloss: 0.0833695\n",
      "[76]\tvalid_0's binary_logloss: 0.0833549\n",
      "[77]\tvalid_0's binary_logloss: 0.0833388\n",
      "[78]\tvalid_0's binary_logloss: 0.083317\n",
      "[79]\tvalid_0's binary_logloss: 0.0833055\n",
      "[80]\tvalid_0's binary_logloss: 0.0832898\n",
      "[81]\tvalid_0's binary_logloss: 0.0832685\n",
      "[82]\tvalid_0's binary_logloss: 0.0832543\n",
      "[83]\tvalid_0's binary_logloss: 0.0832359\n",
      "[84]\tvalid_0's binary_logloss: 0.0832169\n",
      "[85]\tvalid_0's binary_logloss: 0.0831991\n",
      "[86]\tvalid_0's binary_logloss: 0.0831795\n",
      "[87]\tvalid_0's binary_logloss: 0.0831664\n",
      "[88]\tvalid_0's binary_logloss: 0.083154\n",
      "[89]\tvalid_0's binary_logloss: 0.0831447\n",
      "[90]\tvalid_0's binary_logloss: 0.0831356\n",
      "[91]\tvalid_0's binary_logloss: 0.0831238\n",
      "[92]\tvalid_0's binary_logloss: 0.0831107\n",
      "[93]\tvalid_0's binary_logloss: 0.0830994\n",
      "[94]\tvalid_0's binary_logloss: 0.0830931\n",
      "[95]\tvalid_0's binary_logloss: 0.0830787\n",
      "[96]\tvalid_0's binary_logloss: 0.0830686\n",
      "[97]\tvalid_0's binary_logloss: 0.0830599\n",
      "[98]\tvalid_0's binary_logloss: 0.0830454\n",
      "[99]\tvalid_0's binary_logloss: 0.0830382\n",
      "[100]\tvalid_0's binary_logloss: 0.0830286\n",
      "[101]\tvalid_0's binary_logloss: 0.0830192\n",
      "[102]\tvalid_0's binary_logloss: 0.0830057\n",
      "[103]\tvalid_0's binary_logloss: 0.0829975\n",
      "[104]\tvalid_0's binary_logloss: 0.082991\n",
      "[105]\tvalid_0's binary_logloss: 0.08298\n",
      "[106]\tvalid_0's binary_logloss: 0.0829732\n",
      "[107]\tvalid_0's binary_logloss: 0.0829641\n",
      "[108]\tvalid_0's binary_logloss: 0.0829561\n",
      "[109]\tvalid_0's binary_logloss: 0.0829487\n",
      "[110]\tvalid_0's binary_logloss: 0.0829436\n",
      "[111]\tvalid_0's binary_logloss: 0.0829396\n",
      "[112]\tvalid_0's binary_logloss: 0.0829332\n",
      "[113]\tvalid_0's binary_logloss: 0.0829253\n",
      "[114]\tvalid_0's binary_logloss: 0.0829132\n",
      "[115]\tvalid_0's binary_logloss: 0.082908\n",
      "[116]\tvalid_0's binary_logloss: 0.0828988\n",
      "[117]\tvalid_0's binary_logloss: 0.0828909\n",
      "[118]\tvalid_0's binary_logloss: 0.0828843\n",
      "[119]\tvalid_0's binary_logloss: 0.0828782\n",
      "[120]\tvalid_0's binary_logloss: 0.0828729\n",
      "[121]\tvalid_0's binary_logloss: 0.0828668\n",
      "[122]\tvalid_0's binary_logloss: 0.0828587\n",
      "[123]\tvalid_0's binary_logloss: 0.0828525\n",
      "[124]\tvalid_0's binary_logloss: 0.0828472\n",
      "[125]\tvalid_0's binary_logloss: 0.0828388\n",
      "[126]\tvalid_0's binary_logloss: 0.082828\n",
      "[127]\tvalid_0's binary_logloss: 0.0828224\n",
      "[128]\tvalid_0's binary_logloss: 0.082812\n",
      "[129]\tvalid_0's binary_logloss: 0.0828079\n",
      "[130]\tvalid_0's binary_logloss: 0.0827938\n",
      "[131]\tvalid_0's binary_logloss: 0.0827854\n",
      "[132]\tvalid_0's binary_logloss: 0.0827792\n",
      "[133]\tvalid_0's binary_logloss: 0.0827716\n",
      "[134]\tvalid_0's binary_logloss: 0.0827597\n",
      "[135]\tvalid_0's binary_logloss: 0.0827524\n",
      "[136]\tvalid_0's binary_logloss: 0.082743\n",
      "[137]\tvalid_0's binary_logloss: 0.0827363\n",
      "[138]\tvalid_0's binary_logloss: 0.0827304\n",
      "[139]\tvalid_0's binary_logloss: 0.0827241\n",
      "[140]\tvalid_0's binary_logloss: 0.0827163\n",
      "[141]\tvalid_0's binary_logloss: 0.0827056\n",
      "[142]\tvalid_0's binary_logloss: 0.0826972\n",
      "[143]\tvalid_0's binary_logloss: 0.0826889\n",
      "[144]\tvalid_0's binary_logloss: 0.0826813\n",
      "[145]\tvalid_0's binary_logloss: 0.0826708\n",
      "[146]\tvalid_0's binary_logloss: 0.0826661\n",
      "[147]\tvalid_0's binary_logloss: 0.0826595\n",
      "[148]\tvalid_0's binary_logloss: 0.0826514\n",
      "[149]\tvalid_0's binary_logloss: 0.0826461\n",
      "[150]\tvalid_0's binary_logloss: 0.0826424\n",
      "[151]\tvalid_0's binary_logloss: 0.0826324\n",
      "[152]\tvalid_0's binary_logloss: 0.0826276\n",
      "[153]\tvalid_0's binary_logloss: 0.0826236\n",
      "[154]\tvalid_0's binary_logloss: 0.0826171\n",
      "[155]\tvalid_0's binary_logloss: 0.0826091\n",
      "[156]\tvalid_0's binary_logloss: 0.0826073\n",
      "[157]\tvalid_0's binary_logloss: 0.0826027\n",
      "[158]\tvalid_0's binary_logloss: 0.082598\n",
      "[159]\tvalid_0's binary_logloss: 0.0825911\n",
      "[160]\tvalid_0's binary_logloss: 0.082583\n",
      "[161]\tvalid_0's binary_logloss: 0.0825801\n",
      "[162]\tvalid_0's binary_logloss: 0.0825725\n",
      "[163]\tvalid_0's binary_logloss: 0.0825676\n",
      "[164]\tvalid_0's binary_logloss: 0.082563\n",
      "[165]\tvalid_0's binary_logloss: 0.0825571\n",
      "[166]\tvalid_0's binary_logloss: 0.0825459\n",
      "[167]\tvalid_0's binary_logloss: 0.0825442\n",
      "[168]\tvalid_0's binary_logloss: 0.0825396\n",
      "[169]\tvalid_0's binary_logloss: 0.082534\n",
      "[170]\tvalid_0's binary_logloss: 0.0825278\n",
      "[171]\tvalid_0's binary_logloss: 0.082521\n",
      "[172]\tvalid_0's binary_logloss: 0.0825173\n",
      "[173]\tvalid_0's binary_logloss: 0.0825147\n",
      "[174]\tvalid_0's binary_logloss: 0.0825119\n",
      "[175]\tvalid_0's binary_logloss: 0.082509\n",
      "[176]\tvalid_0's binary_logloss: 0.0825034\n",
      "[177]\tvalid_0's binary_logloss: 0.0825009\n",
      "[178]\tvalid_0's binary_logloss: 0.0824948\n",
      "[179]\tvalid_0's binary_logloss: 0.0824928\n",
      "[180]\tvalid_0's binary_logloss: 0.082488\n",
      "[181]\tvalid_0's binary_logloss: 0.0824872\n",
      "[182]\tvalid_0's binary_logloss: 0.0824839\n",
      "[183]\tvalid_0's binary_logloss: 0.0824812\n",
      "[184]\tvalid_0's binary_logloss: 0.0824798\n",
      "[185]\tvalid_0's binary_logloss: 0.0824755\n",
      "[186]\tvalid_0's binary_logloss: 0.0824724\n",
      "[187]\tvalid_0's binary_logloss: 0.0824665\n",
      "[188]\tvalid_0's binary_logloss: 0.0824581\n",
      "[189]\tvalid_0's binary_logloss: 0.0824583\n",
      "[190]\tvalid_0's binary_logloss: 0.0824532\n",
      "[191]\tvalid_0's binary_logloss: 0.0824512\n",
      "[192]\tvalid_0's binary_logloss: 0.0824506\n",
      "[193]\tvalid_0's binary_logloss: 0.0824461\n",
      "[194]\tvalid_0's binary_logloss: 0.082435\n",
      "[195]\tvalid_0's binary_logloss: 0.0824277\n",
      "[196]\tvalid_0's binary_logloss: 0.0824236\n",
      "[197]\tvalid_0's binary_logloss: 0.0824138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[198]\tvalid_0's binary_logloss: 0.082406\n",
      "[199]\tvalid_0's binary_logloss: 0.0824033\n",
      "[200]\tvalid_0's binary_logloss: 0.0824013\n",
      "[201]\tvalid_0's binary_logloss: 0.0823944\n",
      "[202]\tvalid_0's binary_logloss: 0.0823861\n",
      "[203]\tvalid_0's binary_logloss: 0.0823823\n",
      "[204]\tvalid_0's binary_logloss: 0.0823814\n",
      "[205]\tvalid_0's binary_logloss: 0.0823745\n",
      "[206]\tvalid_0's binary_logloss: 0.0823688\n",
      "[207]\tvalid_0's binary_logloss: 0.0823635\n",
      "[208]\tvalid_0's binary_logloss: 0.0823601\n",
      "[209]\tvalid_0's binary_logloss: 0.0823566\n",
      "[210]\tvalid_0's binary_logloss: 0.0823492\n",
      "[211]\tvalid_0's binary_logloss: 0.0823485\n",
      "[212]\tvalid_0's binary_logloss: 0.0823467\n",
      "[213]\tvalid_0's binary_logloss: 0.0823418\n",
      "[214]\tvalid_0's binary_logloss: 0.0823385\n",
      "[215]\tvalid_0's binary_logloss: 0.0823361\n",
      "[216]\tvalid_0's binary_logloss: 0.0823305\n",
      "[217]\tvalid_0's binary_logloss: 0.0823256\n",
      "[218]\tvalid_0's binary_logloss: 0.0823223\n",
      "[219]\tvalid_0's binary_logloss: 0.0823207\n",
      "[220]\tvalid_0's binary_logloss: 0.0823165\n",
      "[221]\tvalid_0's binary_logloss: 0.0823123\n",
      "[222]\tvalid_0's binary_logloss: 0.0823097\n",
      "[223]\tvalid_0's binary_logloss: 0.0823048\n",
      "[224]\tvalid_0's binary_logloss: 0.0823016\n",
      "[225]\tvalid_0's binary_logloss: 0.0822983\n",
      "[226]\tvalid_0's binary_logloss: 0.0822951\n",
      "[227]\tvalid_0's binary_logloss: 0.0822937\n",
      "[228]\tvalid_0's binary_logloss: 0.0822948\n",
      "[229]\tvalid_0's binary_logloss: 0.082292\n",
      "[230]\tvalid_0's binary_logloss: 0.0822915\n",
      "[231]\tvalid_0's binary_logloss: 0.0822862\n",
      "[232]\tvalid_0's binary_logloss: 0.0822855\n",
      "[233]\tvalid_0's binary_logloss: 0.0822816\n",
      "[234]\tvalid_0's binary_logloss: 0.0822781\n",
      "[235]\tvalid_0's binary_logloss: 0.0822732\n",
      "[236]\tvalid_0's binary_logloss: 0.0822699\n",
      "[237]\tvalid_0's binary_logloss: 0.0822672\n",
      "[238]\tvalid_0's binary_logloss: 0.0822689\n",
      "[239]\tvalid_0's binary_logloss: 0.0822656\n",
      "[240]\tvalid_0's binary_logloss: 0.0822642\n",
      "[241]\tvalid_0's binary_logloss: 0.0822626\n",
      "[242]\tvalid_0's binary_logloss: 0.082263\n",
      "[243]\tvalid_0's binary_logloss: 0.0822609\n",
      "[244]\tvalid_0's binary_logloss: 0.0822659\n",
      "[245]\tvalid_0's binary_logloss: 0.082263\n",
      "[246]\tvalid_0's binary_logloss: 0.0822614\n",
      "[247]\tvalid_0's binary_logloss: 0.0822606\n",
      "[248]\tvalid_0's binary_logloss: 0.082258\n",
      "[249]\tvalid_0's binary_logloss: 0.082256\n",
      "[250]\tvalid_0's binary_logloss: 0.0822559\n",
      "[251]\tvalid_0's binary_logloss: 0.0822608\n",
      "[252]\tvalid_0's binary_logloss: 0.0822586\n",
      "[253]\tvalid_0's binary_logloss: 0.0822573\n",
      "[254]\tvalid_0's binary_logloss: 0.0822516\n",
      "[255]\tvalid_0's binary_logloss: 0.0822534\n",
      "[256]\tvalid_0's binary_logloss: 0.0822526\n",
      "[257]\tvalid_0's binary_logloss: 0.0822509\n",
      "[258]\tvalid_0's binary_logloss: 0.08225\n",
      "[259]\tvalid_0's binary_logloss: 0.0822457\n",
      "[260]\tvalid_0's binary_logloss: 0.0822445\n",
      "[261]\tvalid_0's binary_logloss: 0.082241\n",
      "[262]\tvalid_0's binary_logloss: 0.0822448\n",
      "[263]\tvalid_0's binary_logloss: 0.0822434\n",
      "[264]\tvalid_0's binary_logloss: 0.0822399\n",
      "[265]\tvalid_0's binary_logloss: 0.0822436\n",
      "[266]\tvalid_0's binary_logloss: 0.0822413\n",
      "[267]\tvalid_0's binary_logloss: 0.0822415\n",
      "[268]\tvalid_0's binary_logloss: 0.0822356\n",
      "[269]\tvalid_0's binary_logloss: 0.0822327\n",
      "[270]\tvalid_0's binary_logloss: 0.0822323\n",
      "[271]\tvalid_0's binary_logloss: 0.0822307\n",
      "[272]\tvalid_0's binary_logloss: 0.0822308\n",
      "[273]\tvalid_0's binary_logloss: 0.0822324\n",
      "[274]\tvalid_0's binary_logloss: 0.0822299\n",
      "[275]\tvalid_0's binary_logloss: 0.0822273\n",
      "[276]\tvalid_0's binary_logloss: 0.0822272\n",
      "[277]\tvalid_0's binary_logloss: 0.0822273\n",
      "[278]\tvalid_0's binary_logloss: 0.0822263\n",
      "[279]\tvalid_0's binary_logloss: 0.0822253\n",
      "[280]\tvalid_0's binary_logloss: 0.0822281\n",
      "[281]\tvalid_0's binary_logloss: 0.0822281\n",
      "[282]\tvalid_0's binary_logloss: 0.082233\n",
      "[283]\tvalid_0's binary_logloss: 0.0822317\n",
      "[284]\tvalid_0's binary_logloss: 0.0822295\n",
      "[285]\tvalid_0's binary_logloss: 0.082233\n",
      "[286]\tvalid_0's binary_logloss: 0.0822313\n",
      "[287]\tvalid_0's binary_logloss: 0.0822304\n",
      "[288]\tvalid_0's binary_logloss: 0.0822294\n",
      "[289]\tvalid_0's binary_logloss: 0.0822283\n",
      "[290]\tvalid_0's binary_logloss: 0.0822333\n",
      "[291]\tvalid_0's binary_logloss: 0.0822289\n",
      "[292]\tvalid_0's binary_logloss: 0.0822295\n",
      "[293]\tvalid_0's binary_logloss: 0.0822283\n",
      "[294]\tvalid_0's binary_logloss: 0.0822285\n",
      "[295]\tvalid_0's binary_logloss: 0.0822258\n",
      "[296]\tvalid_0's binary_logloss: 0.0822254\n",
      "[297]\tvalid_0's binary_logloss: 0.0822281\n",
      "[298]\tvalid_0's binary_logloss: 0.0822213\n",
      "[299]\tvalid_0's binary_logloss: 0.0822174\n",
      "[300]\tvalid_0's binary_logloss: 0.0822173\n",
      "[301]\tvalid_0's binary_logloss: 0.0822167\n",
      "[302]\tvalid_0's binary_logloss: 0.0822196\n",
      "[303]\tvalid_0's binary_logloss: 0.082218\n",
      "[304]\tvalid_0's binary_logloss: 0.0822194\n",
      "[305]\tvalid_0's binary_logloss: 0.0822112\n",
      "[306]\tvalid_0's binary_logloss: 0.0822054\n",
      "[307]\tvalid_0's binary_logloss: 0.0821976\n",
      "[308]\tvalid_0's binary_logloss: 0.0821968\n",
      "[309]\tvalid_0's binary_logloss: 0.0821955\n",
      "[310]\tvalid_0's binary_logloss: 0.082195\n",
      "[311]\tvalid_0's binary_logloss: 0.0821913\n",
      "[312]\tvalid_0's binary_logloss: 0.0821891\n",
      "[313]\tvalid_0's binary_logloss: 0.0821818\n",
      "[314]\tvalid_0's binary_logloss: 0.0821765\n",
      "[315]\tvalid_0's binary_logloss: 0.0821772\n",
      "[316]\tvalid_0's binary_logloss: 0.0821755\n",
      "[317]\tvalid_0's binary_logloss: 0.0821738\n",
      "[318]\tvalid_0's binary_logloss: 0.0821704\n",
      "[319]\tvalid_0's binary_logloss: 0.0821702\n",
      "[320]\tvalid_0's binary_logloss: 0.0821696\n",
      "[321]\tvalid_0's binary_logloss: 0.0821682\n",
      "[322]\tvalid_0's binary_logloss: 0.0821614\n",
      "[323]\tvalid_0's binary_logloss: 0.0821628\n",
      "[324]\tvalid_0's binary_logloss: 0.082166\n",
      "[325]\tvalid_0's binary_logloss: 0.0821665\n",
      "[326]\tvalid_0's binary_logloss: 0.0821662\n",
      "[327]\tvalid_0's binary_logloss: 0.0821606\n",
      "[328]\tvalid_0's binary_logloss: 0.0821597\n",
      "[329]\tvalid_0's binary_logloss: 0.0821579\n",
      "[330]\tvalid_0's binary_logloss: 0.082159\n",
      "[331]\tvalid_0's binary_logloss: 0.0821588\n",
      "[332]\tvalid_0's binary_logloss: 0.0821579\n",
      "[333]\tvalid_0's binary_logloss: 0.0821553\n",
      "[334]\tvalid_0's binary_logloss: 0.082154\n",
      "[335]\tvalid_0's binary_logloss: 0.0821497\n",
      "[336]\tvalid_0's binary_logloss: 0.0821469\n",
      "[337]\tvalid_0's binary_logloss: 0.0821459\n",
      "[338]\tvalid_0's binary_logloss: 0.0821469\n",
      "[339]\tvalid_0's binary_logloss: 0.0821461\n",
      "[340]\tvalid_0's binary_logloss: 0.0821474\n",
      "[341]\tvalid_0's binary_logloss: 0.0821487\n",
      "[342]\tvalid_0's binary_logloss: 0.0821511\n",
      "[343]\tvalid_0's binary_logloss: 0.0821526\n",
      "[344]\tvalid_0's binary_logloss: 0.0821549\n",
      "[345]\tvalid_0's binary_logloss: 0.0821544\n",
      "[346]\tvalid_0's binary_logloss: 0.082153\n",
      "[347]\tvalid_0's binary_logloss: 0.0821547\n",
      "[348]\tvalid_0's binary_logloss: 0.0821524\n",
      "[349]\tvalid_0's binary_logloss: 0.0821458\n",
      "[350]\tvalid_0's binary_logloss: 0.0821454\n",
      "[351]\tvalid_0's binary_logloss: 0.0821475\n",
      "[352]\tvalid_0's binary_logloss: 0.0821418\n",
      "[353]\tvalid_0's binary_logloss: 0.0821425\n",
      "[354]\tvalid_0's binary_logloss: 0.0821461\n",
      "[355]\tvalid_0's binary_logloss: 0.0821431\n",
      "[356]\tvalid_0's binary_logloss: 0.0821428\n",
      "[357]\tvalid_0's binary_logloss: 0.082147\n",
      "[358]\tvalid_0's binary_logloss: 0.0821434\n",
      "[359]\tvalid_0's binary_logloss: 0.0821401\n",
      "[360]\tvalid_0's binary_logloss: 0.0821398\n",
      "[361]\tvalid_0's binary_logloss: 0.0821367\n",
      "[362]\tvalid_0's binary_logloss: 0.0821364\n",
      "[363]\tvalid_0's binary_logloss: 0.0821371\n",
      "[364]\tvalid_0's binary_logloss: 0.082139\n",
      "[365]\tvalid_0's binary_logloss: 0.0821393\n",
      "[366]\tvalid_0's binary_logloss: 0.0821409\n",
      "[367]\tvalid_0's binary_logloss: 0.0821433\n",
      "[368]\tvalid_0's binary_logloss: 0.0821467\n",
      "[369]\tvalid_0's binary_logloss: 0.0821459\n",
      "[370]\tvalid_0's binary_logloss: 0.0821456\n",
      "[371]\tvalid_0's binary_logloss: 0.0821438\n",
      "[372]\tvalid_0's binary_logloss: 0.0821407\n",
      "[373]\tvalid_0's binary_logloss: 0.0821411\n",
      "[374]\tvalid_0's binary_logloss: 0.0821396\n",
      "[375]\tvalid_0's binary_logloss: 0.0821401\n",
      "[376]\tvalid_0's binary_logloss: 0.0821405\n",
      "[377]\tvalid_0's binary_logloss: 0.0821417\n",
      "[378]\tvalid_0's binary_logloss: 0.082143\n",
      "[379]\tvalid_0's binary_logloss: 0.0821464\n",
      "[380]\tvalid_0's binary_logloss: 0.0821478\n",
      "[381]\tvalid_0's binary_logloss: 0.082148\n",
      "[382]\tvalid_0's binary_logloss: 0.082146\n",
      "[383]\tvalid_0's binary_logloss: 0.0821468\n",
      "[384]\tvalid_0's binary_logloss: 0.0821471\n",
      "[385]\tvalid_0's binary_logloss: 0.0821334\n",
      "[386]\tvalid_0's binary_logloss: 0.0821336\n",
      "[387]\tvalid_0's binary_logloss: 0.0821366\n",
      "[388]\tvalid_0's binary_logloss: 0.082138\n",
      "[389]\tvalid_0's binary_logloss: 0.0821389\n",
      "[390]\tvalid_0's binary_logloss: 0.082137\n",
      "[391]\tvalid_0's binary_logloss: 0.0821386\n",
      "[392]\tvalid_0's binary_logloss: 0.0821383\n",
      "[393]\tvalid_0's binary_logloss: 0.0821421\n",
      "[394]\tvalid_0's binary_logloss: 0.0821408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[395]\tvalid_0's binary_logloss: 0.0821422\n",
      "[396]\tvalid_0's binary_logloss: 0.0821437\n",
      "[397]\tvalid_0's binary_logloss: 0.0821445\n",
      "[398]\tvalid_0's binary_logloss: 0.0821424\n",
      "[399]\tvalid_0's binary_logloss: 0.0821453\n",
      "[400]\tvalid_0's binary_logloss: 0.082147\n",
      "[401]\tvalid_0's binary_logloss: 0.0821481\n",
      "[402]\tvalid_0's binary_logloss: 0.0821499\n",
      "[403]\tvalid_0's binary_logloss: 0.0821517\n",
      "[404]\tvalid_0's binary_logloss: 0.0821543\n",
      "[405]\tvalid_0's binary_logloss: 0.0821551\n",
      "[406]\tvalid_0's binary_logloss: 0.0821559\n",
      "[407]\tvalid_0's binary_logloss: 0.0821577\n",
      "[408]\tvalid_0's binary_logloss: 0.0821564\n",
      "[409]\tvalid_0's binary_logloss: 0.0821578\n",
      "[410]\tvalid_0's binary_logloss: 0.0821544\n",
      "[411]\tvalid_0's binary_logloss: 0.0821546\n",
      "[412]\tvalid_0's binary_logloss: 0.0821521\n",
      "[413]\tvalid_0's binary_logloss: 0.0821517\n",
      "[414]\tvalid_0's binary_logloss: 0.0821545\n",
      "[415]\tvalid_0's binary_logloss: 0.0821547\n",
      "[416]\tvalid_0's binary_logloss: 0.0821533\n",
      "[417]\tvalid_0's binary_logloss: 0.0821545\n",
      "[418]\tvalid_0's binary_logloss: 0.0821557\n",
      "[419]\tvalid_0's binary_logloss: 0.0821522\n",
      "[420]\tvalid_0's binary_logloss: 0.0821486\n",
      "[421]\tvalid_0's binary_logloss: 0.0821558\n",
      "[422]\tvalid_0's binary_logloss: 0.0821522\n",
      "[423]\tvalid_0's binary_logloss: 0.082152\n",
      "[424]\tvalid_0's binary_logloss: 0.0821519\n",
      "[425]\tvalid_0's binary_logloss: 0.0821488\n",
      "[426]\tvalid_0's binary_logloss: 0.0821471\n",
      "[427]\tvalid_0's binary_logloss: 0.0821487\n",
      "[428]\tvalid_0's binary_logloss: 0.08215\n",
      "[429]\tvalid_0's binary_logloss: 0.0821519\n",
      "[430]\tvalid_0's binary_logloss: 0.0821481\n",
      "[431]\tvalid_0's binary_logloss: 0.0821476\n",
      "[432]\tvalid_0's binary_logloss: 0.0821505\n",
      "[433]\tvalid_0's binary_logloss: 0.0821522\n",
      "[434]\tvalid_0's binary_logloss: 0.0821523\n",
      "[435]\tvalid_0's binary_logloss: 0.0821499\n",
      "[436]\tvalid_0's binary_logloss: 0.0821522\n",
      "[437]\tvalid_0's binary_logloss: 0.0821524\n",
      "[438]\tvalid_0's binary_logloss: 0.0821534\n",
      "[439]\tvalid_0's binary_logloss: 0.0821546\n",
      "[440]\tvalid_0's binary_logloss: 0.0821567\n",
      "[441]\tvalid_0's binary_logloss: 0.0821582\n",
      "[442]\tvalid_0's binary_logloss: 0.0821604\n",
      "[443]\tvalid_0's binary_logloss: 0.0821604\n",
      "[444]\tvalid_0's binary_logloss: 0.08216\n",
      "[445]\tvalid_0's binary_logloss: 0.0821627\n",
      "[446]\tvalid_0's binary_logloss: 0.0821634\n",
      "[447]\tvalid_0's binary_logloss: 0.0821642\n",
      "[448]\tvalid_0's binary_logloss: 0.0821681\n",
      "[449]\tvalid_0's binary_logloss: 0.0821692\n",
      "[450]\tvalid_0's binary_logloss: 0.0821705\n",
      "[451]\tvalid_0's binary_logloss: 0.0821696\n",
      "[452]\tvalid_0's binary_logloss: 0.0821698\n",
      "[453]\tvalid_0's binary_logloss: 0.0821721\n",
      "[454]\tvalid_0's binary_logloss: 0.0821741\n",
      "[455]\tvalid_0's binary_logloss: 0.0821735\n",
      "[456]\tvalid_0's binary_logloss: 0.0821711\n",
      "[457]\tvalid_0's binary_logloss: 0.0821711\n",
      "[458]\tvalid_0's binary_logloss: 0.0821737\n",
      "[459]\tvalid_0's binary_logloss: 0.0821729\n",
      "[460]\tvalid_0's binary_logloss: 0.0821782\n",
      "[461]\tvalid_0's binary_logloss: 0.08218\n",
      "[462]\tvalid_0's binary_logloss: 0.0821796\n",
      "[463]\tvalid_0's binary_logloss: 0.0821786\n",
      "[464]\tvalid_0's binary_logloss: 0.082181\n",
      "[465]\tvalid_0's binary_logloss: 0.0821804\n",
      "[466]\tvalid_0's binary_logloss: 0.0821807\n",
      "[467]\tvalid_0's binary_logloss: 0.0821818\n",
      "[468]\tvalid_0's binary_logloss: 0.0821825\n",
      "[469]\tvalid_0's binary_logloss: 0.0821842\n",
      "[470]\tvalid_0's binary_logloss: 0.082187\n",
      "[471]\tvalid_0's binary_logloss: 0.0821868\n",
      "[472]\tvalid_0's binary_logloss: 0.082189\n",
      "[473]\tvalid_0's binary_logloss: 0.0821867\n",
      "[474]\tvalid_0's binary_logloss: 0.0821858\n",
      "[475]\tvalid_0's binary_logloss: 0.0821853\n",
      "[476]\tvalid_0's binary_logloss: 0.0821852\n",
      "[477]\tvalid_0's binary_logloss: 0.0821839\n",
      "[478]\tvalid_0's binary_logloss: 0.0821858\n",
      "[479]\tvalid_0's binary_logloss: 0.0821883\n",
      "[480]\tvalid_0's binary_logloss: 0.0821897\n",
      "[481]\tvalid_0's binary_logloss: 0.0821902\n",
      "[482]\tvalid_0's binary_logloss: 0.0821897\n",
      "[483]\tvalid_0's binary_logloss: 0.0821898\n",
      "[484]\tvalid_0's binary_logloss: 0.0821893\n",
      "[485]\tvalid_0's binary_logloss: 0.0821909\n",
      "[486]\tvalid_0's binary_logloss: 0.0821922\n",
      "[487]\tvalid_0's binary_logloss: 0.0821922\n",
      "[488]\tvalid_0's binary_logloss: 0.0821931\n",
      "[489]\tvalid_0's binary_logloss: 0.0821947\n",
      "[490]\tvalid_0's binary_logloss: 0.082194\n",
      "[491]\tvalid_0's binary_logloss: 0.0821942\n",
      "[492]\tvalid_0's binary_logloss: 0.0821939\n",
      "[493]\tvalid_0's binary_logloss: 0.0821943\n",
      "[494]\tvalid_0's binary_logloss: 0.0821956\n",
      "[495]\tvalid_0's binary_logloss: 0.0822078\n",
      "[496]\tvalid_0's binary_logloss: 0.0822096\n",
      "[497]\tvalid_0's binary_logloss: 0.0822108\n",
      "[498]\tvalid_0's binary_logloss: 0.0822129\n",
      "[499]\tvalid_0's binary_logloss: 0.0822134\n",
      "[500]\tvalid_0's binary_logloss: 0.0822152\n",
      "[501]\tvalid_0's binary_logloss: 0.0822146\n",
      "[502]\tvalid_0's binary_logloss: 0.0822157\n",
      "[503]\tvalid_0's binary_logloss: 0.0822191\n",
      "[504]\tvalid_0's binary_logloss: 0.0822193\n",
      "[505]\tvalid_0's binary_logloss: 0.0822209\n",
      "[506]\tvalid_0's binary_logloss: 0.0822236\n",
      "[507]\tvalid_0's binary_logloss: 0.0822264\n",
      "[508]\tvalid_0's binary_logloss: 0.0822234\n",
      "[509]\tvalid_0's binary_logloss: 0.0822173\n",
      "[510]\tvalid_0's binary_logloss: 0.0822174\n",
      "[511]\tvalid_0's binary_logloss: 0.0822167\n",
      "[512]\tvalid_0's binary_logloss: 0.0822171\n",
      "[513]\tvalid_0's binary_logloss: 0.0822174\n",
      "[514]\tvalid_0's binary_logloss: 0.0822198\n",
      "[515]\tvalid_0's binary_logloss: 0.0822211\n",
      "[516]\tvalid_0's binary_logloss: 0.0822222\n",
      "[517]\tvalid_0's binary_logloss: 0.0822242\n",
      "[518]\tvalid_0's binary_logloss: 0.0822242\n",
      "[519]\tvalid_0's binary_logloss: 0.0822236\n",
      "[520]\tvalid_0's binary_logloss: 0.0822254\n",
      "[521]\tvalid_0's binary_logloss: 0.0822259\n",
      "[522]\tvalid_0's binary_logloss: 0.0822249\n",
      "[523]\tvalid_0's binary_logloss: 0.0822248\n",
      "[524]\tvalid_0's binary_logloss: 0.0822298\n",
      "[525]\tvalid_0's binary_logloss: 0.0822325\n",
      "[526]\tvalid_0's binary_logloss: 0.0822395\n",
      "[527]\tvalid_0's binary_logloss: 0.082241\n",
      "[528]\tvalid_0's binary_logloss: 0.0822416\n",
      "[529]\tvalid_0's binary_logloss: 0.0822438\n",
      "[530]\tvalid_0's binary_logloss: 0.0822458\n",
      "[531]\tvalid_0's binary_logloss: 0.082247\n",
      "[532]\tvalid_0's binary_logloss: 0.0822498\n",
      "[533]\tvalid_0's binary_logloss: 0.0822514\n",
      "[534]\tvalid_0's binary_logloss: 0.0822509\n",
      "[535]\tvalid_0's binary_logloss: 0.0822491\n",
      "[536]\tvalid_0's binary_logloss: 0.0822511\n",
      "[537]\tvalid_0's binary_logloss: 0.0822513\n",
      "[538]\tvalid_0's binary_logloss: 0.0822509\n",
      "[539]\tvalid_0's binary_logloss: 0.0822524\n",
      "[540]\tvalid_0's binary_logloss: 0.0822572\n",
      "[541]\tvalid_0's binary_logloss: 0.0822585\n",
      "[542]\tvalid_0's binary_logloss: 0.0822582\n",
      "[543]\tvalid_0's binary_logloss: 0.0822603\n",
      "[544]\tvalid_0's binary_logloss: 0.0822621\n",
      "[545]\tvalid_0's binary_logloss: 0.0822625\n",
      "[546]\tvalid_0's binary_logloss: 0.0822651\n",
      "[547]\tvalid_0's binary_logloss: 0.0822607\n",
      "[548]\tvalid_0's binary_logloss: 0.0822611\n",
      "[549]\tvalid_0's binary_logloss: 0.0822744\n",
      "[550]\tvalid_0's binary_logloss: 0.0822754\n",
      "[551]\tvalid_0's binary_logloss: 0.0822768\n",
      "[552]\tvalid_0's binary_logloss: 0.0822771\n",
      "[553]\tvalid_0's binary_logloss: 0.082277\n",
      "[554]\tvalid_0's binary_logloss: 0.082278\n",
      "[555]\tvalid_0's binary_logloss: 0.0822794\n",
      "[556]\tvalid_0's binary_logloss: 0.082276\n",
      "[557]\tvalid_0's binary_logloss: 0.0822779\n",
      "[558]\tvalid_0's binary_logloss: 0.0822786\n",
      "[559]\tvalid_0's binary_logloss: 0.0822789\n",
      "[560]\tvalid_0's binary_logloss: 0.0822816\n",
      "[561]\tvalid_0's binary_logloss: 0.0822708\n",
      "[562]\tvalid_0's binary_logloss: 0.0822702\n",
      "[563]\tvalid_0's binary_logloss: 0.0822705\n",
      "[564]\tvalid_0's binary_logloss: 0.0822714\n",
      "[565]\tvalid_0's binary_logloss: 0.0822717\n",
      "[566]\tvalid_0's binary_logloss: 0.0822728\n",
      "[567]\tvalid_0's binary_logloss: 0.0822735\n",
      "[568]\tvalid_0's binary_logloss: 0.0822741\n",
      "[569]\tvalid_0's binary_logloss: 0.082275\n",
      "[570]\tvalid_0's binary_logloss: 0.0822726\n",
      "[571]\tvalid_0's binary_logloss: 0.0822731\n",
      "[572]\tvalid_0's binary_logloss: 0.0822731\n",
      "[573]\tvalid_0's binary_logloss: 0.0822735\n",
      "[574]\tvalid_0's binary_logloss: 0.0822747\n",
      "[575]\tvalid_0's binary_logloss: 0.0822714\n",
      "[576]\tvalid_0's binary_logloss: 0.0822726\n",
      "[577]\tvalid_0's binary_logloss: 0.0822734\n",
      "[578]\tvalid_0's binary_logloss: 0.0822884\n",
      "[579]\tvalid_0's binary_logloss: 0.0822894\n",
      "[580]\tvalid_0's binary_logloss: 0.0822893\n",
      "[581]\tvalid_0's binary_logloss: 0.0822902\n",
      "[582]\tvalid_0's binary_logloss: 0.082291\n",
      "[583]\tvalid_0's binary_logloss: 0.0822824\n",
      "[584]\tvalid_0's binary_logloss: 0.0822834\n",
      "[585]\tvalid_0's binary_logloss: 0.0822858\n",
      "[586]\tvalid_0's binary_logloss: 0.0822843\n",
      "[587]\tvalid_0's binary_logloss: 0.0822842\n",
      "[588]\tvalid_0's binary_logloss: 0.0822854\n",
      "[589]\tvalid_0's binary_logloss: 0.0822857\n",
      "[590]\tvalid_0's binary_logloss: 0.0822871\n",
      "[591]\tvalid_0's binary_logloss: 0.0822847\n",
      "[592]\tvalid_0's binary_logloss: 0.0822834\n",
      "[593]\tvalid_0's binary_logloss: 0.0822859\n",
      "[594]\tvalid_0's binary_logloss: 0.0822861\n",
      "[595]\tvalid_0's binary_logloss: 0.082282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[596]\tvalid_0's binary_logloss: 0.0822839\n",
      "[597]\tvalid_0's binary_logloss: 0.0822815\n",
      "[598]\tvalid_0's binary_logloss: 0.0822821\n",
      "[599]\tvalid_0's binary_logloss: 0.0822822\n",
      "[600]\tvalid_0's binary_logloss: 0.0822832\n",
      "[601]\tvalid_0's binary_logloss: 0.082285\n",
      "[602]\tvalid_0's binary_logloss: 0.0822956\n",
      "[603]\tvalid_0's binary_logloss: 0.0822955\n",
      "[604]\tvalid_0's binary_logloss: 0.0822956\n",
      "[605]\tvalid_0's binary_logloss: 0.0822954\n",
      "[606]\tvalid_0's binary_logloss: 0.0822978\n",
      "[607]\tvalid_0's binary_logloss: 0.0822992\n",
      "[608]\tvalid_0's binary_logloss: 0.0823011\n",
      "[609]\tvalid_0's binary_logloss: 0.0823045\n",
      "[610]\tvalid_0's binary_logloss: 0.0823052\n",
      "[611]\tvalid_0's binary_logloss: 0.0823084\n",
      "[612]\tvalid_0's binary_logloss: 0.0823084\n",
      "[613]\tvalid_0's binary_logloss: 0.0823046\n",
      "[614]\tvalid_0's binary_logloss: 0.0823049\n",
      "[615]\tvalid_0's binary_logloss: 0.0823057\n",
      "[616]\tvalid_0's binary_logloss: 0.0823076\n",
      "[617]\tvalid_0's binary_logloss: 0.0823191\n",
      "[618]\tvalid_0's binary_logloss: 0.0823205\n",
      "[619]\tvalid_0's binary_logloss: 0.0823191\n",
      "[620]\tvalid_0's binary_logloss: 0.0823212\n",
      "[621]\tvalid_0's binary_logloss: 0.0823217\n",
      "[622]\tvalid_0's binary_logloss: 0.0823233\n",
      "[623]\tvalid_0's binary_logloss: 0.0823222\n",
      "[624]\tvalid_0's binary_logloss: 0.0823231\n",
      "[625]\tvalid_0's binary_logloss: 0.0823243\n",
      "[626]\tvalid_0's binary_logloss: 0.0823264\n",
      "[627]\tvalid_0's binary_logloss: 0.0823291\n",
      "[628]\tvalid_0's binary_logloss: 0.0823276\n",
      "[629]\tvalid_0's binary_logloss: 0.0823278\n",
      "[630]\tvalid_0's binary_logloss: 0.0823267\n",
      "[631]\tvalid_0's binary_logloss: 0.0823278\n",
      "[632]\tvalid_0's binary_logloss: 0.0823281\n",
      "[633]\tvalid_0's binary_logloss: 0.0823274\n",
      "[634]\tvalid_0's binary_logloss: 0.082324\n",
      "[635]\tvalid_0's binary_logloss: 0.0823215\n",
      "[636]\tvalid_0's binary_logloss: 0.0823225\n",
      "[637]\tvalid_0's binary_logloss: 0.0823234\n",
      "[638]\tvalid_0's binary_logloss: 0.0823257\n",
      "[639]\tvalid_0's binary_logloss: 0.082327\n",
      "[640]\tvalid_0's binary_logloss: 0.0823273\n",
      "[641]\tvalid_0's binary_logloss: 0.0823275\n",
      "[642]\tvalid_0's binary_logloss: 0.0823292\n",
      "[643]\tvalid_0's binary_logloss: 0.0823312\n",
      "[644]\tvalid_0's binary_logloss: 0.0823313\n",
      "[645]\tvalid_0's binary_logloss: 0.0823316\n",
      "[646]\tvalid_0's binary_logloss: 0.0823282\n",
      "[647]\tvalid_0's binary_logloss: 0.0823289\n",
      "[648]\tvalid_0's binary_logloss: 0.0823406\n",
      "[649]\tvalid_0's binary_logloss: 0.082342\n",
      "[650]\tvalid_0's binary_logloss: 0.0823409\n",
      "[651]\tvalid_0's binary_logloss: 0.0823431\n",
      "[652]\tvalid_0's binary_logloss: 0.082345\n",
      "[653]\tvalid_0's binary_logloss: 0.0823463\n",
      "[654]\tvalid_0's binary_logloss: 0.0823451\n",
      "[655]\tvalid_0's binary_logloss: 0.0823482\n",
      "[656]\tvalid_0's binary_logloss: 0.0823495\n",
      "[657]\tvalid_0's binary_logloss: 0.08235\n",
      "[658]\tvalid_0's binary_logloss: 0.0823504\n",
      "[659]\tvalid_0's binary_logloss: 0.0823528\n",
      "[660]\tvalid_0's binary_logloss: 0.0823531\n",
      "[661]\tvalid_0's binary_logloss: 0.0823523\n",
      "[662]\tvalid_0's binary_logloss: 0.0823493\n",
      "[663]\tvalid_0's binary_logloss: 0.0823586\n",
      "[664]\tvalid_0's binary_logloss: 0.0823596\n",
      "[665]\tvalid_0's binary_logloss: 0.0823609\n",
      "[666]\tvalid_0's binary_logloss: 0.082361\n",
      "[667]\tvalid_0's binary_logloss: 0.0823625\n",
      "[668]\tvalid_0's binary_logloss: 0.08236\n",
      "[669]\tvalid_0's binary_logloss: 0.0823615\n",
      "[670]\tvalid_0's binary_logloss: 0.0823639\n",
      "[671]\tvalid_0's binary_logloss: 0.0823648\n",
      "[672]\tvalid_0's binary_logloss: 0.0823663\n",
      "[673]\tvalid_0's binary_logloss: 0.0823672\n",
      "[674]\tvalid_0's binary_logloss: 0.082369\n",
      "[675]\tvalid_0's binary_logloss: 0.082368\n",
      "[676]\tvalid_0's binary_logloss: 0.082366\n",
      "[677]\tvalid_0's binary_logloss: 0.0823668\n",
      "[678]\tvalid_0's binary_logloss: 0.0823639\n",
      "[679]\tvalid_0's binary_logloss: 0.0823642\n",
      "[680]\tvalid_0's binary_logloss: 0.0823593\n",
      "[681]\tvalid_0's binary_logloss: 0.0823611\n",
      "[682]\tvalid_0's binary_logloss: 0.0823621\n",
      "[683]\tvalid_0's binary_logloss: 0.0823644\n",
      "[684]\tvalid_0's binary_logloss: 0.0823663\n",
      "[685]\tvalid_0's binary_logloss: 0.0823767\n",
      "Early stopping, best iteration is:\n",
      "[385]\tvalid_0's binary_logloss: 0.0821334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', colsample_bytree=1.0, learning_rate=0.01,\n",
       "       max_bin=255, max_depth=-1, min_child_samples=10, min_child_weight=5,\n",
       "       min_split_gain=0.0, n_estimators=2000, n_jobs=-1, num_leaves=64,\n",
       "       objective='binary', random_state=0, reg_alpha=0.0, reg_lambda=0.0,\n",
       "       silent=True, subsample=1.0, subsample_for_bin=50000,\n",
       "       subsample_freq=1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "gbm = lgb.LGBMRegressor(objective='binary',\n",
    "\n",
    "                        num_leaves=64,\n",
    "\n",
    "                        learning_rate=0.01,\n",
    "\n",
    "                        n_estimators=2000)\n",
    "\n",
    "gbm.fit(feature_train, label_train,\n",
    "\n",
    "        eval_set=[(feature_val, label_val)],\n",
    "\n",
    "        eval_metric='binary_logloss',\n",
    "\n",
    "        early_stopping_rounds=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0821334490226\n"
     ]
    }
   ],
   "source": [
    "label_val_bgm = gbm.predict(feature_val, num_iteration=gbm.best_iteration_)\n",
    "\n",
    "label_test = gbm.predict(feature_test, num_iteration=gbm.best_iteration_)\n",
    "\n",
    "print(log_loss(label_val,label_val_bgm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('data/round1_result_lgb.txt', np.c_[data_index, label_test], delimiter=',', header='instance_id predicted_score',comments='', fmt='%s %f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.584341164082\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier    \n",
    "model = GradientBoostingClassifier()    \n",
    "model.fit(feature_train, label_train)    \n",
    "label_val_gbdt = clf.predict(feature_val)  \n",
    "print(log_loss(label_val,label_val_gbdt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.FM/FFM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_train[label_train < 0.5] = -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_train = csc_matrix(feature_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_val = csc_matrix(feature_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/opt/conda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/opt/conda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1735: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  cond2 = (x >= self.b) & cond0\n",
      "/opt/conda/lib/python3.6/site-packages/fastFM/base.py:123: RuntimeWarning: invalid value encountered in greater\n",
      "  y_pred[y_proba > .5] = self.classes_[1]\n"
     ]
    }
   ],
   "source": [
    "from fastFM import sgd,mcmc\n",
    "fm = sgd.FMClassification(\n",
    "#     n_iter=1000, init_stdev=0.1, l2_reg_w=0,l2_reg_V=0, rank=2, step_size=0.1\n",
    ")\n",
    "fm.fit(feature_train, label_train)\n",
    "label_val_fm = fm.predict(feature_val)\n",
    "\n",
    "# fm = mcmc.FMClassification(n_iter=1000, rank=2, init_stdev=0.1)\n",
    "# y_pred = fm.fit_predict(X_train, y_train, X_test)\n",
    "# label_val_fm = fm.fit_predict_proba(X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0    57334\n",
       "dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(label_val_fm).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.集成\n",
    "要求：\n",
    "- Base Model 之间的相关性要尽可能的小\n",
    "- Base Model 之间的性能表现不能差距太大\n",
    "\n",
    "## 9.1Bagging\n",
    "- 使用训练数据的不同随机子集来训练每个 Base Model，最后进行每个 Base Model 权重相同的 Vote。\n",
    "\n",
    "## 9.2Boosting\n",
    "- 迭代地训练 Base Model，每次根据上一个迭代中预测错误的情况修改训练样本的权重。\n",
    "\n",
    "## 9.3Blending\n",
    "- 用不相交的数据训练不同的 Base Model，将它们的输出取（加权）平均。实现简单，但对训练数据利用少了。\n",
    "\n",
    "## 9.4Stacking\n",
    "- 我用了两层的模型融合，Level 1使用了：XGBoost、LightGBM、RandomForest、ExtraTrees、DecisionTree、AdaBoost，一共6个模型，Level 2使用了LinearRegression来拟合第一层的结果。\n",
    "- http://blog.csdn.net/a358463121/article/details/53054686"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
