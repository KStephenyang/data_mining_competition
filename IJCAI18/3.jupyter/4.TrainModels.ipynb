{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.导包、导数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.sparse import csc_matrix\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 取17-23作为测试，24作为验证\n",
    "data = pd.read_pickle('data/round1_train2')\n",
    "feature_train = data[data['f16'] <= 1537718400]\n",
    "feature_train.drop('label',axis = 1,inplace = True)\n",
    "feature_val = data[data['f16'] > 1537718400]\n",
    "feature_val.drop('label',axis = 1,inplace = True)\n",
    "\n",
    "label_train = data[data['f16'] <= 1537718400]['label']\n",
    "label_val = data[data['f16'] > 1537718400]['label']\n",
    "\n",
    "data = pd.read_pickle('data/round1_test2')\n",
    "feature_test = data\n",
    "\n",
    "data_index = pd.DataFrame(np.genfromtxt('data/round1_test.txt',dtype = np.str, delimiter=' ',skip_header=1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取18-23作为测试，24作为验证\n",
    "data = pd.read_pickle('data/round1_train2')\n",
    "feature_train = data[(data['f16'] <= 1537718400) & (data['f16'] > 1537200000)]\n",
    "feature_train.drop('label',axis = 1,inplace = True)\n",
    "feature_val = data[data['f16'] > 1537718400]\n",
    "feature_val.drop('label',axis = 1,inplace = True)\n",
    "\n",
    "label_train = data[(data['f16'] <= 1537718400) & (data['f16'] > 1537200000)]['label']\n",
    "label_val = data[data['f16'] > 1537718400]['label']\n",
    "\n",
    "data = pd.read_pickle('data/round1_test2')\n",
    "feature_test = data\n",
    "\n",
    "data_index = pd.DataFrame(np.genfromtxt('data/round1_test.txt',dtype = np.str, delimiter=' ',skip_header=1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 取17-22作为测试，23作为验证\n",
    "data = pd.read_pickle('data/round1_train2')\n",
    "feature_train = data[data['f16'] <= 1537632000]\n",
    "feature_train.drop('label',axis = 1,inplace = True)\n",
    "feature_val = data[(data['f16'] > 1537632000) & (data['f16'] <= 1537718400)]\n",
    "feature_val.drop('label',axis = 1,inplace = True)\n",
    "\n",
    "label_train = data[data['f16'] <= 1537632000]['label']\n",
    "label_val = data[(data['f16'] > 1537632000) & (data['f16'] <= 1537718400)]['label']\n",
    "\n",
    "data = pd.read_pickle('data/round1_test2')\n",
    "feature_test = data\n",
    "\n",
    "data_index = pd.DataFrame(np.genfromtxt('data/round1_test.txt',dtype = np.str, delimiter=' ',skip_header=1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 取18-22作为测试，23作为验证\n",
    "data = pd.read_pickle('data/round1_train2')\n",
    "feature_train = data[(data['f16'] <= 1537632000) & (data['f16'] > 1537200000)]\n",
    "feature_train.drop('label',axis = 1,inplace = True)\n",
    "feature_val = data[(data['f16'] > 1537632000) & (data['f16'] <= 1537718400)]\n",
    "feature_val.drop('label',axis = 1,inplace = True)\n",
    "\n",
    "label_train = data[(data['f16'] <= 1537632000) & (data['f16'] > 1537200000)]['label']\n",
    "label_val = data[(data['f16'] > 1537632000) & (data['f16'] <= 1537718400)]['label']\n",
    "\n",
    "data = pd.read_pickle('data/round1_test2')\n",
    "feature_test = data\n",
    "\n",
    "data_index = pd.DataFrame(np.genfromtxt('data/round1_test.txt',dtype = np.str, delimiter=' ',skip_header=1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['f0', 'f1', 'f2:2_22731265849056483', 'f2:2_509660095530134768',\n",
       "       'f2:2_1968056100269760729', 'f2:2_2011981573061447208',\n",
       "       'f2:2_2436715285093487584', 'f2:2_2642175453151805566',\n",
       "       'f2:2_3203673979138763595', 'f2:2_4879721024980945592',\n",
       "       'f2:2_5755694407684602296', 'f2:2_5799347067982556520',\n",
       "       'f2:2_7258015885215914736', 'f2:2_8277336076276184272',\n",
       "       'f2:2_8710739180200009128', 'f2:3_-1', 'f2:3_6233669177166538628',\n",
       "       'f2:3_8868887661186419229', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10',\n",
       "       'f11:woman', 'f11:man', 'f11:family', 'f11:other', 'f12', 'f13:2002.0',\n",
       "       'f13:2003.0', 'f13:2004.0', 'f13:2005.0', 'f14', 'f15', 'f16',\n",
       "       'f16:day', 'f16:hour', 'f16:minute', 'f16:second', 'f16:dayofweek',\n",
       "       'f16:dayofyear', 'f17', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25',\n",
       "       'f1-ratio', 'f4-ratio', 'f5-ratio', 'f10-ratio', 'f19-ratio',\n",
       "       'f1-f4-ratio', 'f1-f5-ratio', 'f4-f5-ratio', 'f4-f19-ratio',\n",
       "       'f5-f19-ratio', 'f1-count', 'f4-count', 'f5-count', 'f10-count',\n",
       "       'f19-count', 'f1-f4-count', 'f1-f5-count', 'f4-f5-count',\n",
       "       'f4-f19-count', 'f5-f19-count', 'f1-browse-count', 'f4-browse-count',\n",
       "       'f5-browse-count', 'f10-browse-count', 'f19-browse-count',\n",
       "       'f1-f4-browse-count', 'f1-f5-browse-count', 'f1-f10-browse-count',\n",
       "       'f1-f19-browse-count', 'f4-f5-browse-count', 'f4-f10-browse-count',\n",
       "       'f4-f19-browse-count', 'f5-f10-browse-count', 'f5-f19-browse-count',\n",
       "       'f10-f19-browse-count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_train.drop(['f1-f4-ratio', 'f1-f5-ratio', 'f4-f5-ratio', 'f4-f19-ratio','f5-f19-ratio'],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_val.drop(['f1-f4-ratio', 'f1-f5-ratio', 'f4-f5-ratio', 'f4-f19-ratio','f5-f19-ratio'],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_test.drop(['f1-f4-ratio', 'f1-f5-ratio', 'f4-f5-ratio', 'f4-f19-ratio','f5-f19-ratio'],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.训练SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.957631066\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm,datasets\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "#调用SVC()\n",
    "# 参数见：http://blog.csdn.net/xiaodongxiexie/article/details/70667101\n",
    "clf = svm.SVC(\n",
    "            C=0.001, \n",
    "            kernel='rbf', \n",
    "            degree=3, \n",
    "            gamma='auto', \n",
    "            coef0=0.0, \n",
    "            probability=False, \n",
    "            shrinking=True, \n",
    "            tol=0.001, \n",
    "            cache_size=200, \n",
    "            class_weight='balanced', \n",
    "            verbose=True, \n",
    "            max_iter=1000, \n",
    "            decision_function_shape='ovr', \n",
    "            random_state=None\n",
    ")\n",
    "#载入鸢尾花数据集\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "#fit()训练\n",
    "clf.fit(feature_train,label_train)\n",
    "\n",
    "#predict()预测\n",
    "label_val_svm = clf.predict(feature_val)\n",
    "print(log_loss(label_val,label_val_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.训练逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1711058309\n",
      "1.1711058309\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "feature_train_now = StandardScaler().fit_transform(feature_train)\n",
    "\n",
    "\n",
    "clf_l1_LR= LogisticRegression(\n",
    "#    tol = 0.0001,C = 0.00005, penalty = 'l1',max_iter = 500,class_weight = 'balanced',verbose = 2\n",
    ") \n",
    "clf_l1_LR.fit(feature_train_now,label_train)\n",
    "label_val_lr = clf_l1_LR.predict(feature_val)  \n",
    "print(log_loss(label_val,label_val_lr))\n",
    "\n",
    "\n",
    "clf_l2_LR= LogisticRegression(\n",
    "#    tol = 0.0001,C = 0.0001, penalty = 'l2',max_iter = 500,class_weight = 'balanced',verbose = 2\n",
    ") \n",
    "clf_l2_LR.fit(feature_train_now,label_train)\n",
    "label_val_lr = clf_l2_LR.predict(feature_val)\n",
    "print(log_loss(label_val,label_val_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.训练xgboost\n",
    "这里要重点讲一下 Xgboost 的调参。通常认为对它性能影响较大的参数有：\n",
    "\n",
    "- eta：每次迭代完成后更新权重时的步长。越小训练越慢。\n",
    "- num_round：总共迭代的次数。\n",
    "- subsample：训练每棵树时用来训练的数据占全部的比例。用于防止 Overfitting。\n",
    "- colsample_bytree：训练每棵树时用来训练的特征的比例，类似 RandomForestClassifier 的 max_features。\n",
    "- max_depth：每棵树的最大深度限制。与 Random Forest 不同，Gradient Boosting 如果不对深度加以限制，最终是会 Overfit 的。\n",
    "- early_stopping_rounds：用于控制在 Out Of Sample 的验证集上连续多少个迭代的分数都没有提高后就提前终止训练。用于防止 Overfitting。\n",
    "\n",
    "一般的调参步骤是：\n",
    "\n",
    "- 将训练数据的一部分划出来作为验证集。\n",
    "- 先将 eta 设得比较高（比如 0.1），num_round 设为 300 ~ 500。\n",
    "- 用 Grid Search 对其他参数进行搜索\n",
    "- 逐步将 eta 降低，找到最佳值。\n",
    "- 以验证集为 watchlist，用找到的最佳参数组合重新在训练集上训练。注意观察算法的输出，看每次迭代后在验证集上分数的变化情况，从而得到最佳的 early_stopping_rounds。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = xgb.DMatrix(feature_train,label = label_train)\n",
    "val = xgb.DMatrix(feature_val,label = label_val)\n",
    "test = xgb.DMatrix(feature_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "\n",
    "\n",
    "param = {\n",
    "    # 1.General Parameters\n",
    "    'booster': 'gbtree', # 提升计算的模型，可以是：gbtree, gblinear or dart\n",
    "    'silent': 0,  # 0为打印信息，1为缄默\n",
    "    # 'nthread': 4, # XGBoost运行时的线程数\n",
    "    # 'num_pbuffer': # 预测缓冲区的大小，通常设置为训练实例的数量。缓冲区用于保存最后一次提升步骤的预测结果。\n",
    "    # 'num_feature': # 特征值数量\n",
    "    \n",
    "    # 2.Booster Parameters\n",
    "    'eta': 0.05,  # 学习率，默认0.3，取值范围为：[0,1]，典型值为0.01-0.2，越小越保守\n",
    "    'gamma': 0.1,  # 节点分裂所需的最小损失函数下降值，和损失函数息息相关。默认0，典型值0.1、0.2，越大越保守，\n",
    "    'max_depth': 5,  # 树的最大深度，默认6，典型值为3-10，越大越易过拟合。\n",
    "    'min_child_weight':1, # 这个参数非常影响结果，最小叶子节点样本权重和。如果一个叶子节点的样本权重和小于min_child_weight则拆分过程结束。在现行回归模型中，这个参数是指建立每个模型所需要的最小样本数。默认1，越大算法越易欠拟合\n",
    "    'max_delta_step ':0, # 限制每棵树权重改变的最大步长。越大越保守。通常，这个参数不需要设置。但是当各类别的样本十分不平衡时，它对逻辑回归是很有帮助的。\n",
    "    'subsample':0.8, # 用于训练模型的子样本占整个样本集合的比例，行采样，典型值0.5-1，越小越保守。\n",
    "    'colsample_bytree ': 0.8,  # 在建立树时对特征采样的比例，列采样。典型值0.5-1\n",
    "\n",
    "    'lambda':10, # 权重的L2正则化项，默认是1\n",
    "    'alpha':1,# 权重的L1正则化项，默认是1\n",
    "    'lambda_bias': 0,  # 在偏置上的L2正则。缺省值为0（在L1上没有偏置项的正则，因为L1时偏置不重要）\n",
    "    # 'scale_pos_weight':0.5, # 各类样本十分不平衡时，把这个参数设置为一个正数，可以使算法更快收敛。默认是1\n",
    "\n",
    "    # 3.Task Parameters\n",
    "    # 'tree_method': 'approx',\n",
    "    'objective': 'binary:logistic',  # 使用的模型，分类的数目\n",
    "    'eval_metric':'logloss', # 校验数据所需要的评价指标，不同的目标函数将会有缺省的评价指标\n",
    "    'base_score':0.5, # 对于所有样本预测为正样本的全局偏置（初始分数）。如果迭代次数够多，改变这个参数对结果不会有影响。\n",
    "    'seed':0 # 随机数的种子\n",
    "}\n",
    "num_boost_round = 1000  # 迭代的次数，弱分类器的数量\n",
    "\n",
    "watchlist = [(train, 'train'),(val, 'eval')]  # 看板，每次迭代都可以在控制台打印出训练集与测试集的损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.648113\teval-logloss:0.647871\n",
      "Multiple eval metrics have been passed: 'eval-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-logloss hasn't improved in 200 rounds.\n",
      "[1]\ttrain-logloss:0.607363\teval-logloss:0.606894\n",
      "[2]\ttrain-logloss:0.570336\teval-logloss:0.569654\n",
      "[3]\ttrain-logloss:0.536558\teval-logloss:0.53567\n",
      "[4]\ttrain-logloss:0.505631\teval-logloss:0.504551\n",
      "[5]\ttrain-logloss:0.477269\teval-logloss:0.476002\n",
      "[6]\ttrain-logloss:0.451169\teval-logloss:0.449803\n",
      "[7]\ttrain-logloss:0.427103\teval-logloss:0.425585\n",
      "[8]\ttrain-logloss:0.404849\teval-logloss:0.403162\n",
      "[9]\ttrain-logloss:0.384252\teval-logloss:0.382428\n",
      "[10]\ttrain-logloss:0.365151\teval-logloss:0.363163\n",
      "[11]\ttrain-logloss:0.347418\teval-logloss:0.345277\n",
      "[12]\ttrain-logloss:0.330933\teval-logloss:0.328646\n",
      "[13]\ttrain-logloss:0.315595\teval-logloss:0.31316\n",
      "[14]\ttrain-logloss:0.301291\teval-logloss:0.298736\n",
      "[15]\ttrain-logloss:0.287951\teval-logloss:0.285266\n",
      "[16]\ttrain-logloss:0.275497\teval-logloss:0.27269\n",
      "[17]\ttrain-logloss:0.263873\teval-logloss:0.260937\n",
      "[18]\ttrain-logloss:0.253002\teval-logloss:0.249942\n",
      "[19]\ttrain-logloss:0.242837\teval-logloss:0.239664\n",
      "[20]\ttrain-logloss:0.233327\teval-logloss:0.230047\n",
      "[21]\ttrain-logloss:0.224422\teval-logloss:0.221033\n",
      "[22]\ttrain-logloss:0.216063\teval-logloss:0.212565\n",
      "[23]\ttrain-logloss:0.208231\teval-logloss:0.204647\n",
      "[24]\ttrain-logloss:0.200897\teval-logloss:0.197213\n",
      "[25]\ttrain-logloss:0.194021\teval-logloss:0.19023\n",
      "[26]\ttrain-logloss:0.187579\teval-logloss:0.183688\n",
      "[27]\ttrain-logloss:0.181513\teval-logloss:0.177607\n",
      "[28]\ttrain-logloss:0.175833\teval-logloss:0.171878\n",
      "[29]\ttrain-logloss:0.17051\teval-logloss:0.166488\n",
      "[30]\ttrain-logloss:0.165509\teval-logloss:0.161395\n",
      "[31]\ttrain-logloss:0.160813\teval-logloss:0.156635\n",
      "[32]\ttrain-logloss:0.156413\teval-logloss:0.152133\n",
      "[33]\ttrain-logloss:0.152282\teval-logloss:0.147919\n",
      "[34]\ttrain-logloss:0.148409\teval-logloss:0.143957\n",
      "[35]\ttrain-logloss:0.144761\teval-logloss:0.140223\n",
      "[36]\ttrain-logloss:0.141335\teval-logloss:0.136713\n",
      "[37]\ttrain-logloss:0.138131\teval-logloss:0.13343\n",
      "[38]\ttrain-logloss:0.13512\teval-logloss:0.13034\n",
      "[39]\ttrain-logloss:0.132288\teval-logloss:0.127443\n",
      "[40]\ttrain-logloss:0.129628\teval-logloss:0.124734\n",
      "[41]\ttrain-logloss:0.127137\teval-logloss:0.12216\n",
      "[42]\ttrain-logloss:0.124789\teval-logloss:0.119766\n",
      "[43]\ttrain-logloss:0.122596\teval-logloss:0.117499\n",
      "[44]\ttrain-logloss:0.120533\teval-logloss:0.115366\n",
      "[45]\ttrain-logloss:0.118609\teval-logloss:0.113385\n",
      "[46]\ttrain-logloss:0.116808\teval-logloss:0.111524\n",
      "[47]\ttrain-logloss:0.115116\teval-logloss:0.109778\n",
      "[48]\ttrain-logloss:0.113532\teval-logloss:0.108144\n",
      "[49]\ttrain-logloss:0.112046\teval-logloss:0.106598\n",
      "[50]\ttrain-logloss:0.110638\teval-logloss:0.105134\n",
      "[51]\ttrain-logloss:0.109341\teval-logloss:0.103782\n",
      "[52]\ttrain-logloss:0.108117\teval-logloss:0.102505\n",
      "[53]\ttrain-logloss:0.106962\teval-logloss:0.101303\n",
      "[54]\ttrain-logloss:0.105883\teval-logloss:0.100183\n",
      "[55]\ttrain-logloss:0.104877\teval-logloss:0.099117\n",
      "[56]\ttrain-logloss:0.103937\teval-logloss:0.098121\n",
      "[57]\ttrain-logloss:0.103056\teval-logloss:0.097194\n",
      "[58]\ttrain-logloss:0.102238\teval-logloss:0.096345\n",
      "[59]\ttrain-logloss:0.10147\teval-logloss:0.095547\n",
      "[60]\ttrain-logloss:0.100751\teval-logloss:0.094791\n",
      "[61]\ttrain-logloss:0.10008\teval-logloss:0.094091\n",
      "[62]\ttrain-logloss:0.099454\teval-logloss:0.093429\n",
      "[63]\ttrain-logloss:0.098868\teval-logloss:0.092816\n",
      "[64]\ttrain-logloss:0.098325\teval-logloss:0.092234\n",
      "[65]\ttrain-logloss:0.097809\teval-logloss:0.091662\n",
      "[66]\ttrain-logloss:0.097332\teval-logloss:0.091166\n",
      "[67]\ttrain-logloss:0.096884\teval-logloss:0.090684\n",
      "[68]\ttrain-logloss:0.096461\teval-logloss:0.090274\n",
      "[69]\ttrain-logloss:0.096066\teval-logloss:0.089861\n",
      "[70]\ttrain-logloss:0.0957\teval-logloss:0.089488\n",
      "[71]\ttrain-logloss:0.095355\teval-logloss:0.089137\n",
      "[72]\ttrain-logloss:0.095039\teval-logloss:0.088806\n",
      "[73]\ttrain-logloss:0.094736\teval-logloss:0.088471\n",
      "[74]\ttrain-logloss:0.094456\teval-logloss:0.088163\n",
      "[75]\ttrain-logloss:0.094189\teval-logloss:0.087889\n",
      "[76]\ttrain-logloss:0.093943\teval-logloss:0.087624\n",
      "[77]\ttrain-logloss:0.093705\teval-logloss:0.087355\n",
      "[78]\ttrain-logloss:0.093488\teval-logloss:0.087141\n",
      "[79]\ttrain-logloss:0.093285\teval-logloss:0.086911\n",
      "[80]\ttrain-logloss:0.093099\teval-logloss:0.08671\n",
      "[81]\ttrain-logloss:0.092918\teval-logloss:0.086519\n",
      "[82]\ttrain-logloss:0.092754\teval-logloss:0.086347\n",
      "[83]\ttrain-logloss:0.092598\teval-logloss:0.086192\n",
      "[84]\ttrain-logloss:0.092454\teval-logloss:0.086042\n",
      "[85]\ttrain-logloss:0.092313\teval-logloss:0.085893\n",
      "[86]\ttrain-logloss:0.092182\teval-logloss:0.085756\n",
      "[87]\ttrain-logloss:0.092056\teval-logloss:0.085627\n",
      "[88]\ttrain-logloss:0.091936\teval-logloss:0.085507\n",
      "[89]\ttrain-logloss:0.091825\teval-logloss:0.085405\n",
      "[90]\ttrain-logloss:0.091718\teval-logloss:0.085294\n",
      "[91]\ttrain-logloss:0.091616\teval-logloss:0.085194\n",
      "[92]\ttrain-logloss:0.091524\teval-logloss:0.085089\n",
      "[93]\ttrain-logloss:0.09143\teval-logloss:0.085003\n",
      "[94]\ttrain-logloss:0.091344\teval-logloss:0.084922\n",
      "[95]\ttrain-logloss:0.091263\teval-logloss:0.08485\n",
      "[96]\ttrain-logloss:0.091187\teval-logloss:0.084782\n",
      "[97]\ttrain-logloss:0.091117\teval-logloss:0.084716\n",
      "[98]\ttrain-logloss:0.091043\teval-logloss:0.084655\n",
      "[99]\ttrain-logloss:0.090973\teval-logloss:0.084585\n",
      "[100]\ttrain-logloss:0.090904\teval-logloss:0.08453\n",
      "[101]\ttrain-logloss:0.09084\teval-logloss:0.084478\n",
      "[102]\ttrain-logloss:0.090784\teval-logloss:0.084426\n",
      "[103]\ttrain-logloss:0.090724\teval-logloss:0.084381\n",
      "[104]\ttrain-logloss:0.090669\teval-logloss:0.084341\n",
      "[105]\ttrain-logloss:0.090616\teval-logloss:0.084291\n",
      "[106]\ttrain-logloss:0.090561\teval-logloss:0.084254\n",
      "[107]\ttrain-logloss:0.090517\teval-logloss:0.084214\n",
      "[108]\ttrain-logloss:0.090473\teval-logloss:0.084174\n",
      "[109]\ttrain-logloss:0.09043\teval-logloss:0.084134\n",
      "[110]\ttrain-logloss:0.090378\teval-logloss:0.084106\n",
      "[111]\ttrain-logloss:0.090337\teval-logloss:0.084075\n",
      "[112]\ttrain-logloss:0.09029\teval-logloss:0.08405\n",
      "[113]\ttrain-logloss:0.090245\teval-logloss:0.084023\n",
      "[114]\ttrain-logloss:0.090205\teval-logloss:0.084003\n",
      "[115]\ttrain-logloss:0.09017\teval-logloss:0.083971\n",
      "[116]\ttrain-logloss:0.090132\teval-logloss:0.083945\n",
      "[117]\ttrain-logloss:0.09009\teval-logloss:0.083926\n",
      "[118]\ttrain-logloss:0.090051\teval-logloss:0.083904\n",
      "[119]\ttrain-logloss:0.090014\teval-logloss:0.083891\n",
      "[120]\ttrain-logloss:0.089973\teval-logloss:0.083877\n",
      "[121]\ttrain-logloss:0.089941\teval-logloss:0.08385\n",
      "[122]\ttrain-logloss:0.089904\teval-logloss:0.083826\n",
      "[123]\ttrain-logloss:0.089865\teval-logloss:0.083808\n",
      "[124]\ttrain-logloss:0.089838\teval-logloss:0.08377\n",
      "[125]\ttrain-logloss:0.08981\teval-logloss:0.083758\n",
      "[126]\ttrain-logloss:0.089783\teval-logloss:0.083747\n",
      "[127]\ttrain-logloss:0.089751\teval-logloss:0.083748\n",
      "[128]\ttrain-logloss:0.089707\teval-logloss:0.083742\n",
      "[129]\ttrain-logloss:0.089675\teval-logloss:0.083728\n",
      "[130]\ttrain-logloss:0.089641\teval-logloss:0.083721\n",
      "[131]\ttrain-logloss:0.089617\teval-logloss:0.083709\n",
      "[132]\ttrain-logloss:0.089594\teval-logloss:0.083694\n",
      "[133]\ttrain-logloss:0.08957\teval-logloss:0.083676\n",
      "[134]\ttrain-logloss:0.08954\teval-logloss:0.083675\n",
      "[135]\ttrain-logloss:0.089509\teval-logloss:0.083671\n",
      "[136]\ttrain-logloss:0.089481\teval-logloss:0.083668\n",
      "[137]\ttrain-logloss:0.089447\teval-logloss:0.083661\n",
      "[138]\ttrain-logloss:0.089424\teval-logloss:0.083651\n",
      "[139]\ttrain-logloss:0.089396\teval-logloss:0.08364\n",
      "[140]\ttrain-logloss:0.089372\teval-logloss:0.083633\n",
      "[141]\ttrain-logloss:0.08934\teval-logloss:0.083632\n",
      "[142]\ttrain-logloss:0.08932\teval-logloss:0.083633\n",
      "[143]\ttrain-logloss:0.089297\teval-logloss:0.083608\n",
      "[144]\ttrain-logloss:0.089265\teval-logloss:0.083598\n",
      "[145]\ttrain-logloss:0.089246\teval-logloss:0.083601\n",
      "[146]\ttrain-logloss:0.089223\teval-logloss:0.083586\n",
      "[147]\ttrain-logloss:0.0892\teval-logloss:0.083574\n",
      "[148]\ttrain-logloss:0.089179\teval-logloss:0.083565\n",
      "[149]\ttrain-logloss:0.089146\teval-logloss:0.083564\n",
      "[150]\ttrain-logloss:0.089125\teval-logloss:0.083555\n",
      "[151]\ttrain-logloss:0.089104\teval-logloss:0.083552\n",
      "[152]\ttrain-logloss:0.089085\teval-logloss:0.083552\n",
      "[153]\ttrain-logloss:0.08906\teval-logloss:0.083554\n",
      "[154]\ttrain-logloss:0.089038\teval-logloss:0.083537\n",
      "[155]\ttrain-logloss:0.089012\teval-logloss:0.083538\n",
      "[156]\ttrain-logloss:0.088986\teval-logloss:0.083531\n",
      "[157]\ttrain-logloss:0.08896\teval-logloss:0.083528\n",
      "[158]\ttrain-logloss:0.088937\teval-logloss:0.083528\n",
      "[159]\ttrain-logloss:0.088904\teval-logloss:0.083525\n",
      "[160]\ttrain-logloss:0.088879\teval-logloss:0.083524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[161]\ttrain-logloss:0.088855\teval-logloss:0.083508\n",
      "[162]\ttrain-logloss:0.088834\teval-logloss:0.083506\n",
      "[163]\ttrain-logloss:0.088808\teval-logloss:0.083513\n",
      "[164]\ttrain-logloss:0.088785\teval-logloss:0.083512\n",
      "[165]\ttrain-logloss:0.088764\teval-logloss:0.083508\n",
      "[166]\ttrain-logloss:0.088747\teval-logloss:0.083509\n",
      "[167]\ttrain-logloss:0.088723\teval-logloss:0.083503\n",
      "[168]\ttrain-logloss:0.088698\teval-logloss:0.083498\n",
      "[169]\ttrain-logloss:0.08868\teval-logloss:0.083491\n",
      "[170]\ttrain-logloss:0.088655\teval-logloss:0.083496\n",
      "[171]\ttrain-logloss:0.088631\teval-logloss:0.083492\n",
      "[172]\ttrain-logloss:0.088618\teval-logloss:0.083489\n",
      "[173]\ttrain-logloss:0.088591\teval-logloss:0.083483\n",
      "[174]\ttrain-logloss:0.088576\teval-logloss:0.083479\n",
      "[175]\ttrain-logloss:0.088556\teval-logloss:0.083472\n",
      "[176]\ttrain-logloss:0.08854\teval-logloss:0.083471\n",
      "[177]\ttrain-logloss:0.08852\teval-logloss:0.083469\n",
      "[178]\ttrain-logloss:0.088496\teval-logloss:0.083466\n",
      "[179]\ttrain-logloss:0.088473\teval-logloss:0.083463\n",
      "[180]\ttrain-logloss:0.08845\teval-logloss:0.083454\n",
      "[181]\ttrain-logloss:0.088431\teval-logloss:0.083451\n",
      "[182]\ttrain-logloss:0.088418\teval-logloss:0.083453\n",
      "[183]\ttrain-logloss:0.088405\teval-logloss:0.083458\n",
      "[184]\ttrain-logloss:0.088381\teval-logloss:0.083456\n",
      "[185]\ttrain-logloss:0.088355\teval-logloss:0.083457\n",
      "[186]\ttrain-logloss:0.088325\teval-logloss:0.083453\n",
      "[187]\ttrain-logloss:0.088301\teval-logloss:0.083456\n",
      "[188]\ttrain-logloss:0.088285\teval-logloss:0.083456\n",
      "[189]\ttrain-logloss:0.088259\teval-logloss:0.083451\n",
      "[190]\ttrain-logloss:0.088231\teval-logloss:0.083437\n",
      "[191]\ttrain-logloss:0.088199\teval-logloss:0.083431\n",
      "[192]\ttrain-logloss:0.088175\teval-logloss:0.08343\n",
      "[193]\ttrain-logloss:0.088153\teval-logloss:0.08343\n",
      "[194]\ttrain-logloss:0.088136\teval-logloss:0.08343\n",
      "[195]\ttrain-logloss:0.08811\teval-logloss:0.083425\n",
      "[196]\ttrain-logloss:0.088082\teval-logloss:0.083427\n",
      "[197]\ttrain-logloss:0.088058\teval-logloss:0.083426\n",
      "[198]\ttrain-logloss:0.088032\teval-logloss:0.083422\n",
      "[199]\ttrain-logloss:0.08801\teval-logloss:0.083409\n",
      "[200]\ttrain-logloss:0.087986\teval-logloss:0.083398\n",
      "[201]\ttrain-logloss:0.087962\teval-logloss:0.083387\n",
      "[202]\ttrain-logloss:0.087941\teval-logloss:0.083392\n",
      "[203]\ttrain-logloss:0.087924\teval-logloss:0.083394\n",
      "[204]\ttrain-logloss:0.087897\teval-logloss:0.083388\n",
      "[205]\ttrain-logloss:0.087876\teval-logloss:0.083386\n",
      "[206]\ttrain-logloss:0.087855\teval-logloss:0.083386\n",
      "[207]\ttrain-logloss:0.087827\teval-logloss:0.083391\n",
      "[208]\ttrain-logloss:0.087812\teval-logloss:0.08339\n",
      "[209]\ttrain-logloss:0.087795\teval-logloss:0.083392\n",
      "[210]\ttrain-logloss:0.087772\teval-logloss:0.083393\n",
      "[211]\ttrain-logloss:0.087754\teval-logloss:0.083396\n",
      "[212]\ttrain-logloss:0.087739\teval-logloss:0.083396\n",
      "[213]\ttrain-logloss:0.087717\teval-logloss:0.083399\n",
      "[214]\ttrain-logloss:0.087688\teval-logloss:0.083396\n",
      "[215]\ttrain-logloss:0.087678\teval-logloss:0.083397\n",
      "[216]\ttrain-logloss:0.087663\teval-logloss:0.083397\n",
      "[217]\ttrain-logloss:0.08765\teval-logloss:0.083394\n",
      "[218]\ttrain-logloss:0.087637\teval-logloss:0.083391\n",
      "[219]\ttrain-logloss:0.087614\teval-logloss:0.083387\n",
      "[220]\ttrain-logloss:0.087601\teval-logloss:0.083385\n",
      "[221]\ttrain-logloss:0.087578\teval-logloss:0.08338\n",
      "[222]\ttrain-logloss:0.087561\teval-logloss:0.08338\n",
      "[223]\ttrain-logloss:0.087537\teval-logloss:0.083383\n",
      "[224]\ttrain-logloss:0.087516\teval-logloss:0.083382\n",
      "[225]\ttrain-logloss:0.087506\teval-logloss:0.083394\n",
      "[226]\ttrain-logloss:0.087479\teval-logloss:0.083401\n",
      "[227]\ttrain-logloss:0.087469\teval-logloss:0.083404\n",
      "[228]\ttrain-logloss:0.08744\teval-logloss:0.083402\n",
      "[229]\ttrain-logloss:0.08742\teval-logloss:0.083398\n",
      "[230]\ttrain-logloss:0.087398\teval-logloss:0.083392\n",
      "[231]\ttrain-logloss:0.087381\teval-logloss:0.083391\n",
      "[232]\ttrain-logloss:0.087366\teval-logloss:0.083388\n",
      "[233]\ttrain-logloss:0.087355\teval-logloss:0.083382\n",
      "[234]\ttrain-logloss:0.087332\teval-logloss:0.08338\n",
      "[235]\ttrain-logloss:0.087314\teval-logloss:0.083378\n",
      "[236]\ttrain-logloss:0.087301\teval-logloss:0.083378\n",
      "[237]\ttrain-logloss:0.087283\teval-logloss:0.08338\n",
      "[238]\ttrain-logloss:0.087265\teval-logloss:0.083382\n",
      "[239]\ttrain-logloss:0.087239\teval-logloss:0.08339\n",
      "[240]\ttrain-logloss:0.087216\teval-logloss:0.083391\n",
      "[241]\ttrain-logloss:0.087197\teval-logloss:0.083388\n",
      "[242]\ttrain-logloss:0.08718\teval-logloss:0.083388\n",
      "[243]\ttrain-logloss:0.087169\teval-logloss:0.083409\n",
      "[244]\ttrain-logloss:0.087153\teval-logloss:0.083408\n",
      "[245]\ttrain-logloss:0.087136\teval-logloss:0.083408\n",
      "[246]\ttrain-logloss:0.087118\teval-logloss:0.083409\n",
      "[247]\ttrain-logloss:0.087092\teval-logloss:0.083416\n",
      "[248]\ttrain-logloss:0.087081\teval-logloss:0.083417\n",
      "[249]\ttrain-logloss:0.087068\teval-logloss:0.083415\n",
      "[250]\ttrain-logloss:0.08705\teval-logloss:0.083411\n",
      "[251]\ttrain-logloss:0.087026\teval-logloss:0.083408\n",
      "[252]\ttrain-logloss:0.087011\teval-logloss:0.083401\n",
      "[253]\ttrain-logloss:0.086991\teval-logloss:0.083401\n",
      "[254]\ttrain-logloss:0.086984\teval-logloss:0.083399\n",
      "[255]\ttrain-logloss:0.086958\teval-logloss:0.083393\n",
      "[256]\ttrain-logloss:0.086938\teval-logloss:0.083394\n",
      "[257]\ttrain-logloss:0.086928\teval-logloss:0.083393\n",
      "[258]\ttrain-logloss:0.086908\teval-logloss:0.08339\n",
      "[259]\ttrain-logloss:0.08689\teval-logloss:0.083396\n",
      "[260]\ttrain-logloss:0.086873\teval-logloss:0.083392\n",
      "[261]\ttrain-logloss:0.086861\teval-logloss:0.083394\n",
      "[262]\ttrain-logloss:0.086844\teval-logloss:0.083387\n",
      "[263]\ttrain-logloss:0.086827\teval-logloss:0.083389\n",
      "[264]\ttrain-logloss:0.086803\teval-logloss:0.083391\n",
      "[265]\ttrain-logloss:0.086791\teval-logloss:0.083392\n",
      "[266]\ttrain-logloss:0.086774\teval-logloss:0.083392\n",
      "[267]\ttrain-logloss:0.086756\teval-logloss:0.083393\n",
      "[268]\ttrain-logloss:0.086736\teval-logloss:0.083388\n",
      "[269]\ttrain-logloss:0.086723\teval-logloss:0.083382\n",
      "[270]\ttrain-logloss:0.08671\teval-logloss:0.083383\n",
      "[271]\ttrain-logloss:0.086691\teval-logloss:0.083395\n",
      "[272]\ttrain-logloss:0.086672\teval-logloss:0.083401\n",
      "[273]\ttrain-logloss:0.086653\teval-logloss:0.083398\n",
      "[274]\ttrain-logloss:0.086633\teval-logloss:0.083406\n",
      "[275]\ttrain-logloss:0.086615\teval-logloss:0.083402\n",
      "[276]\ttrain-logloss:0.0866\teval-logloss:0.0834\n",
      "[277]\ttrain-logloss:0.086588\teval-logloss:0.083405\n",
      "[278]\ttrain-logloss:0.086565\teval-logloss:0.083405\n",
      "[279]\ttrain-logloss:0.086558\teval-logloss:0.083404\n",
      "[280]\ttrain-logloss:0.086544\teval-logloss:0.083398\n",
      "[281]\ttrain-logloss:0.086525\teval-logloss:0.083402\n",
      "[282]\ttrain-logloss:0.086505\teval-logloss:0.083396\n",
      "[283]\ttrain-logloss:0.086494\teval-logloss:0.083399\n",
      "[284]\ttrain-logloss:0.086474\teval-logloss:0.083401\n",
      "[285]\ttrain-logloss:0.086454\teval-logloss:0.083398\n",
      "[286]\ttrain-logloss:0.086441\teval-logloss:0.083401\n",
      "[287]\ttrain-logloss:0.086419\teval-logloss:0.083399\n",
      "[288]\ttrain-logloss:0.0864\teval-logloss:0.083402\n",
      "[289]\ttrain-logloss:0.086382\teval-logloss:0.083402\n",
      "[290]\ttrain-logloss:0.086364\teval-logloss:0.083404\n",
      "[291]\ttrain-logloss:0.086347\teval-logloss:0.083398\n",
      "[292]\ttrain-logloss:0.086338\teval-logloss:0.083398\n",
      "[293]\ttrain-logloss:0.086319\teval-logloss:0.083404\n",
      "[294]\ttrain-logloss:0.086306\teval-logloss:0.083397\n",
      "[295]\ttrain-logloss:0.086289\teval-logloss:0.083395\n",
      "[296]\ttrain-logloss:0.086268\teval-logloss:0.083406\n",
      "[297]\ttrain-logloss:0.086255\teval-logloss:0.083411\n",
      "[298]\ttrain-logloss:0.086233\teval-logloss:0.083408\n",
      "[299]\ttrain-logloss:0.086216\teval-logloss:0.083414\n",
      "[300]\ttrain-logloss:0.086193\teval-logloss:0.083416\n",
      "[301]\ttrain-logloss:0.086171\teval-logloss:0.083416\n",
      "[302]\ttrain-logloss:0.086165\teval-logloss:0.083416\n",
      "[303]\ttrain-logloss:0.086144\teval-logloss:0.083419\n",
      "[304]\ttrain-logloss:0.086135\teval-logloss:0.083414\n",
      "[305]\ttrain-logloss:0.086111\teval-logloss:0.083413\n",
      "[306]\ttrain-logloss:0.086088\teval-logloss:0.083411\n",
      "[307]\ttrain-logloss:0.086073\teval-logloss:0.083409\n",
      "[308]\ttrain-logloss:0.086062\teval-logloss:0.083407\n",
      "[309]\ttrain-logloss:0.08604\teval-logloss:0.083411\n",
      "[310]\ttrain-logloss:0.086032\teval-logloss:0.083422\n",
      "[311]\ttrain-logloss:0.086021\teval-logloss:0.083427\n",
      "[312]\ttrain-logloss:0.086\teval-logloss:0.083426\n",
      "[313]\ttrain-logloss:0.085983\teval-logloss:0.083428\n",
      "[314]\ttrain-logloss:0.085972\teval-logloss:0.08343\n",
      "[315]\ttrain-logloss:0.08595\teval-logloss:0.083429\n",
      "[316]\ttrain-logloss:0.085935\teval-logloss:0.083429\n",
      "[317]\ttrain-logloss:0.085922\teval-logloss:0.08343\n",
      "[318]\ttrain-logloss:0.085898\teval-logloss:0.083433\n",
      "[319]\ttrain-logloss:0.08588\teval-logloss:0.083436\n",
      "[320]\ttrain-logloss:0.085859\teval-logloss:0.083443\n",
      "[321]\ttrain-logloss:0.085842\teval-logloss:0.083451\n",
      "[322]\ttrain-logloss:0.085822\teval-logloss:0.083448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[323]\ttrain-logloss:0.085802\teval-logloss:0.083448\n",
      "[324]\ttrain-logloss:0.085786\teval-logloss:0.08346\n",
      "[325]\ttrain-logloss:0.085777\teval-logloss:0.083459\n",
      "[326]\ttrain-logloss:0.085757\teval-logloss:0.083462\n",
      "[327]\ttrain-logloss:0.085745\teval-logloss:0.083462\n",
      "[328]\ttrain-logloss:0.085726\teval-logloss:0.083461\n",
      "[329]\ttrain-logloss:0.085703\teval-logloss:0.083462\n",
      "[330]\ttrain-logloss:0.085684\teval-logloss:0.083464\n",
      "[331]\ttrain-logloss:0.085673\teval-logloss:0.083468\n",
      "[332]\ttrain-logloss:0.085662\teval-logloss:0.083469\n",
      "[333]\ttrain-logloss:0.085647\teval-logloss:0.083468\n",
      "[334]\ttrain-logloss:0.085638\teval-logloss:0.083465\n",
      "[335]\ttrain-logloss:0.085623\teval-logloss:0.083466\n",
      "[336]\ttrain-logloss:0.0856\teval-logloss:0.083478\n",
      "[337]\ttrain-logloss:0.085584\teval-logloss:0.083475\n",
      "[338]\ttrain-logloss:0.085568\teval-logloss:0.083489\n",
      "[339]\ttrain-logloss:0.085543\teval-logloss:0.083483\n",
      "[340]\ttrain-logloss:0.085528\teval-logloss:0.083478\n",
      "[341]\ttrain-logloss:0.085505\teval-logloss:0.08348\n",
      "[342]\ttrain-logloss:0.085492\teval-logloss:0.083478\n",
      "[343]\ttrain-logloss:0.085484\teval-logloss:0.083477\n",
      "[344]\ttrain-logloss:0.08546\teval-logloss:0.083479\n",
      "[345]\ttrain-logloss:0.085438\teval-logloss:0.083487\n",
      "[346]\ttrain-logloss:0.085423\teval-logloss:0.083489\n",
      "[347]\ttrain-logloss:0.085409\teval-logloss:0.083489\n",
      "[348]\ttrain-logloss:0.085385\teval-logloss:0.083488\n",
      "[349]\ttrain-logloss:0.085365\teval-logloss:0.083488\n",
      "[350]\ttrain-logloss:0.085345\teval-logloss:0.083484\n",
      "[351]\ttrain-logloss:0.085331\teval-logloss:0.083477\n",
      "[352]\ttrain-logloss:0.08532\teval-logloss:0.083477\n",
      "[353]\ttrain-logloss:0.0853\teval-logloss:0.083476\n",
      "[354]\ttrain-logloss:0.085285\teval-logloss:0.083478\n",
      "[355]\ttrain-logloss:0.085264\teval-logloss:0.08348\n",
      "[356]\ttrain-logloss:0.085249\teval-logloss:0.083478\n",
      "[357]\ttrain-logloss:0.085232\teval-logloss:0.083471\n",
      "[358]\ttrain-logloss:0.085218\teval-logloss:0.083477\n",
      "[359]\ttrain-logloss:0.085202\teval-logloss:0.083473\n",
      "[360]\ttrain-logloss:0.085193\teval-logloss:0.083474\n",
      "[361]\ttrain-logloss:0.08518\teval-logloss:0.083477\n",
      "[362]\ttrain-logloss:0.085161\teval-logloss:0.083483\n",
      "[363]\ttrain-logloss:0.085146\teval-logloss:0.083483\n",
      "[364]\ttrain-logloss:0.085137\teval-logloss:0.083511\n",
      "[365]\ttrain-logloss:0.085123\teval-logloss:0.083504\n",
      "[366]\ttrain-logloss:0.085113\teval-logloss:0.083503\n",
      "[367]\ttrain-logloss:0.085091\teval-logloss:0.083504\n",
      "[368]\ttrain-logloss:0.085081\teval-logloss:0.083499\n",
      "[369]\ttrain-logloss:0.085069\teval-logloss:0.083506\n",
      "[370]\ttrain-logloss:0.085061\teval-logloss:0.083505\n",
      "[371]\ttrain-logloss:0.085049\teval-logloss:0.083504\n",
      "[372]\ttrain-logloss:0.085032\teval-logloss:0.083505\n",
      "[373]\ttrain-logloss:0.085015\teval-logloss:0.083508\n",
      "[374]\ttrain-logloss:0.084999\teval-logloss:0.083508\n",
      "[375]\ttrain-logloss:0.084976\teval-logloss:0.083511\n",
      "[376]\ttrain-logloss:0.084967\teval-logloss:0.08352\n",
      "[377]\ttrain-logloss:0.084951\teval-logloss:0.08352\n",
      "[378]\ttrain-logloss:0.084939\teval-logloss:0.083524\n",
      "[379]\ttrain-logloss:0.084927\teval-logloss:0.083524\n",
      "[380]\ttrain-logloss:0.084917\teval-logloss:0.083528\n",
      "[381]\ttrain-logloss:0.084896\teval-logloss:0.083519\n",
      "[382]\ttrain-logloss:0.084877\teval-logloss:0.083514\n",
      "[383]\ttrain-logloss:0.084853\teval-logloss:0.083521\n",
      "[384]\ttrain-logloss:0.084836\teval-logloss:0.083525\n",
      "[385]\ttrain-logloss:0.084825\teval-logloss:0.083522\n",
      "[386]\ttrain-logloss:0.084805\teval-logloss:0.083522\n",
      "[387]\ttrain-logloss:0.084781\teval-logloss:0.083519\n",
      "[388]\ttrain-logloss:0.084761\teval-logloss:0.083521\n",
      "[389]\ttrain-logloss:0.084748\teval-logloss:0.083523\n",
      "[390]\ttrain-logloss:0.084737\teval-logloss:0.08352\n",
      "[391]\ttrain-logloss:0.084715\teval-logloss:0.083526\n",
      "[392]\ttrain-logloss:0.084696\teval-logloss:0.083532\n",
      "[393]\ttrain-logloss:0.084682\teval-logloss:0.08353\n",
      "[394]\ttrain-logloss:0.084665\teval-logloss:0.083526\n",
      "[395]\ttrain-logloss:0.084655\teval-logloss:0.083529\n",
      "[396]\ttrain-logloss:0.084639\teval-logloss:0.083536\n",
      "[397]\ttrain-logloss:0.084623\teval-logloss:0.083532\n",
      "[398]\ttrain-logloss:0.08461\teval-logloss:0.083533\n",
      "[399]\ttrain-logloss:0.084589\teval-logloss:0.083538\n",
      "[400]\ttrain-logloss:0.084577\teval-logloss:0.083541\n",
      "[401]\ttrain-logloss:0.084561\teval-logloss:0.08354\n",
      "[402]\ttrain-logloss:0.08455\teval-logloss:0.083536\n",
      "[403]\ttrain-logloss:0.08453\teval-logloss:0.083536\n",
      "[404]\ttrain-logloss:0.084515\teval-logloss:0.083538\n",
      "[405]\ttrain-logloss:0.084493\teval-logloss:0.083542\n",
      "[406]\ttrain-logloss:0.084469\teval-logloss:0.083539\n",
      "[407]\ttrain-logloss:0.084448\teval-logloss:0.08354\n",
      "[408]\ttrain-logloss:0.084428\teval-logloss:0.083539\n",
      "[409]\ttrain-logloss:0.084415\teval-logloss:0.083538\n",
      "[410]\ttrain-logloss:0.084402\teval-logloss:0.083534\n",
      "[411]\ttrain-logloss:0.084392\teval-logloss:0.083533\n",
      "[412]\ttrain-logloss:0.084375\teval-logloss:0.083533\n",
      "[413]\ttrain-logloss:0.084366\teval-logloss:0.08353\n",
      "[414]\ttrain-logloss:0.084344\teval-logloss:0.083537\n",
      "[415]\ttrain-logloss:0.084331\teval-logloss:0.083535\n",
      "[416]\ttrain-logloss:0.084317\teval-logloss:0.083534\n",
      "[417]\ttrain-logloss:0.084299\teval-logloss:0.083533\n",
      "[418]\ttrain-logloss:0.084283\teval-logloss:0.083535\n",
      "[419]\ttrain-logloss:0.084267\teval-logloss:0.083527\n",
      "[420]\ttrain-logloss:0.084245\teval-logloss:0.083528\n",
      "[421]\ttrain-logloss:0.084224\teval-logloss:0.083533\n",
      "[422]\ttrain-logloss:0.084214\teval-logloss:0.083534\n",
      "[423]\ttrain-logloss:0.0842\teval-logloss:0.083534\n",
      "[424]\ttrain-logloss:0.084186\teval-logloss:0.083555\n",
      "[425]\ttrain-logloss:0.084165\teval-logloss:0.083553\n",
      "[426]\ttrain-logloss:0.084153\teval-logloss:0.083549\n",
      "[427]\ttrain-logloss:0.084142\teval-logloss:0.08355\n",
      "[428]\ttrain-logloss:0.084131\teval-logloss:0.083553\n",
      "[429]\ttrain-logloss:0.084116\teval-logloss:0.083563\n",
      "[430]\ttrain-logloss:0.084098\teval-logloss:0.083569\n",
      "[431]\ttrain-logloss:0.084087\teval-logloss:0.083568\n",
      "[432]\ttrain-logloss:0.084068\teval-logloss:0.08357\n",
      "[433]\ttrain-logloss:0.084046\teval-logloss:0.083579\n",
      "[434]\ttrain-logloss:0.084037\teval-logloss:0.083584\n",
      "[435]\ttrain-logloss:0.084019\teval-logloss:0.08359\n",
      "Stopping. Best iteration:\n",
      "[235]\ttrain-logloss:0.087314\teval-logloss:0.083378\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# obj、feval、early_stopping_rounds、evals_result、verbose_eval、xgb_model\n",
    "\n",
    "bst = xgb.train(param, train, num_boost_round, evals=watchlist, obj=None, feval=None, maximize=False,\n",
    "      early_stopping_rounds=200, evals_result=None, verbose_eval=True, xgb_model=None, callbacks=None,learning_rates=None)\n",
    "bst.save_model('data/model_xgboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importance = bst.get_fscore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importance = sorted(importance,key=lambda s: s[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f 2 : 2 _ 7 2 5 8 0 1 5 8 8 5 2 1 5 9 1 4 7 3 6',\n",
       " 'f 1 0 - r a t i o',\n",
       " 'f 1 9',\n",
       " 'f 1 - f 5 - r a t i o',\n",
       " 'f 2 : 3 _ 6 2 3 3 6 6 9 1 7 7 1 6 6 5 3 8 6 2 8',\n",
       " 'f 0',\n",
       " 'f 4 - f 5 - r a t i o',\n",
       " 'f 6',\n",
       " 'f 1 - c o u n t',\n",
       " 'f 1 - r a t i o',\n",
       " 'f 4 - r a t i o',\n",
       " 'f 1 5',\n",
       " 'f 1 6',\n",
       " 'f 1',\n",
       " 'f 1 0',\n",
       " 'f 2 4',\n",
       " 'f 1 - f 1 0 - b r o w s e - c o u n t',\n",
       " 'f 5 - r a t i o',\n",
       " 'f 1 9 - r a t i o',\n",
       " 'f 5',\n",
       " 'f 5 - f 1 9 - r a t i o',\n",
       " 'f 4 - f 1 9 - r a t i o',\n",
       " 'f 4',\n",
       " 'f 1 4',\n",
       " 'f 5 - f 1 0 - b r o w s e - c o u n t',\n",
       " 'f 4 - f 5 - b r o w s e - c o u n t',\n",
       " 'f 2 1',\n",
       " 'f 2 3',\n",
       " 'f 2 0',\n",
       " 'f 1 - b r o w s e - c o u n t',\n",
       " 'f 4 - f 5 - c o u n t',\n",
       " 'f 2 : 3 _ 8 8 6 8 8 8 7 6 6 1 1 8 6 4 1 9 2 2 9',\n",
       " 'f 2 5',\n",
       " 'f 1 2',\n",
       " 'f 1 6 : s e c o n d',\n",
       " 'f 2 : 2 _ 4 8 7 9 7 2 1 0 2 4 9 8 0 9 4 5 5 9 2',\n",
       " 'f 1 6 : m i n u t e',\n",
       " 'f 1 0 - c o u n t',\n",
       " 'f 1 7',\n",
       " 'f 1 0 - b r o w s e - c o u n t',\n",
       " 'f 2 : 2 _ 1 9 6 8 0 5 6 1 0 0 2 6 9 7 6 0 7 2 9',\n",
       " 'f 1 6 : h o u r',\n",
       " 'f 7',\n",
       " 'f 2 : 2 _ 2 6 4 2 1 7 5 4 5 3 1 5 1 8 0 5 5 6 6',\n",
       " 'f 2 : 2 _ 2 4 3 6 7 1 5 2 8 5 0 9 3 4 8 7 5 8 4',\n",
       " 'f 4 - f 1 0 - b r o w s e - c o u n t',\n",
       " 'f 1 - f 5 - b r o w s e - c o u n t',\n",
       " 'f 8',\n",
       " 'f 1 3 : 2 0 0 2 . 0',\n",
       " 'f 4 - b r o w s e - c o u n t',\n",
       " 'f 2 : 2 _ 3 2 0 3 6 7 3 9 7 9 1 3 8 7 6 3 5 9 5',\n",
       " 'f 4 - c o u n t',\n",
       " 'f 4 - f 1 9 - c o u n t',\n",
       " 'f 1 1 : w o m a n',\n",
       " 'f 9',\n",
       " 'f 1 0 - f 1 9 - b r o w s e - c o u n t',\n",
       " 'f 5 - b r o w s e - c o u n t',\n",
       " 'f 2 2',\n",
       " 'f 2 : 2 _ 5 7 9 9 3 4 7 0 6 7 9 8 2 5 5 6 5 2 0',\n",
       " 'f 1 - f 4 - c o u n t',\n",
       " 'f 1 9 - c o u n t',\n",
       " 'f 4 - f 1 9 - b r o w s e - c o u n t',\n",
       " 'f 1 1 : m a n',\n",
       " 'f 2 : 2 _ 8 7 1 0 7 3 9 1 8 0 2 0 0 0 0 9 1 2 8',\n",
       " 'f 1 9 - b r o w s e - c o u n t',\n",
       " 'f 5 - c o u n t',\n",
       " 'f 1 - f 5 - c o u n t',\n",
       " 'f 2 : 2 _ 8 2 7 7 3 3 6 0 7 6 2 7 6 1 8 4 2 7 2',\n",
       " 'f 1 3 : 2 0 0 5 . 0',\n",
       " 'f 5 - f 1 9 - b r o w s e - c o u n t',\n",
       " 'f 1 - f 4 - r a t i o',\n",
       " 'f 1 3 : 2 0 0 3 . 0',\n",
       " 'f 1 3 : 2 0 0 4 . 0',\n",
       " 'f 5 - f 1 9 - c o u n t',\n",
       " 'f 2 : 2 _ 5 7 5 5 6 9 4 4 0 7 6 8 4 6 0 2 2 9 6',\n",
       " 'f 1 1 : f a m i l y',\n",
       " 'f 2 : 2 _ 5 0 9 6 6 0 0 9 5 5 3 0 1 3 4 7 6 8',\n",
       " 'f 1 - f 4 - b r o w s e - c o u n t',\n",
       " 'f 1 1 : o t h e r',\n",
       " 'f 2 : 2 _ 2 0 1 1 9 8 1 5 7 3 0 6 1 4 4 7 2 0 8']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 预测\n",
    "label_test_xgb = bst.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4.保存\n",
    "np.savetxt('data/round1_result_xgb.txt', np.c_[data_index, label_test_xgb], delimiter=',', header='instance_id predicted_score',comments='', fmt='%s %f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.581931454242\n"
     ]
    }
   ],
   "source": [
    "clf = AdaBoostClassifier() #迭代100次  \n",
    "clf.fit(feature_train,label_train)\n",
    "label_val_lr = clf.predict(feature_val)  \n",
    "print(log_loss(label_val,label_val_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.训练Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.584341164082\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier()\n",
    "clf.fit(feature_train , label_train)\n",
    "label_val_rf = clf.predict(feature_val)    \n",
    "print(log_loss(label_val,label_val_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.lightbgm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's binary_logloss: 0.0855711\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.0855023\n",
      "[3]\tvalid_0's binary_logloss: 0.085435\n",
      "[4]\tvalid_0's binary_logloss: 0.0853771\n",
      "[5]\tvalid_0's binary_logloss: 0.0853116\n",
      "[6]\tvalid_0's binary_logloss: 0.0852492\n",
      "[7]\tvalid_0's binary_logloss: 0.0851836\n",
      "[8]\tvalid_0's binary_logloss: 0.0851136\n",
      "[9]\tvalid_0's binary_logloss: 0.085054\n",
      "[10]\tvalid_0's binary_logloss: 0.0849983\n",
      "[11]\tvalid_0's binary_logloss: 0.0849607\n",
      "[12]\tvalid_0's binary_logloss: 0.0849113\n",
      "[13]\tvalid_0's binary_logloss: 0.0848758\n",
      "[14]\tvalid_0's binary_logloss: 0.0848263\n",
      "[15]\tvalid_0's binary_logloss: 0.0847919\n",
      "[16]\tvalid_0's binary_logloss: 0.0847416\n",
      "[17]\tvalid_0's binary_logloss: 0.0846964\n",
      "[18]\tvalid_0's binary_logloss: 0.0846587\n",
      "[19]\tvalid_0's binary_logloss: 0.0846124\n",
      "[20]\tvalid_0's binary_logloss: 0.0845807\n",
      "[21]\tvalid_0's binary_logloss: 0.084534\n",
      "[22]\tvalid_0's binary_logloss: 0.0845046\n",
      "[23]\tvalid_0's binary_logloss: 0.0844671\n",
      "[24]\tvalid_0's binary_logloss: 0.0844365\n",
      "[25]\tvalid_0's binary_logloss: 0.0844056\n",
      "[26]\tvalid_0's binary_logloss: 0.0843696\n",
      "[27]\tvalid_0's binary_logloss: 0.0843404\n",
      "[28]\tvalid_0's binary_logloss: 0.0843055\n",
      "[29]\tvalid_0's binary_logloss: 0.0842781\n",
      "[30]\tvalid_0's binary_logloss: 0.0842399\n",
      "[31]\tvalid_0's binary_logloss: 0.0842113\n",
      "[32]\tvalid_0's binary_logloss: 0.0841908\n",
      "[33]\tvalid_0's binary_logloss: 0.08416\n",
      "[34]\tvalid_0's binary_logloss: 0.0841297\n",
      "[35]\tvalid_0's binary_logloss: 0.0841057\n",
      "[36]\tvalid_0's binary_logloss: 0.084079\n",
      "[37]\tvalid_0's binary_logloss: 0.0840496\n",
      "[38]\tvalid_0's binary_logloss: 0.0840218\n",
      "[39]\tvalid_0's binary_logloss: 0.0839952\n",
      "[40]\tvalid_0's binary_logloss: 0.0839711\n",
      "[41]\tvalid_0's binary_logloss: 0.0839479\n",
      "[42]\tvalid_0's binary_logloss: 0.0839223\n",
      "[43]\tvalid_0's binary_logloss: 0.0838985\n",
      "[44]\tvalid_0's binary_logloss: 0.083877\n",
      "[45]\tvalid_0's binary_logloss: 0.0838566\n",
      "[46]\tvalid_0's binary_logloss: 0.0838341\n",
      "[47]\tvalid_0's binary_logloss: 0.083812\n",
      "[48]\tvalid_0's binary_logloss: 0.0837924\n",
      "[49]\tvalid_0's binary_logloss: 0.0837699\n",
      "[50]\tvalid_0's binary_logloss: 0.0837512\n",
      "[51]\tvalid_0's binary_logloss: 0.0837283\n",
      "[52]\tvalid_0's binary_logloss: 0.0837088\n",
      "[53]\tvalid_0's binary_logloss: 0.0836852\n",
      "[54]\tvalid_0's binary_logloss: 0.0836684\n",
      "[55]\tvalid_0's binary_logloss: 0.083646\n",
      "[56]\tvalid_0's binary_logloss: 0.0836291\n",
      "[57]\tvalid_0's binary_logloss: 0.0836153\n",
      "[58]\tvalid_0's binary_logloss: 0.0836003\n",
      "[59]\tvalid_0's binary_logloss: 0.0835786\n",
      "[60]\tvalid_0's binary_logloss: 0.0835615\n",
      "[61]\tvalid_0's binary_logloss: 0.0835447\n",
      "[62]\tvalid_0's binary_logloss: 0.0835272\n",
      "[63]\tvalid_0's binary_logloss: 0.0835083\n",
      "[64]\tvalid_0's binary_logloss: 0.0834929\n",
      "[65]\tvalid_0's binary_logloss: 0.0834753\n",
      "[66]\tvalid_0's binary_logloss: 0.0834581\n",
      "[67]\tvalid_0's binary_logloss: 0.0834451\n",
      "[68]\tvalid_0's binary_logloss: 0.083431\n",
      "[69]\tvalid_0's binary_logloss: 0.0834118\n",
      "[70]\tvalid_0's binary_logloss: 0.0833983\n",
      "[71]\tvalid_0's binary_logloss: 0.0833814\n",
      "[72]\tvalid_0's binary_logloss: 0.0833686\n",
      "[73]\tvalid_0's binary_logloss: 0.0833518\n",
      "[74]\tvalid_0's binary_logloss: 0.0833368\n",
      "[75]\tvalid_0's binary_logloss: 0.0833239\n",
      "[76]\tvalid_0's binary_logloss: 0.0833111\n",
      "[77]\tvalid_0's binary_logloss: 0.0833009\n",
      "[78]\tvalid_0's binary_logloss: 0.0832835\n",
      "[79]\tvalid_0's binary_logloss: 0.083268\n",
      "[80]\tvalid_0's binary_logloss: 0.0832588\n",
      "[81]\tvalid_0's binary_logloss: 0.0832446\n",
      "[82]\tvalid_0's binary_logloss: 0.0832278\n",
      "[83]\tvalid_0's binary_logloss: 0.0832168\n",
      "[84]\tvalid_0's binary_logloss: 0.0831961\n",
      "[85]\tvalid_0's binary_logloss: 0.083184\n",
      "[86]\tvalid_0's binary_logloss: 0.0831646\n",
      "[87]\tvalid_0's binary_logloss: 0.0831535\n",
      "[88]\tvalid_0's binary_logloss: 0.0831423\n",
      "[89]\tvalid_0's binary_logloss: 0.0831281\n",
      "[90]\tvalid_0's binary_logloss: 0.0831203\n",
      "[91]\tvalid_0's binary_logloss: 0.0831045\n",
      "[92]\tvalid_0's binary_logloss: 0.0830965\n",
      "[93]\tvalid_0's binary_logloss: 0.0830892\n",
      "[94]\tvalid_0's binary_logloss: 0.0830782\n",
      "[95]\tvalid_0's binary_logloss: 0.0830691\n",
      "[96]\tvalid_0's binary_logloss: 0.0830611\n",
      "[97]\tvalid_0's binary_logloss: 0.0830472\n",
      "[98]\tvalid_0's binary_logloss: 0.083034\n",
      "[99]\tvalid_0's binary_logloss: 0.0830297\n",
      "[100]\tvalid_0's binary_logloss: 0.0830179\n",
      "[101]\tvalid_0's binary_logloss: 0.0830053\n",
      "[102]\tvalid_0's binary_logloss: 0.0829991\n",
      "[103]\tvalid_0's binary_logloss: 0.0829888\n",
      "[104]\tvalid_0's binary_logloss: 0.0829776\n",
      "[105]\tvalid_0's binary_logloss: 0.0829706\n",
      "[106]\tvalid_0's binary_logloss: 0.0829561\n",
      "[107]\tvalid_0's binary_logloss: 0.082947\n",
      "[108]\tvalid_0's binary_logloss: 0.0829317\n",
      "[109]\tvalid_0's binary_logloss: 0.0829199\n",
      "[110]\tvalid_0's binary_logloss: 0.082909\n",
      "[111]\tvalid_0's binary_logloss: 0.0828984\n",
      "[112]\tvalid_0's binary_logloss: 0.0828852\n",
      "[113]\tvalid_0's binary_logloss: 0.0828729\n",
      "[114]\tvalid_0's binary_logloss: 0.082868\n",
      "[115]\tvalid_0's binary_logloss: 0.0828595\n",
      "[116]\tvalid_0's binary_logloss: 0.0828494\n",
      "[117]\tvalid_0's binary_logloss: 0.082841\n",
      "[118]\tvalid_0's binary_logloss: 0.0828329\n",
      "[119]\tvalid_0's binary_logloss: 0.0828194\n",
      "[120]\tvalid_0's binary_logloss: 0.0828109\n",
      "[121]\tvalid_0's binary_logloss: 0.0828074\n",
      "[122]\tvalid_0's binary_logloss: 0.0827975\n",
      "[123]\tvalid_0's binary_logloss: 0.0827911\n",
      "[124]\tvalid_0's binary_logloss: 0.082785\n",
      "[125]\tvalid_0's binary_logloss: 0.0827742\n",
      "[126]\tvalid_0's binary_logloss: 0.0827711\n",
      "[127]\tvalid_0's binary_logloss: 0.0827655\n",
      "[128]\tvalid_0's binary_logloss: 0.0827588\n",
      "[129]\tvalid_0's binary_logloss: 0.0827507\n",
      "[130]\tvalid_0's binary_logloss: 0.0827387\n",
      "[131]\tvalid_0's binary_logloss: 0.082736\n",
      "[132]\tvalid_0's binary_logloss: 0.0827291\n",
      "[133]\tvalid_0's binary_logloss: 0.0827177\n",
      "[134]\tvalid_0's binary_logloss: 0.0827117\n",
      "[135]\tvalid_0's binary_logloss: 0.082706\n",
      "[136]\tvalid_0's binary_logloss: 0.0826948\n",
      "[137]\tvalid_0's binary_logloss: 0.0826881\n",
      "[138]\tvalid_0's binary_logloss: 0.0826777\n",
      "[139]\tvalid_0's binary_logloss: 0.0826677\n",
      "[140]\tvalid_0's binary_logloss: 0.0826632\n",
      "[141]\tvalid_0's binary_logloss: 0.0826582\n",
      "[142]\tvalid_0's binary_logloss: 0.0826497\n",
      "[143]\tvalid_0's binary_logloss: 0.0826441\n",
      "[144]\tvalid_0's binary_logloss: 0.0826389\n",
      "[145]\tvalid_0's binary_logloss: 0.0826328\n",
      "[146]\tvalid_0's binary_logloss: 0.0826257\n",
      "[147]\tvalid_0's binary_logloss: 0.0826168\n",
      "[148]\tvalid_0's binary_logloss: 0.0826093\n",
      "[149]\tvalid_0's binary_logloss: 0.0825993\n",
      "[150]\tvalid_0's binary_logloss: 0.082597\n",
      "[151]\tvalid_0's binary_logloss: 0.0825912\n",
      "[152]\tvalid_0's binary_logloss: 0.0825869\n",
      "[153]\tvalid_0's binary_logloss: 0.0825811\n",
      "[154]\tvalid_0's binary_logloss: 0.0825756\n",
      "[155]\tvalid_0's binary_logloss: 0.0825704\n",
      "[156]\tvalid_0's binary_logloss: 0.0825643\n",
      "[157]\tvalid_0's binary_logloss: 0.0825578\n",
      "[158]\tvalid_0's binary_logloss: 0.0825521\n",
      "[159]\tvalid_0's binary_logloss: 0.0825471\n",
      "[160]\tvalid_0's binary_logloss: 0.0825425\n",
      "[161]\tvalid_0's binary_logloss: 0.0825387\n",
      "[162]\tvalid_0's binary_logloss: 0.0825324\n",
      "[163]\tvalid_0's binary_logloss: 0.0825281\n",
      "[164]\tvalid_0's binary_logloss: 0.0825196\n",
      "[165]\tvalid_0's binary_logloss: 0.0825137\n",
      "[166]\tvalid_0's binary_logloss: 0.0825074\n",
      "[167]\tvalid_0's binary_logloss: 0.0825022\n",
      "[168]\tvalid_0's binary_logloss: 0.0824953\n",
      "[169]\tvalid_0's binary_logloss: 0.082487\n",
      "[170]\tvalid_0's binary_logloss: 0.0824838\n",
      "[171]\tvalid_0's binary_logloss: 0.0824793\n",
      "[172]\tvalid_0's binary_logloss: 0.0824744\n",
      "[173]\tvalid_0's binary_logloss: 0.0824708\n",
      "[174]\tvalid_0's binary_logloss: 0.082463\n",
      "[175]\tvalid_0's binary_logloss: 0.0824558\n",
      "[176]\tvalid_0's binary_logloss: 0.0824522\n",
      "[177]\tvalid_0's binary_logloss: 0.0824477\n",
      "[178]\tvalid_0's binary_logloss: 0.0824435\n",
      "[179]\tvalid_0's binary_logloss: 0.0824391\n",
      "[180]\tvalid_0's binary_logloss: 0.0824332\n",
      "[181]\tvalid_0's binary_logloss: 0.0824276\n",
      "[182]\tvalid_0's binary_logloss: 0.0824255\n",
      "[183]\tvalid_0's binary_logloss: 0.0824199\n",
      "[184]\tvalid_0's binary_logloss: 0.0824167\n",
      "[185]\tvalid_0's binary_logloss: 0.0824128\n",
      "[186]\tvalid_0's binary_logloss: 0.0824103\n",
      "[187]\tvalid_0's binary_logloss: 0.0824071\n",
      "[188]\tvalid_0's binary_logloss: 0.0824075\n",
      "[189]\tvalid_0's binary_logloss: 0.0823919\n",
      "[190]\tvalid_0's binary_logloss: 0.0823864\n",
      "[191]\tvalid_0's binary_logloss: 0.0823873\n",
      "[192]\tvalid_0's binary_logloss: 0.0823873\n",
      "[193]\tvalid_0's binary_logloss: 0.0823743\n",
      "[194]\tvalid_0's binary_logloss: 0.0823716\n",
      "[195]\tvalid_0's binary_logloss: 0.0823686\n",
      "[196]\tvalid_0's binary_logloss: 0.0823653\n",
      "[197]\tvalid_0's binary_logloss: 0.0823661\n",
      "[198]\tvalid_0's binary_logloss: 0.0823647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[199]\tvalid_0's binary_logloss: 0.0823638\n",
      "[200]\tvalid_0's binary_logloss: 0.0823614\n",
      "[201]\tvalid_0's binary_logloss: 0.0823569\n",
      "[202]\tvalid_0's binary_logloss: 0.082353\n",
      "[203]\tvalid_0's binary_logloss: 0.0823513\n",
      "[204]\tvalid_0's binary_logloss: 0.0823463\n",
      "[205]\tvalid_0's binary_logloss: 0.082346\n",
      "[206]\tvalid_0's binary_logloss: 0.0823319\n",
      "[207]\tvalid_0's binary_logloss: 0.0823313\n",
      "[208]\tvalid_0's binary_logloss: 0.0823276\n",
      "[209]\tvalid_0's binary_logloss: 0.0823253\n",
      "[210]\tvalid_0's binary_logloss: 0.0823281\n",
      "[211]\tvalid_0's binary_logloss: 0.0823262\n",
      "[212]\tvalid_0's binary_logloss: 0.0823262\n",
      "[213]\tvalid_0's binary_logloss: 0.0823235\n",
      "[214]\tvalid_0's binary_logloss: 0.0823169\n",
      "[215]\tvalid_0's binary_logloss: 0.0823192\n",
      "[216]\tvalid_0's binary_logloss: 0.0823189\n",
      "[217]\tvalid_0's binary_logloss: 0.0823165\n",
      "[218]\tvalid_0's binary_logloss: 0.0823126\n",
      "[219]\tvalid_0's binary_logloss: 0.0823101\n",
      "[220]\tvalid_0's binary_logloss: 0.082308\n",
      "[221]\tvalid_0's binary_logloss: 0.0823046\n",
      "[222]\tvalid_0's binary_logloss: 0.0823053\n",
      "[223]\tvalid_0's binary_logloss: 0.0822999\n",
      "[224]\tvalid_0's binary_logloss: 0.0822999\n",
      "[225]\tvalid_0's binary_logloss: 0.0822978\n",
      "[226]\tvalid_0's binary_logloss: 0.0822947\n",
      "[227]\tvalid_0's binary_logloss: 0.0822888\n",
      "[228]\tvalid_0's binary_logloss: 0.082287\n",
      "[229]\tvalid_0's binary_logloss: 0.082284\n",
      "[230]\tvalid_0's binary_logloss: 0.0822728\n",
      "[231]\tvalid_0's binary_logloss: 0.082272\n",
      "[232]\tvalid_0's binary_logloss: 0.082276\n",
      "[233]\tvalid_0's binary_logloss: 0.0822759\n",
      "[234]\tvalid_0's binary_logloss: 0.0822754\n",
      "[235]\tvalid_0's binary_logloss: 0.0822705\n",
      "[236]\tvalid_0's binary_logloss: 0.0822716\n",
      "[237]\tvalid_0's binary_logloss: 0.0822701\n",
      "[238]\tvalid_0's binary_logloss: 0.0822594\n",
      "[239]\tvalid_0's binary_logloss: 0.0822623\n",
      "[240]\tvalid_0's binary_logloss: 0.0822584\n",
      "[241]\tvalid_0's binary_logloss: 0.082257\n",
      "[242]\tvalid_0's binary_logloss: 0.0822511\n",
      "[243]\tvalid_0's binary_logloss: 0.082255\n",
      "[244]\tvalid_0's binary_logloss: 0.0822551\n",
      "[245]\tvalid_0's binary_logloss: 0.0822489\n",
      "[246]\tvalid_0's binary_logloss: 0.0822474\n",
      "[247]\tvalid_0's binary_logloss: 0.0822464\n",
      "[248]\tvalid_0's binary_logloss: 0.0822459\n",
      "[249]\tvalid_0's binary_logloss: 0.0822462\n",
      "[250]\tvalid_0's binary_logloss: 0.0822445\n",
      "[251]\tvalid_0's binary_logloss: 0.0822401\n",
      "[252]\tvalid_0's binary_logloss: 0.0822388\n",
      "[253]\tvalid_0's binary_logloss: 0.0822359\n",
      "[254]\tvalid_0's binary_logloss: 0.0822317\n",
      "[255]\tvalid_0's binary_logloss: 0.0822324\n",
      "[256]\tvalid_0's binary_logloss: 0.0822243\n",
      "[257]\tvalid_0's binary_logloss: 0.0822231\n",
      "[258]\tvalid_0's binary_logloss: 0.0822213\n",
      "[259]\tvalid_0's binary_logloss: 0.0822143\n",
      "[260]\tvalid_0's binary_logloss: 0.0822072\n",
      "[261]\tvalid_0's binary_logloss: 0.0822071\n",
      "[262]\tvalid_0's binary_logloss: 0.0822033\n",
      "[263]\tvalid_0's binary_logloss: 0.0822049\n",
      "[264]\tvalid_0's binary_logloss: 0.0822031\n",
      "[265]\tvalid_0's binary_logloss: 0.0821984\n",
      "[266]\tvalid_0's binary_logloss: 0.0821961\n",
      "[267]\tvalid_0's binary_logloss: 0.0821955\n",
      "[268]\tvalid_0's binary_logloss: 0.0821893\n",
      "[269]\tvalid_0's binary_logloss: 0.082188\n",
      "[270]\tvalid_0's binary_logloss: 0.082186\n",
      "[271]\tvalid_0's binary_logloss: 0.0821775\n",
      "[272]\tvalid_0's binary_logloss: 0.082169\n",
      "[273]\tvalid_0's binary_logloss: 0.0821712\n",
      "[274]\tvalid_0's binary_logloss: 0.0821719\n",
      "[275]\tvalid_0's binary_logloss: 0.0821712\n",
      "[276]\tvalid_0's binary_logloss: 0.0821691\n",
      "[277]\tvalid_0's binary_logloss: 0.0821671\n",
      "[278]\tvalid_0's binary_logloss: 0.0821644\n",
      "[279]\tvalid_0's binary_logloss: 0.0821644\n",
      "[280]\tvalid_0's binary_logloss: 0.0821606\n",
      "[281]\tvalid_0's binary_logloss: 0.0821591\n",
      "[282]\tvalid_0's binary_logloss: 0.0821588\n",
      "[283]\tvalid_0's binary_logloss: 0.0821547\n",
      "[284]\tvalid_0's binary_logloss: 0.0821538\n",
      "[285]\tvalid_0's binary_logloss: 0.082152\n",
      "[286]\tvalid_0's binary_logloss: 0.0821539\n",
      "[287]\tvalid_0's binary_logloss: 0.0821532\n",
      "[288]\tvalid_0's binary_logloss: 0.0821517\n",
      "[289]\tvalid_0's binary_logloss: 0.0821517\n",
      "[290]\tvalid_0's binary_logloss: 0.082152\n",
      "[291]\tvalid_0's binary_logloss: 0.0821496\n",
      "[292]\tvalid_0's binary_logloss: 0.0821478\n",
      "[293]\tvalid_0's binary_logloss: 0.0821477\n",
      "[294]\tvalid_0's binary_logloss: 0.0821477\n",
      "[295]\tvalid_0's binary_logloss: 0.082149\n",
      "[296]\tvalid_0's binary_logloss: 0.0821464\n",
      "[297]\tvalid_0's binary_logloss: 0.0821429\n",
      "[298]\tvalid_0's binary_logloss: 0.0821397\n",
      "[299]\tvalid_0's binary_logloss: 0.0821388\n",
      "[300]\tvalid_0's binary_logloss: 0.0821356\n",
      "[301]\tvalid_0's binary_logloss: 0.0821379\n",
      "[302]\tvalid_0's binary_logloss: 0.082139\n",
      "[303]\tvalid_0's binary_logloss: 0.0821386\n",
      "[304]\tvalid_0's binary_logloss: 0.0821371\n",
      "[305]\tvalid_0's binary_logloss: 0.0821351\n",
      "[306]\tvalid_0's binary_logloss: 0.0821327\n",
      "[307]\tvalid_0's binary_logloss: 0.0821315\n",
      "[308]\tvalid_0's binary_logloss: 0.0821178\n",
      "[309]\tvalid_0's binary_logloss: 0.0821169\n",
      "[310]\tvalid_0's binary_logloss: 0.0821161\n",
      "[311]\tvalid_0's binary_logloss: 0.0821132\n",
      "[312]\tvalid_0's binary_logloss: 0.0821161\n",
      "[313]\tvalid_0's binary_logloss: 0.0821086\n",
      "[314]\tvalid_0's binary_logloss: 0.0821046\n",
      "[315]\tvalid_0's binary_logloss: 0.0821055\n",
      "[316]\tvalid_0's binary_logloss: 0.0821038\n",
      "[317]\tvalid_0's binary_logloss: 0.0821036\n",
      "[318]\tvalid_0's binary_logloss: 0.0821018\n",
      "[319]\tvalid_0's binary_logloss: 0.0821025\n",
      "[320]\tvalid_0's binary_logloss: 0.0821051\n",
      "[321]\tvalid_0's binary_logloss: 0.0820981\n",
      "[322]\tvalid_0's binary_logloss: 0.0820976\n",
      "[323]\tvalid_0's binary_logloss: 0.0820972\n",
      "[324]\tvalid_0's binary_logloss: 0.0820945\n",
      "[325]\tvalid_0's binary_logloss: 0.0820896\n",
      "[326]\tvalid_0's binary_logloss: 0.0820897\n",
      "[327]\tvalid_0's binary_logloss: 0.0820871\n",
      "[328]\tvalid_0's binary_logloss: 0.0820808\n",
      "[329]\tvalid_0's binary_logloss: 0.08208\n",
      "[330]\tvalid_0's binary_logloss: 0.0820822\n",
      "[331]\tvalid_0's binary_logloss: 0.0820739\n",
      "[332]\tvalid_0's binary_logloss: 0.0820688\n",
      "[333]\tvalid_0's binary_logloss: 0.0820639\n",
      "[334]\tvalid_0's binary_logloss: 0.0820603\n",
      "[335]\tvalid_0's binary_logloss: 0.08206\n",
      "[336]\tvalid_0's binary_logloss: 0.0820651\n",
      "[337]\tvalid_0's binary_logloss: 0.0820644\n",
      "[338]\tvalid_0's binary_logloss: 0.082062\n",
      "[339]\tvalid_0's binary_logloss: 0.0820626\n",
      "[340]\tvalid_0's binary_logloss: 0.0820632\n",
      "[341]\tvalid_0's binary_logloss: 0.0820578\n",
      "[342]\tvalid_0's binary_logloss: 0.082056\n",
      "[343]\tvalid_0's binary_logloss: 0.0820561\n",
      "[344]\tvalid_0's binary_logloss: 0.0820547\n",
      "[345]\tvalid_0's binary_logloss: 0.0820536\n",
      "[346]\tvalid_0's binary_logloss: 0.0820531\n",
      "[347]\tvalid_0's binary_logloss: 0.0820451\n",
      "[348]\tvalid_0's binary_logloss: 0.0820431\n",
      "[349]\tvalid_0's binary_logloss: 0.0820418\n",
      "[350]\tvalid_0's binary_logloss: 0.0820426\n",
      "[351]\tvalid_0's binary_logloss: 0.0820411\n",
      "[352]\tvalid_0's binary_logloss: 0.0820346\n",
      "[353]\tvalid_0's binary_logloss: 0.0820365\n",
      "[354]\tvalid_0's binary_logloss: 0.0820361\n",
      "[355]\tvalid_0's binary_logloss: 0.0820355\n",
      "[356]\tvalid_0's binary_logloss: 0.0820377\n",
      "[357]\tvalid_0's binary_logloss: 0.082034\n",
      "[358]\tvalid_0's binary_logloss: 0.0820331\n",
      "[359]\tvalid_0's binary_logloss: 0.0820302\n",
      "[360]\tvalid_0's binary_logloss: 0.0820294\n",
      "[361]\tvalid_0's binary_logloss: 0.0820271\n",
      "[362]\tvalid_0's binary_logloss: 0.0820237\n",
      "[363]\tvalid_0's binary_logloss: 0.0820244\n",
      "[364]\tvalid_0's binary_logloss: 0.0820247\n",
      "[365]\tvalid_0's binary_logloss: 0.082025\n",
      "[366]\tvalid_0's binary_logloss: 0.0820235\n",
      "[367]\tvalid_0's binary_logloss: 0.0820239\n",
      "[368]\tvalid_0's binary_logloss: 0.0820272\n",
      "[369]\tvalid_0's binary_logloss: 0.0820276\n",
      "[370]\tvalid_0's binary_logloss: 0.0820267\n",
      "[371]\tvalid_0's binary_logloss: 0.0820194\n",
      "[372]\tvalid_0's binary_logloss: 0.082018\n",
      "[373]\tvalid_0's binary_logloss: 0.0820197\n",
      "[374]\tvalid_0's binary_logloss: 0.0820209\n",
      "[375]\tvalid_0's binary_logloss: 0.082019\n",
      "[376]\tvalid_0's binary_logloss: 0.0820161\n",
      "[377]\tvalid_0's binary_logloss: 0.0820118\n",
      "[378]\tvalid_0's binary_logloss: 0.0820101\n",
      "[379]\tvalid_0's binary_logloss: 0.0820086\n",
      "[380]\tvalid_0's binary_logloss: 0.0820084\n",
      "[381]\tvalid_0's binary_logloss: 0.0820082\n",
      "[382]\tvalid_0's binary_logloss: 0.0820063\n",
      "[383]\tvalid_0's binary_logloss: 0.0820041\n",
      "[384]\tvalid_0's binary_logloss: 0.0820061\n",
      "[385]\tvalid_0's binary_logloss: 0.0820024\n",
      "[386]\tvalid_0's binary_logloss: 0.0820022\n",
      "[387]\tvalid_0's binary_logloss: 0.0820027\n",
      "[388]\tvalid_0's binary_logloss: 0.0820023\n",
      "[389]\tvalid_0's binary_logloss: 0.0820009\n",
      "[390]\tvalid_0's binary_logloss: 0.0819967\n",
      "[391]\tvalid_0's binary_logloss: 0.0819926\n",
      "[392]\tvalid_0's binary_logloss: 0.0819931\n",
      "[393]\tvalid_0's binary_logloss: 0.0819912\n",
      "[394]\tvalid_0's binary_logloss: 0.0819917\n",
      "[395]\tvalid_0's binary_logloss: 0.0819929\n",
      "[396]\tvalid_0's binary_logloss: 0.0819891\n",
      "[397]\tvalid_0's binary_logloss: 0.0819908\n",
      "[398]\tvalid_0's binary_logloss: 0.0819855\n",
      "[399]\tvalid_0's binary_logloss: 0.0819856\n",
      "[400]\tvalid_0's binary_logloss: 0.0819863\n",
      "[401]\tvalid_0's binary_logloss: 0.0819856\n",
      "[402]\tvalid_0's binary_logloss: 0.0819838\n",
      "[403]\tvalid_0's binary_logloss: 0.0819847\n",
      "[404]\tvalid_0's binary_logloss: 0.0819841\n",
      "[405]\tvalid_0's binary_logloss: 0.0819853\n",
      "[406]\tvalid_0's binary_logloss: 0.0819847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[407]\tvalid_0's binary_logloss: 0.0819869\n",
      "[408]\tvalid_0's binary_logloss: 0.0819882\n",
      "[409]\tvalid_0's binary_logloss: 0.0819894\n",
      "[410]\tvalid_0's binary_logloss: 0.0819897\n",
      "[411]\tvalid_0's binary_logloss: 0.0819925\n",
      "[412]\tvalid_0's binary_logloss: 0.0819932\n",
      "[413]\tvalid_0's binary_logloss: 0.0819915\n",
      "[414]\tvalid_0's binary_logloss: 0.0819932\n",
      "[415]\tvalid_0's binary_logloss: 0.0819934\n",
      "[416]\tvalid_0's binary_logloss: 0.0819946\n",
      "[417]\tvalid_0's binary_logloss: 0.0819972\n",
      "[418]\tvalid_0's binary_logloss: 0.0819982\n",
      "[419]\tvalid_0's binary_logloss: 0.0819989\n",
      "[420]\tvalid_0's binary_logloss: 0.0819967\n",
      "[421]\tvalid_0's binary_logloss: 0.0819976\n",
      "[422]\tvalid_0's binary_logloss: 0.0819983\n",
      "[423]\tvalid_0's binary_logloss: 0.082004\n",
      "[424]\tvalid_0's binary_logloss: 0.0820062\n",
      "[425]\tvalid_0's binary_logloss: 0.0820058\n",
      "[426]\tvalid_0's binary_logloss: 0.0820077\n",
      "[427]\tvalid_0's binary_logloss: 0.0820152\n",
      "[428]\tvalid_0's binary_logloss: 0.0820161\n",
      "[429]\tvalid_0's binary_logloss: 0.082015\n",
      "[430]\tvalid_0's binary_logloss: 0.0820168\n",
      "[431]\tvalid_0's binary_logloss: 0.0820187\n",
      "[432]\tvalid_0's binary_logloss: 0.0820173\n",
      "[433]\tvalid_0's binary_logloss: 0.0820178\n",
      "[434]\tvalid_0's binary_logloss: 0.0820281\n",
      "[435]\tvalid_0's binary_logloss: 0.08203\n",
      "[436]\tvalid_0's binary_logloss: 0.0820278\n",
      "[437]\tvalid_0's binary_logloss: 0.0820277\n",
      "[438]\tvalid_0's binary_logloss: 0.0820291\n",
      "[439]\tvalid_0's binary_logloss: 0.0820308\n",
      "[440]\tvalid_0's binary_logloss: 0.0820312\n",
      "[441]\tvalid_0's binary_logloss: 0.0820334\n",
      "[442]\tvalid_0's binary_logloss: 0.0820321\n",
      "[443]\tvalid_0's binary_logloss: 0.0820337\n",
      "[444]\tvalid_0's binary_logloss: 0.0820328\n",
      "[445]\tvalid_0's binary_logloss: 0.0820313\n",
      "[446]\tvalid_0's binary_logloss: 0.0820326\n",
      "[447]\tvalid_0's binary_logloss: 0.0820335\n",
      "[448]\tvalid_0's binary_logloss: 0.0820381\n",
      "[449]\tvalid_0's binary_logloss: 0.0820383\n",
      "[450]\tvalid_0's binary_logloss: 0.0820377\n",
      "[451]\tvalid_0's binary_logloss: 0.0820376\n",
      "[452]\tvalid_0's binary_logloss: 0.0820389\n",
      "[453]\tvalid_0's binary_logloss: 0.0820374\n",
      "[454]\tvalid_0's binary_logloss: 0.0820372\n",
      "[455]\tvalid_0's binary_logloss: 0.0820375\n",
      "[456]\tvalid_0's binary_logloss: 0.0820417\n",
      "[457]\tvalid_0's binary_logloss: 0.0820428\n",
      "[458]\tvalid_0's binary_logloss: 0.0820325\n",
      "[459]\tvalid_0's binary_logloss: 0.0820344\n",
      "[460]\tvalid_0's binary_logloss: 0.0820371\n",
      "[461]\tvalid_0's binary_logloss: 0.0820368\n",
      "[462]\tvalid_0's binary_logloss: 0.0820396\n",
      "[463]\tvalid_0's binary_logloss: 0.0820386\n",
      "[464]\tvalid_0's binary_logloss: 0.0820422\n",
      "[465]\tvalid_0's binary_logloss: 0.0820423\n",
      "[466]\tvalid_0's binary_logloss: 0.0820415\n",
      "[467]\tvalid_0's binary_logloss: 0.0820421\n",
      "[468]\tvalid_0's binary_logloss: 0.0820388\n",
      "[469]\tvalid_0's binary_logloss: 0.0820387\n",
      "[470]\tvalid_0's binary_logloss: 0.0820363\n",
      "[471]\tvalid_0's binary_logloss: 0.0820284\n",
      "[472]\tvalid_0's binary_logloss: 0.0820298\n",
      "[473]\tvalid_0's binary_logloss: 0.0820319\n",
      "[474]\tvalid_0's binary_logloss: 0.0820318\n",
      "[475]\tvalid_0's binary_logloss: 0.0820335\n",
      "[476]\tvalid_0's binary_logloss: 0.0820351\n",
      "[477]\tvalid_0's binary_logloss: 0.0820366\n",
      "[478]\tvalid_0's binary_logloss: 0.0820303\n",
      "[479]\tvalid_0's binary_logloss: 0.0820353\n",
      "[480]\tvalid_0's binary_logloss: 0.0820351\n",
      "[481]\tvalid_0's binary_logloss: 0.0820371\n",
      "[482]\tvalid_0's binary_logloss: 0.082036\n",
      "[483]\tvalid_0's binary_logloss: 0.0820374\n",
      "[484]\tvalid_0's binary_logloss: 0.0820284\n",
      "[485]\tvalid_0's binary_logloss: 0.082023\n",
      "[486]\tvalid_0's binary_logloss: 0.0820223\n",
      "[487]\tvalid_0's binary_logloss: 0.0820237\n",
      "[488]\tvalid_0's binary_logloss: 0.0820247\n",
      "[489]\tvalid_0's binary_logloss: 0.082015\n",
      "[490]\tvalid_0's binary_logloss: 0.0820162\n",
      "[491]\tvalid_0's binary_logloss: 0.0820174\n",
      "[492]\tvalid_0's binary_logloss: 0.0820093\n",
      "[493]\tvalid_0's binary_logloss: 0.0820089\n",
      "[494]\tvalid_0's binary_logloss: 0.0820031\n",
      "[495]\tvalid_0's binary_logloss: 0.0820037\n",
      "[496]\tvalid_0's binary_logloss: 0.0820062\n",
      "[497]\tvalid_0's binary_logloss: 0.0820059\n",
      "[498]\tvalid_0's binary_logloss: 0.0820081\n",
      "[499]\tvalid_0's binary_logloss: 0.0820101\n",
      "[500]\tvalid_0's binary_logloss: 0.0820086\n",
      "[501]\tvalid_0's binary_logloss: 0.0820007\n",
      "[502]\tvalid_0's binary_logloss: 0.0820027\n",
      "[503]\tvalid_0's binary_logloss: 0.0820108\n",
      "[504]\tvalid_0's binary_logloss: 0.0820058\n",
      "[505]\tvalid_0's binary_logloss: 0.0820059\n",
      "[506]\tvalid_0's binary_logloss: 0.0820032\n",
      "[507]\tvalid_0's binary_logloss: 0.0820017\n",
      "[508]\tvalid_0's binary_logloss: 0.0820013\n",
      "[509]\tvalid_0's binary_logloss: 0.0820035\n",
      "[510]\tvalid_0's binary_logloss: 0.0819991\n",
      "[511]\tvalid_0's binary_logloss: 0.081997\n",
      "[512]\tvalid_0's binary_logloss: 0.0820037\n",
      "[513]\tvalid_0's binary_logloss: 0.0820037\n",
      "[514]\tvalid_0's binary_logloss: 0.0819983\n",
      "[515]\tvalid_0's binary_logloss: 0.0819977\n",
      "[516]\tvalid_0's binary_logloss: 0.0819979\n",
      "[517]\tvalid_0's binary_logloss: 0.0819993\n",
      "[518]\tvalid_0's binary_logloss: 0.0819891\n",
      "[519]\tvalid_0's binary_logloss: 0.0819885\n",
      "[520]\tvalid_0's binary_logloss: 0.0819868\n",
      "[521]\tvalid_0's binary_logloss: 0.0819973\n",
      "[522]\tvalid_0's binary_logloss: 0.0819984\n",
      "[523]\tvalid_0's binary_logloss: 0.0820011\n",
      "[524]\tvalid_0's binary_logloss: 0.0819999\n",
      "[525]\tvalid_0's binary_logloss: 0.0819983\n",
      "[526]\tvalid_0's binary_logloss: 0.081997\n",
      "[527]\tvalid_0's binary_logloss: 0.0819973\n",
      "[528]\tvalid_0's binary_logloss: 0.081998\n",
      "[529]\tvalid_0's binary_logloss: 0.0820011\n",
      "[530]\tvalid_0's binary_logloss: 0.0819995\n",
      "[531]\tvalid_0's binary_logloss: 0.0820098\n",
      "[532]\tvalid_0's binary_logloss: 0.0820109\n",
      "[533]\tvalid_0's binary_logloss: 0.082012\n",
      "[534]\tvalid_0's binary_logloss: 0.0820113\n",
      "[535]\tvalid_0's binary_logloss: 0.0820118\n",
      "[536]\tvalid_0's binary_logloss: 0.0820108\n",
      "[537]\tvalid_0's binary_logloss: 0.082002\n",
      "[538]\tvalid_0's binary_logloss: 0.0820039\n",
      "[539]\tvalid_0's binary_logloss: 0.0820043\n",
      "[540]\tvalid_0's binary_logloss: 0.0820025\n",
      "[541]\tvalid_0's binary_logloss: 0.0820062\n",
      "[542]\tvalid_0's binary_logloss: 0.0820157\n",
      "[543]\tvalid_0's binary_logloss: 0.0820169\n",
      "[544]\tvalid_0's binary_logloss: 0.0820148\n",
      "[545]\tvalid_0's binary_logloss: 0.0820148\n",
      "[546]\tvalid_0's binary_logloss: 0.0820142\n",
      "[547]\tvalid_0's binary_logloss: 0.0820145\n",
      "[548]\tvalid_0's binary_logloss: 0.0820168\n",
      "[549]\tvalid_0's binary_logloss: 0.0820292\n",
      "[550]\tvalid_0's binary_logloss: 0.0820285\n",
      "[551]\tvalid_0's binary_logloss: 0.0820317\n",
      "[552]\tvalid_0's binary_logloss: 0.0820325\n",
      "[553]\tvalid_0's binary_logloss: 0.0820263\n",
      "[554]\tvalid_0's binary_logloss: 0.0820248\n",
      "[555]\tvalid_0's binary_logloss: 0.0820253\n",
      "[556]\tvalid_0's binary_logloss: 0.0820267\n",
      "[557]\tvalid_0's binary_logloss: 0.0820258\n",
      "[558]\tvalid_0's binary_logloss: 0.0820262\n",
      "[559]\tvalid_0's binary_logloss: 0.0820393\n",
      "[560]\tvalid_0's binary_logloss: 0.0820371\n",
      "[561]\tvalid_0's binary_logloss: 0.0820382\n",
      "[562]\tvalid_0's binary_logloss: 0.0820388\n",
      "[563]\tvalid_0's binary_logloss: 0.0820355\n",
      "[564]\tvalid_0's binary_logloss: 0.0820355\n",
      "[565]\tvalid_0's binary_logloss: 0.0820362\n",
      "[566]\tvalid_0's binary_logloss: 0.0820383\n",
      "[567]\tvalid_0's binary_logloss: 0.0820402\n",
      "[568]\tvalid_0's binary_logloss: 0.0820393\n",
      "[569]\tvalid_0's binary_logloss: 0.0820415\n",
      "[570]\tvalid_0's binary_logloss: 0.0820433\n",
      "[571]\tvalid_0's binary_logloss: 0.0820435\n",
      "[572]\tvalid_0's binary_logloss: 0.0820439\n",
      "[573]\tvalid_0's binary_logloss: 0.082045\n",
      "[574]\tvalid_0's binary_logloss: 0.0820466\n",
      "[575]\tvalid_0's binary_logloss: 0.0820453\n",
      "[576]\tvalid_0's binary_logloss: 0.0820577\n",
      "[577]\tvalid_0's binary_logloss: 0.08206\n",
      "[578]\tvalid_0's binary_logloss: 0.0820593\n",
      "[579]\tvalid_0's binary_logloss: 0.0820618\n",
      "[580]\tvalid_0's binary_logloss: 0.0820628\n",
      "[581]\tvalid_0's binary_logloss: 0.0820636\n",
      "[582]\tvalid_0's binary_logloss: 0.0820665\n",
      "[583]\tvalid_0's binary_logloss: 0.0820639\n",
      "[584]\tvalid_0's binary_logloss: 0.0820647\n",
      "[585]\tvalid_0's binary_logloss: 0.0820648\n",
      "[586]\tvalid_0's binary_logloss: 0.0820651\n",
      "[587]\tvalid_0's binary_logloss: 0.0820622\n",
      "[588]\tvalid_0's binary_logloss: 0.0820636\n",
      "[589]\tvalid_0's binary_logloss: 0.082069\n",
      "[590]\tvalid_0's binary_logloss: 0.0820716\n",
      "[591]\tvalid_0's binary_logloss: 0.0820742\n",
      "[592]\tvalid_0's binary_logloss: 0.0820749\n",
      "[593]\tvalid_0's binary_logloss: 0.0820783\n",
      "[594]\tvalid_0's binary_logloss: 0.0820794\n",
      "[595]\tvalid_0's binary_logloss: 0.0820726\n",
      "[596]\tvalid_0's binary_logloss: 0.0820759\n",
      "[597]\tvalid_0's binary_logloss: 0.082075\n",
      "[598]\tvalid_0's binary_logloss: 0.0820809\n",
      "[599]\tvalid_0's binary_logloss: 0.0820829\n",
      "[600]\tvalid_0's binary_logloss: 0.0820823\n",
      "[601]\tvalid_0's binary_logloss: 0.0820825\n",
      "[602]\tvalid_0's binary_logloss: 0.0820804\n",
      "[603]\tvalid_0's binary_logloss: 0.0820824\n",
      "[604]\tvalid_0's binary_logloss: 0.0820829\n",
      "[605]\tvalid_0's binary_logloss: 0.0820844\n",
      "[606]\tvalid_0's binary_logloss: 0.0820843\n",
      "[607]\tvalid_0's binary_logloss: 0.0820873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[608]\tvalid_0's binary_logloss: 0.0820889\n",
      "[609]\tvalid_0's binary_logloss: 0.0820916\n",
      "[610]\tvalid_0's binary_logloss: 0.0820937\n",
      "[611]\tvalid_0's binary_logloss: 0.0820942\n",
      "[612]\tvalid_0's binary_logloss: 0.0820928\n",
      "[613]\tvalid_0's binary_logloss: 0.0820861\n",
      "[614]\tvalid_0's binary_logloss: 0.0820902\n",
      "[615]\tvalid_0's binary_logloss: 0.0820974\n",
      "[616]\tvalid_0's binary_logloss: 0.082097\n",
      "[617]\tvalid_0's binary_logloss: 0.0821001\n",
      "[618]\tvalid_0's binary_logloss: 0.0820982\n",
      "[619]\tvalid_0's binary_logloss: 0.0820999\n",
      "[620]\tvalid_0's binary_logloss: 0.0821003\n",
      "[621]\tvalid_0's binary_logloss: 0.0821015\n",
      "[622]\tvalid_0's binary_logloss: 0.082104\n",
      "[623]\tvalid_0's binary_logloss: 0.0821045\n",
      "[624]\tvalid_0's binary_logloss: 0.0821045\n",
      "[625]\tvalid_0's binary_logloss: 0.082108\n",
      "[626]\tvalid_0's binary_logloss: 0.082111\n",
      "[627]\tvalid_0's binary_logloss: 0.0821212\n",
      "[628]\tvalid_0's binary_logloss: 0.0821229\n",
      "[629]\tvalid_0's binary_logloss: 0.0821217\n",
      "[630]\tvalid_0's binary_logloss: 0.0821222\n",
      "[631]\tvalid_0's binary_logloss: 0.0821258\n",
      "[632]\tvalid_0's binary_logloss: 0.0821288\n",
      "[633]\tvalid_0's binary_logloss: 0.0821306\n",
      "[634]\tvalid_0's binary_logloss: 0.0821338\n",
      "[635]\tvalid_0's binary_logloss: 0.0821348\n",
      "[636]\tvalid_0's binary_logloss: 0.0821376\n",
      "[637]\tvalid_0's binary_logloss: 0.0821397\n",
      "[638]\tvalid_0's binary_logloss: 0.082141\n",
      "[639]\tvalid_0's binary_logloss: 0.0821439\n",
      "[640]\tvalid_0's binary_logloss: 0.0821406\n",
      "[641]\tvalid_0's binary_logloss: 0.0821401\n",
      "[642]\tvalid_0's binary_logloss: 0.0821409\n",
      "[643]\tvalid_0's binary_logloss: 0.0821453\n",
      "[644]\tvalid_0's binary_logloss: 0.0821482\n",
      "[645]\tvalid_0's binary_logloss: 0.0821479\n",
      "[646]\tvalid_0's binary_logloss: 0.0821482\n",
      "[647]\tvalid_0's binary_logloss: 0.0821482\n",
      "[648]\tvalid_0's binary_logloss: 0.0821516\n",
      "[649]\tvalid_0's binary_logloss: 0.0821529\n",
      "[650]\tvalid_0's binary_logloss: 0.0821554\n",
      "[651]\tvalid_0's binary_logloss: 0.0821578\n",
      "[652]\tvalid_0's binary_logloss: 0.0821577\n",
      "[653]\tvalid_0's binary_logloss: 0.0821583\n",
      "[654]\tvalid_0's binary_logloss: 0.0821541\n",
      "[655]\tvalid_0's binary_logloss: 0.0821557\n",
      "[656]\tvalid_0's binary_logloss: 0.0821557\n",
      "[657]\tvalid_0's binary_logloss: 0.0821588\n",
      "[658]\tvalid_0's binary_logloss: 0.0821611\n",
      "[659]\tvalid_0's binary_logloss: 0.0821602\n",
      "[660]\tvalid_0's binary_logloss: 0.082159\n",
      "[661]\tvalid_0's binary_logloss: 0.0821617\n",
      "[662]\tvalid_0's binary_logloss: 0.0821626\n",
      "[663]\tvalid_0's binary_logloss: 0.0821656\n",
      "[664]\tvalid_0's binary_logloss: 0.0821661\n",
      "[665]\tvalid_0's binary_logloss: 0.0821706\n",
      "[666]\tvalid_0's binary_logloss: 0.0821717\n",
      "[667]\tvalid_0's binary_logloss: 0.0821735\n",
      "[668]\tvalid_0's binary_logloss: 0.082174\n",
      "[669]\tvalid_0's binary_logloss: 0.0821759\n",
      "[670]\tvalid_0's binary_logloss: 0.0821781\n",
      "[671]\tvalid_0's binary_logloss: 0.0821778\n",
      "[672]\tvalid_0's binary_logloss: 0.0821915\n",
      "[673]\tvalid_0's binary_logloss: 0.0821937\n",
      "[674]\tvalid_0's binary_logloss: 0.0822\n",
      "[675]\tvalid_0's binary_logloss: 0.0821972\n",
      "[676]\tvalid_0's binary_logloss: 0.0821952\n",
      "[677]\tvalid_0's binary_logloss: 0.082194\n",
      "[678]\tvalid_0's binary_logloss: 0.0821944\n",
      "[679]\tvalid_0's binary_logloss: 0.0822071\n",
      "[680]\tvalid_0's binary_logloss: 0.0822069\n",
      "[681]\tvalid_0's binary_logloss: 0.0822037\n",
      "[682]\tvalid_0's binary_logloss: 0.0822084\n",
      "[683]\tvalid_0's binary_logloss: 0.0822054\n",
      "[684]\tvalid_0's binary_logloss: 0.0822027\n",
      "[685]\tvalid_0's binary_logloss: 0.0822031\n",
      "[686]\tvalid_0's binary_logloss: 0.0822042\n",
      "[687]\tvalid_0's binary_logloss: 0.0822192\n",
      "[688]\tvalid_0's binary_logloss: 0.0822197\n",
      "[689]\tvalid_0's binary_logloss: 0.0822212\n",
      "[690]\tvalid_0's binary_logloss: 0.0822221\n",
      "[691]\tvalid_0's binary_logloss: 0.0822238\n",
      "[692]\tvalid_0's binary_logloss: 0.0822248\n",
      "[693]\tvalid_0's binary_logloss: 0.0822268\n",
      "[694]\tvalid_0's binary_logloss: 0.0822288\n",
      "[695]\tvalid_0's binary_logloss: 0.082228\n",
      "[696]\tvalid_0's binary_logloss: 0.0822323\n",
      "[697]\tvalid_0's binary_logloss: 0.0822362\n",
      "[698]\tvalid_0's binary_logloss: 0.0822375\n",
      "[699]\tvalid_0's binary_logloss: 0.0822419\n",
      "[700]\tvalid_0's binary_logloss: 0.0822598\n",
      "[701]\tvalid_0's binary_logloss: 0.0822587\n",
      "[702]\tvalid_0's binary_logloss: 0.0822592\n",
      "Early stopping, best iteration is:\n",
      "[402]\tvalid_0's binary_logloss: 0.0819838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', colsample_bytree=1.0, learning_rate=0.01,\n",
       "       max_bin=255, max_depth=-1, min_child_samples=10, min_child_weight=5,\n",
       "       min_split_gain=0.0, n_estimators=2000, n_jobs=-1, num_leaves=64,\n",
       "       objective='binary', random_state=0, reg_alpha=0.0, reg_lambda=0.0,\n",
       "       silent=True, subsample=1.0, subsample_for_bin=50000,\n",
       "       subsample_freq=1)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "gbm = lgb.LGBMRegressor(objective='binary',\n",
    "\n",
    "                        num_leaves=64,\n",
    "\n",
    "                        learning_rate=0.01,\n",
    "\n",
    "                        n_estimators=2000)\n",
    "\n",
    "gbm.fit(feature_train, label_train,\n",
    "\n",
    "        eval_set=[(feature_val, label_val)],\n",
    "\n",
    "        eval_metric='binary_logloss',\n",
    "\n",
    "        early_stopping_rounds=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0821183476381\n"
     ]
    }
   ],
   "source": [
    "label_val_bgm = gbm.predict(feature_val, num_iteration=gbm.best_iteration_)\n",
    "\n",
    "label_test = gbm.predict(feature_test, num_iteration=gbm.best_iteration_)\n",
    "\n",
    "print(log_loss(label_val,label_val_bgm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('data/round1_result_lgb.txt', np.c_[data_index, label_test], delimiter=',', header='instance_id predicted_score',comments='', fmt='%s %f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.584341164082\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier    \n",
    "model = GradientBoostingClassifier()    \n",
    "model.fit(feature_train, label_train)    \n",
    "label_val_gbdt = clf.predict(feature_val)  \n",
    "print(log_loss(label_val,label_val_gbdt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.FM/FFM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_train[label_train < 0.5] = -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_train = csc_matrix(feature_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_val = csc_matrix(feature_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/opt/conda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/opt/conda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1735: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  cond2 = (x >= self.b) & cond0\n",
      "/opt/conda/lib/python3.6/site-packages/fastFM/base.py:123: RuntimeWarning: invalid value encountered in greater\n",
      "  y_pred[y_proba > .5] = self.classes_[1]\n"
     ]
    }
   ],
   "source": [
    "from fastFM import sgd,mcmc\n",
    "fm = sgd.FMClassification(\n",
    "#     n_iter=1000, init_stdev=0.1, l2_reg_w=0,l2_reg_V=0, rank=2, step_size=0.1\n",
    ")\n",
    "fm.fit(feature_train, label_train)\n",
    "label_val_fm = fm.predict(feature_val)\n",
    "\n",
    "# fm = mcmc.FMClassification(n_iter=1000, rank=2, init_stdev=0.1)\n",
    "# y_pred = fm.fit_predict(X_train, y_train, X_test)\n",
    "# label_val_fm = fm.fit_predict_proba(X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0    57334\n",
       "dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(label_val_fm).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.集成\n",
    "要求：\n",
    "- Base Model 之间的相关性要尽可能的小\n",
    "- Base Model 之间的性能表现不能差距太大\n",
    "\n",
    "## 9.1Bagging\n",
    "- 使用训练数据的不同随机子集来训练每个 Base Model，最后进行每个 Base Model 权重相同的 Vote。\n",
    "\n",
    "## 9.2Boosting\n",
    "- 迭代地训练 Base Model，每次根据上一个迭代中预测错误的情况修改训练样本的权重。\n",
    "\n",
    "## 9.3Blending\n",
    "- 用不相交的数据训练不同的 Base Model，将它们的输出取（加权）平均。实现简单，但对训练数据利用少了。\n",
    "\n",
    "## 9.4Stacking\n",
    "- 我用了两层的模型融合，Level 1使用了：XGBoost、LightGBM、RandomForest、ExtraTrees、DecisionTree、AdaBoost，一共6个模型，Level 2使用了LinearRegression来拟合第一层的结果。\n",
    "- http://blog.csdn.net/a358463121/article/details/53054686"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
