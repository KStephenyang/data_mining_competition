{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiprocess_apply_func(func,data_now,args):\n",
    "    if(type(data_now) == pd.core.frame.DataFrame):\n",
    "        return data_now.apply(func,args = args,axis = 1)\n",
    "    else:\n",
    "        return data_now.apply(func,args = args)\n",
    "    \n",
    "def multiprocess_apply(data_now,cpu_count,func,args = ()):   \n",
    "    \"\"\"\n",
    "        @data_now:要apply的数据，DataFrame或Series格式\n",
    "        @cpu_count:动用的核数\n",
    "        @func:apply传入的函数（注意：不能是lambda函数）\n",
    "        @args:func传入的参数\n",
    "    \"\"\"\n",
    "    pool = Pool(cpu_count)\n",
    "    data_count_per_cpu = len(data_now) / cpu_count\n",
    "    res_pool = [None] * (cpu_count)\n",
    "    \n",
    "    for i in range(cpu_count):\n",
    "        start = int(i * data_count_per_cpu)\n",
    "        if(start == len(data_now)):\n",
    "            break\n",
    "        end = int(min((i + 1) * data_count_per_cpu - 1,len(data_now)))\n",
    "\n",
    "        res_pool[i] = pool.apply_async(multiprocess_apply_func,args = (func,data_now.iloc[start:end + 1],args,))\n",
    "    \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    res = res_pool[0].get()\n",
    "    for i in range(1,cpu_count):\n",
    "        res = res.append(res_pool[i].get())\n",
    "    assert len(res) == len(data_now)\n",
    "    \n",
    "    pool.terminate()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "# OS\n",
    "import os\n",
    "import datetime\n",
    "import pytz\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "import gc\n",
    "\n",
    "#数据处理\n",
    "import pandas as pd\n",
    "# import ray.dataframe as pd\n",
    "from numpy import linalg\n",
    "import numpy as np\n",
    "import random\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from scipy.special import boxcox1p\n",
    "from numpy import linalg\n",
    "\n",
    "#可视化\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "print(multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.数据载入、对齐、排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10432036, 27)\n",
      "(519888, 26)\n",
      "(1209768, 26)\n",
      "(10432036, 28) (1729656, 28) (1209768, 28)\n",
      "(12161692, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_table('../2.data/round2_train.txt',sep = ' ',na_values = [-1.0,-1,'-1'])\n",
    "print(data.shape)\n",
    "data_test = pd.read_table('../2.data/round2_test_a.txt',sep = ' ',na_values = [-1.0,-1,'-1'])\n",
    "print(data_test.shape)\n",
    "data_test2 = pd.read_table('../2.data/round2_test_b.txt',sep = ' ',na_values = [-1.0,-1,'-1'])\n",
    "print(data_test2.shape)\n",
    "\n",
    "data_test['label'] = pd.Series([0] * len(data_test))\n",
    "data_test2['label'] = pd.Series([0] * len(data_test2))\n",
    "data.rename({'is_trade':'label'}, axis='columns',inplace = True)\n",
    "\n",
    "data['flag'] = pd.Series([0] * len(data))\n",
    "data_test['flag'] = pd.Series([1] * len(data_test))\n",
    "data_test2['flag'] = pd.Series([1] * len(data_test2))\n",
    "data_test = data_test.append(data_test2,ignore_index = True)\n",
    "print(data.shape,data_test.shape,data_test2.shape)\n",
    "del data_test2\n",
    "\n",
    "data = data.append(data_test,ignore_index = True)\n",
    "del data_test\n",
    "print(data.shape)\n",
    "\n",
    "data.reset_index(inplace = True)\n",
    "data.set_index('context_timestamp', drop = False,inplace=True)# f16转换为索引\n",
    "data.sort_values(['context_timestamp'],inplace = True)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_pickle('../2.data/round1_train1')\n",
    "# data_test = pd.read_pickle('../2.data/round1_test1')\n",
    "\n",
    "# # 给test数据拼接一个f26特征\n",
    "# data_test = pd.concat([\n",
    "#     data_test,\n",
    "#     pd.Series([0] * len(data_test),name = 'label')\n",
    "# ], axis = 1)\n",
    "# data.rename({'is_trade':'label'}, axis='columns',inplace = True)\n",
    "\n",
    "# # 拼接    \n",
    "# data = data.append(data_test)\n",
    "\n",
    "# # 按时间排序，重置索引\n",
    "# data.reset_index(drop = True,inplace = True)\n",
    "# data.reset_index(inplace = True)\n",
    "# data.set_index('context_timestamp', drop = False,inplace=True)# f16转换为索引\n",
    "# data.sort_values(['context_timestamp'],inplace = True)\n",
    "# del data_test\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.特征转换、衍生\n",
    "## 3.1离散数据\n",
    "### 3.1.1将出现次数少的值合并到统一类别中\n",
    "- f6：将10（456）、2（347）、1（85）、11（21）、0（12）、17（1）、16（1）这几个取值单独拉出一个类。\n",
    "- f9：将8（449）、7（245）、0（123）、6（116）、5（63）、4（33）3（11）、2（5）、1（1）这几个取值单独拉出一个类。\n",
    "- f20：将23（353）、4（266）、2（87）、3（80）、1（20）、0（7）、25（4）这几个取值单独拉出一个类。\n",
    "- f22：将5002（477）、5020（357）、5000（81）、5019（70）、5001（60）、4999（7）这几个取值单独拉出一个类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2onehot/dummy-trap\n",
    "- f6广告商品的价格等级、f9广告商品被展示次数的等级、f11用户的预测性别编号、f12用户的预测年龄等级、f13用户的预测职业编号、f14用户的星级编号\n",
    "- f7、f8、f17、f20、f22不确定\n",
    "\n",
    "#### 3.1.2.1f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12161692, 126)\n"
     ]
    }
   ],
   "source": [
    "def split_f2(data):\n",
    "    # 切分\n",
    "    data = pd.concat([\n",
    "        data.loc[:,:'item_category_list'],\n",
    "        data['item_category_list'].astype(np.str).str.split(';', expand=True),\n",
    "        data.loc[:,'item_property_list':]\n",
    "    ], axis = 1)\n",
    "    data.rename({\n",
    "        0:'item_category_list:1',\n",
    "        1:'item_category_list:2',\n",
    "        2:'item_category_list:3'\n",
    "    }, axis='columns',inplace = True)\n",
    "\n",
    "    # 类型转换\n",
    "    data['item_category_list:1'].fillna('-1',inplace = True)\n",
    "    data['item_category_list:2'].fillna('-1',inplace = True)\n",
    "    data['item_category_list:3'].fillna('-1',inplace = True)\n",
    "    data['item_category_list:1'] = data['item_category_list:1'].astype('int')\n",
    "    data['item_category_list:2'] =data['item_category_list:2'].astype('int')\n",
    "    data['item_category_list:3'] = data['item_category_list:3'].astype('int')\n",
    "\n",
    "    # 对f2:2、f2:3进行onehot编码\n",
    "    temp = pd.get_dummies(data['item_category_list:2'],dummy_na  = True,prefix = 'item_category_list_2')  \n",
    "    data = pd.concat([\n",
    "        data.loc[:,:'item_category_list:2'],\n",
    "        temp,\n",
    "        data.loc[:,'item_category_list:3':]\n",
    "    ], axis = 1)\n",
    "    \n",
    "    temp = pd.get_dummies(data['item_category_list:3'],dummy_na  = True,prefix = 'item_category_list_3')  \n",
    "    return pd.concat([\n",
    "        data.loc[:,:'item_category_list:3'],\n",
    "        temp,\n",
    "        data.loc[:,'item_property_list':]\n",
    "    ], axis = 1)\n",
    "\n",
    "data = split_f2(data)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2.2f11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12161692, 130)\n"
     ]
    }
   ],
   "source": [
    "def onehot_f11(data):\n",
    "    temp = pd.get_dummies(data['user_gender_id'],dummy_na  = True,prefix = 'user_gender_id:')  \n",
    "\n",
    "    return pd.concat([\n",
    "        data.loc[:,:'user_gender_id'],\n",
    "        temp,\n",
    "        data.loc[:,'user_age_level':]\n",
    "    ], axis = 1)\n",
    "\n",
    "data = onehot_f11(data)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2.3f13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12161692, 135)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def onehot_f13(data):\n",
    "    temp = pd.get_dummies(data['user_occupation_id'],dummy_na  = True,prefix = 'user_occupation_id_')\n",
    "    return pd.concat([\n",
    "        data.loc[:,:'user_occupation_id'],\n",
    "        temp,\n",
    "        data.loc[:,'user_star_level':]\n",
    "    ], axis = 1)\n",
    "\n",
    "data = onehot_f13(data)\n",
    "print(data.shape)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12161692, 135) ['index' 'instance_id' 'item_id' 'item_category_list'\n",
      " 'item_category_list:1' 'item_category_list:2'\n",
      " 'item_category_list_2_3.949153947410967e+17'\n",
      " 'item_category_list_2_4.535254803665509e+17'\n",
      " 'item_category_list_2_5.974242234565864e+17'\n",
      " 'item_category_list_2_7.685797875215753e+17'\n",
      " 'item_category_list_2_9.262054013029023e+17'\n",
      " 'item_category_list_2_1.1470741689685322e+18'\n",
      " 'item_category_list_2_1.3671771540733827e+18'\n",
      " 'item_category_list_2_1.8526005172650624e+18'\n",
      " 'item_category_list_2_1.909641874861641e+18'\n",
      " 'item_category_list_2_1.9163903451332127e+18'\n",
      " 'item_category_list_2_1.9200841681043348e+18'\n",
      " 'item_category_list_2_2.211060154630359e+18'\n",
      " 'item_category_list_2_2.8717293836713016e+18'\n",
      " 'item_category_list_2_3.0892543029476204e+18'\n",
      " 'item_category_list_2_3.348197449185791e+18'\n",
      " 'item_category_list_2_3.434689896486063e+18'\n",
      " 'item_category_list_2_3.6137835631996273e+18'\n",
      " 'item_category_list_2_4.911723539855588e+18'\n",
      " 'item_category_list_2_5.066527928272239e+18'\n",
      " 'item_category_list_2_5.68569013987941e+18'\n",
      " 'item_category_list_2_6.254910033820815e+18'\n",
      " 'item_category_list_2_6.670526099037031e+18'\n",
      " 'item_category_list_2_6.693726201323252e+18'\n",
      " 'item_category_list_2_7.226013370341272e+18'\n",
      " 'item_category_list_2_7.314150500379498e+18'\n",
      " 'item_category_list_2_7.423553047267511e+18'\n",
      " 'item_category_list_2_8.009556227083202e+18'\n",
      " 'item_category_list_2_8.468007938333143e+18'\n",
      " 'item_category_list_2_8.46837010590862e+18'\n",
      " 'item_category_list_2_8.769426218101861e+18'\n",
      " 'item_category_list_2_8.841625760168847e+18' 'item_category_list_2_nan'\n",
      " 'item_category_list:3' 'item_category_list_3_-1.0'\n",
      " 'item_category_list_3_2.4292692654358803e+17'\n",
      " 'item_category_list_3_3.671363205864541e+17'\n",
      " 'item_category_list_3_6.69661864989165e+17'\n",
      " 'item_category_list_3_1.0360823570540439e+18'\n",
      " 'item_category_list_3_1.2112187474183503e+18'\n",
      " 'item_category_list_3_1.4678707293165668e+18'\n",
      " 'item_category_list_3_1.4773376140024678e+18'\n",
      " 'item_category_list_3_1.4953887999089408e+18'\n",
      " 'item_category_list_3_1.5189163023981494e+18'\n",
      " 'item_category_list_3_1.6484457152688369e+18'\n",
      " 'item_category_list_3_1.7737573992370422e+18'\n",
      " 'item_category_list_3_2.0667146032918502e+18'\n",
      " 'item_category_list_3_2.104555450664742e+18'\n",
      " 'item_category_list_3_2.1919081694948444e+18'\n",
      " 'item_category_list_3_2.3942618305577574e+18'\n",
      " 'item_category_list_3_2.633274017530891e+18'\n",
      " 'item_category_list_3_3.0350724682773396e+18'\n",
      " 'item_category_list_3_3.0561722848750684e+18'\n",
      " 'item_category_list_3_3.1239555576744207e+18'\n",
      " 'item_category_list_3_3.124898744105968e+18'\n",
      " 'item_category_list_3_3.492642177859571e+18'\n",
      " 'item_category_list_3_3.8766935579183923e+18'\n",
      " 'item_category_list_3_4.0395627406939643e+18'\n",
      " 'item_category_list_3_4.3324492077594117e+18'\n",
      " 'item_category_list_3_4.4237569224751427e+18'\n",
      " 'item_category_list_3_4.521717672641793e+18'\n",
      " 'item_category_list_3_4.5598378274862894e+18'\n",
      " 'item_category_list_3_4.62004237070379e+18'\n",
      " 'item_category_list_3_4.642777236640082e+18'\n",
      " 'item_category_list_3_4.782940324184009e+18'\n",
      " 'item_category_list_3_4.919080589661355e+18'\n",
      " 'item_category_list_3_4.993377953084092e+18'\n",
      " 'item_category_list_3_5.031957903352581e+18'\n",
      " 'item_category_list_3_5.103300281712297e+18'\n",
      " 'item_category_list_3_5.575767811251205e+18'\n",
      " 'item_category_list_3_5.896127768779494e+18'\n",
      " 'item_category_list_3_5.941301806663871e+18'\n",
      " 'item_category_list_3_6.119230265477212e+18'\n",
      " 'item_category_list_3_6.370392357088947e+18'\n",
      " 'item_category_list_3_6.731593623692005e+18'\n",
      " 'item_category_list_3_6.747145348367564e+18'\n",
      " 'item_category_list_3_6.783634657823732e+18'\n",
      " 'item_category_list_3_6.861763693457621e+18'\n",
      " 'item_category_list_3_7.017624410306476e+18'\n",
      " 'item_category_list_3_7.438434275307066e+18'\n",
      " 'item_category_list_3_7.497531498747093e+18'\n",
      " 'item_category_list_3_7.505733337486864e+18'\n",
      " 'item_category_list_3_7.848078693615387e+18'\n",
      " 'item_category_list_3_7.978017778431587e+18'\n",
      " 'item_category_list_3_8.010602907289638e+18'\n",
      " 'item_category_list_3_8.045794075432612e+18'\n",
      " 'item_category_list_3_8.077497621372666e+18'\n",
      " 'item_category_list_3_8.096917686185033e+18'\n",
      " 'item_category_list_3_8.110966937369051e+18'\n",
      " 'item_category_list_3_8.123435010580049e+18'\n",
      " 'item_category_list_3_8.418308787262291e+18'\n",
      " 'item_category_list_3_8.72739728582695e+18'\n",
      " 'item_category_list_3_8.807331407484389e+18'\n",
      " 'item_category_list_3_8.82646757569844e+18'\n",
      " 'item_category_list_3_9.030622377694665e+18' 'item_category_list_3_nan'\n",
      " 'item_property_list' 'item_brand_id' 'item_city_id' 'item_price_level'\n",
      " 'item_sales_level' 'item_collected_level' 'item_pv_level' 'user_id'\n",
      " 'user_gender_id' 'user_gender_id:_0.0' 'user_gender_id:_1.0'\n",
      " 'user_gender_id:_2.0' 'user_gender_id:_nan' 'user_age_level'\n",
      " 'user_occupation_id' 'user_occupation_id__2002.0'\n",
      " 'user_occupation_id__2003.0' 'user_occupation_id__2004.0'\n",
      " 'user_occupation_id__2005.0' 'user_occupation_id__nan' 'user_star_level'\n",
      " 'context_id' 'context_timestamp' 'context_page_id'\n",
      " 'predict_category_property' 'shop_id' 'shop_review_num_level'\n",
      " 'shop_review_positive_rate' 'shop_star_level' 'shop_score_service'\n",
      " 'shop_score_delivery' 'shop_score_description' 'label' 'flag']\n"
     ]
    }
   ],
   "source": [
    "print(data.shape,data.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 将f11、f13进行onehot编码后，xgboost的验证集loss由0.084306减小到0.084273\n",
    "- 将f2;2用onehot表示后，xgboost的验证集loss由0.0775减小到0.07373\n",
    "- 将f5用onehot表示后，xgboost的验证集loss由0.086977提高到0.087148，没用\n",
    "\n",
    "### 3.1.3自然数编码\n",
    "- 消耗内存小，训练时间快，但是相比one-hot特征的质量不高，含了一个假设：不同的类别之间，存在一种顺序关系。\n",
    "\n",
    "### 3.1.4聚类编码\n",
    "- 和独热编码相比，聚类编码试图充分利用每一列0与1的信息表达能力。聚类编码时一般需要特定的专业知识（domain knowledge），例如ZIP码可以根据精确度分层为ZIP3、ZIP4、ZIP5、ZIP6，然后按层次进行编码。\n",
    "\n",
    "## 3.2连续数据\n",
    "### 3.2.1标准化、归一化：分布太宽，做一下scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling\n",
    "# def scaling(data):\n",
    "#     for i in range(0,8):\n",
    "#         data['f12'] = data['f12'].replace(1000. + i,0 + i)\n",
    "#     for i in range(0,11):\n",
    "#         data['f14'] = data['f14'].replace(3000. + i,0 + i)\n",
    "#     for i in range(0,20):\n",
    "#         data['f17'] = data['f17'].replace(4001. + i,0 + i)\n",
    "#     for i in range(0,22):\n",
    "#         data['f22'] = data['f22'].replace(4999. + i,0 + i)\n",
    "#         return data\n",
    "\n",
    "# data = scaling(data)\n",
    "# if(flag is False):\n",
    "#     data_test = scaling(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 将f112、f14、f17、f22进行范围缩放后，xgboost的验证集loss由0.0775没变，没用。\n",
    "\n",
    "### 3.2.2正态化：对偏度大于0.75的数值特征（长尾分布）\n",
    "- 用log1p函数进行转化使其更加服从高斯分布\n",
    "np.log1p(train.SalePrice)\n",
    "- Box-Cox变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f7:2 f8:2.5 f9:5 f20:1.5\n",
    "# def boxcox(data):\n",
    "#     data['f7'] = boxcox1p(data['f7'],2)\n",
    "#     data['f8'] = boxcox1p(data['f8'],2.5)\n",
    "#     data['f9'] = boxcox1p(data['f9'],5)\n",
    "#     data['f20'] = boxcox1p(data['f20'],1.5)\n",
    "#     return data\n",
    "\n",
    "# data = boxcox(data)\n",
    "# if(flag is False):\n",
    "#     data_test = boxcox(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3离散化：Binning\n",
    "- 只有在了解属性的领域知识的基础，确定属性能够划分成简洁的范围时分箱才有意义，即所有的数值落入一个分区时能够呈现出共同的特征。\n",
    "- 当不想让模型总是尝试区分值之间是否太近时，分区可以避免出现过拟合。\n",
    "\n",
    "\n",
    "\n",
    "- 正态化后，xgboost的验证集loss不变，没用\n",
    "\n",
    "### 3.2.4时间数据的转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_f16(data):\n",
    "    now = pd.to_datetime(data['context_timestamp'],unit='s',utc = True)\n",
    "    now = now.apply(lambda x:x.astimezone(pytz.timezone('Asia/Shanghai')))\n",
    "    \n",
    "    # year = pd.Series([-1] * 477303)\n",
    "    # month = pd.Series([-1] * 477303)\n",
    "    day = now.apply(lambda x:x.day).rename('context_timestamp:day')\n",
    "    hour = now.apply(lambda x:x.hour).rename('context_timestamp:hour')\n",
    "    minute = now.apply(lambda x:x.minute).rename('context_timestamp:minute')\n",
    "    second = now.apply(lambda x:x.second).rename('context_timestamp:second')\n",
    "    dayofweek = now.apply(lambda x:x.dayofweek).rename('context_timestamp:dayofweek')\n",
    "\n",
    "    data = pd.concat([\n",
    "        data.loc[:,:'context_timestamp'],\n",
    "        day,\n",
    "        hour,\n",
    "        minute,\n",
    "        second,\n",
    "        dayofweek,\n",
    "        data.loc[:,'context_page_id':]\n",
    "    ], axis = 1)\n",
    "    \n",
    "    data['context_timestamp:day'] = data['context_timestamp:day'].map({31:0,1:1,2:2,3:3,4:4,5:5,6:6,7:7})\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = transform_f16(data)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12161692, 140) ['index' 'instance_id' 'item_id' 'item_category_list'\n",
      " 'item_category_list:1' 'item_category_list:2'\n",
      " 'item_category_list_2_3.949153947410967e+17'\n",
      " 'item_category_list_2_4.535254803665509e+17'\n",
      " 'item_category_list_2_5.974242234565864e+17'\n",
      " 'item_category_list_2_7.685797875215753e+17'\n",
      " 'item_category_list_2_9.262054013029023e+17'\n",
      " 'item_category_list_2_1.1470741689685322e+18'\n",
      " 'item_category_list_2_1.3671771540733827e+18'\n",
      " 'item_category_list_2_1.8526005172650624e+18'\n",
      " 'item_category_list_2_1.909641874861641e+18'\n",
      " 'item_category_list_2_1.9163903451332127e+18'\n",
      " 'item_category_list_2_1.9200841681043348e+18'\n",
      " 'item_category_list_2_2.211060154630359e+18'\n",
      " 'item_category_list_2_2.8717293836713016e+18'\n",
      " 'item_category_list_2_3.0892543029476204e+18'\n",
      " 'item_category_list_2_3.348197449185791e+18'\n",
      " 'item_category_list_2_3.434689896486063e+18'\n",
      " 'item_category_list_2_3.6137835631996273e+18'\n",
      " 'item_category_list_2_4.911723539855588e+18'\n",
      " 'item_category_list_2_5.066527928272239e+18'\n",
      " 'item_category_list_2_5.68569013987941e+18'\n",
      " 'item_category_list_2_6.254910033820815e+18'\n",
      " 'item_category_list_2_6.670526099037031e+18'\n",
      " 'item_category_list_2_6.693726201323252e+18'\n",
      " 'item_category_list_2_7.226013370341272e+18'\n",
      " 'item_category_list_2_7.314150500379498e+18'\n",
      " 'item_category_list_2_7.423553047267511e+18'\n",
      " 'item_category_list_2_8.009556227083202e+18'\n",
      " 'item_category_list_2_8.468007938333143e+18'\n",
      " 'item_category_list_2_8.46837010590862e+18'\n",
      " 'item_category_list_2_8.769426218101861e+18'\n",
      " 'item_category_list_2_8.841625760168847e+18' 'item_category_list_2_nan'\n",
      " 'item_category_list:3' 'item_category_list_3_-1.0'\n",
      " 'item_category_list_3_2.4292692654358803e+17'\n",
      " 'item_category_list_3_3.671363205864541e+17'\n",
      " 'item_category_list_3_6.69661864989165e+17'\n",
      " 'item_category_list_3_1.0360823570540439e+18'\n",
      " 'item_category_list_3_1.2112187474183503e+18'\n",
      " 'item_category_list_3_1.4678707293165668e+18'\n",
      " 'item_category_list_3_1.4773376140024678e+18'\n",
      " 'item_category_list_3_1.4953887999089408e+18'\n",
      " 'item_category_list_3_1.5189163023981494e+18'\n",
      " 'item_category_list_3_1.6484457152688369e+18'\n",
      " 'item_category_list_3_1.7737573992370422e+18'\n",
      " 'item_category_list_3_2.0667146032918502e+18'\n",
      " 'item_category_list_3_2.104555450664742e+18'\n",
      " 'item_category_list_3_2.1919081694948444e+18'\n",
      " 'item_category_list_3_2.3942618305577574e+18'\n",
      " 'item_category_list_3_2.633274017530891e+18'\n",
      " 'item_category_list_3_3.0350724682773396e+18'\n",
      " 'item_category_list_3_3.0561722848750684e+18'\n",
      " 'item_category_list_3_3.1239555576744207e+18'\n",
      " 'item_category_list_3_3.124898744105968e+18'\n",
      " 'item_category_list_3_3.492642177859571e+18'\n",
      " 'item_category_list_3_3.8766935579183923e+18'\n",
      " 'item_category_list_3_4.0395627406939643e+18'\n",
      " 'item_category_list_3_4.3324492077594117e+18'\n",
      " 'item_category_list_3_4.4237569224751427e+18'\n",
      " 'item_category_list_3_4.521717672641793e+18'\n",
      " 'item_category_list_3_4.5598378274862894e+18'\n",
      " 'item_category_list_3_4.62004237070379e+18'\n",
      " 'item_category_list_3_4.642777236640082e+18'\n",
      " 'item_category_list_3_4.782940324184009e+18'\n",
      " 'item_category_list_3_4.919080589661355e+18'\n",
      " 'item_category_list_3_4.993377953084092e+18'\n",
      " 'item_category_list_3_5.031957903352581e+18'\n",
      " 'item_category_list_3_5.103300281712297e+18'\n",
      " 'item_category_list_3_5.575767811251205e+18'\n",
      " 'item_category_list_3_5.896127768779494e+18'\n",
      " 'item_category_list_3_5.941301806663871e+18'\n",
      " 'item_category_list_3_6.119230265477212e+18'\n",
      " 'item_category_list_3_6.370392357088947e+18'\n",
      " 'item_category_list_3_6.731593623692005e+18'\n",
      " 'item_category_list_3_6.747145348367564e+18'\n",
      " 'item_category_list_3_6.783634657823732e+18'\n",
      " 'item_category_list_3_6.861763693457621e+18'\n",
      " 'item_category_list_3_7.017624410306476e+18'\n",
      " 'item_category_list_3_7.438434275307066e+18'\n",
      " 'item_category_list_3_7.497531498747093e+18'\n",
      " 'item_category_list_3_7.505733337486864e+18'\n",
      " 'item_category_list_3_7.848078693615387e+18'\n",
      " 'item_category_list_3_7.978017778431587e+18'\n",
      " 'item_category_list_3_8.010602907289638e+18'\n",
      " 'item_category_list_3_8.045794075432612e+18'\n",
      " 'item_category_list_3_8.077497621372666e+18'\n",
      " 'item_category_list_3_8.096917686185033e+18'\n",
      " 'item_category_list_3_8.110966937369051e+18'\n",
      " 'item_category_list_3_8.123435010580049e+18'\n",
      " 'item_category_list_3_8.418308787262291e+18'\n",
      " 'item_category_list_3_8.72739728582695e+18'\n",
      " 'item_category_list_3_8.807331407484389e+18'\n",
      " 'item_category_list_3_8.82646757569844e+18'\n",
      " 'item_category_list_3_9.030622377694665e+18' 'item_category_list_3_nan'\n",
      " 'item_property_list' 'item_brand_id' 'item_city_id' 'item_price_level'\n",
      " 'item_sales_level' 'item_collected_level' 'item_pv_level' 'user_id'\n",
      " 'user_gender_id' 'user_gender_id:_0.0' 'user_gender_id:_1.0'\n",
      " 'user_gender_id:_2.0' 'user_gender_id:_nan' 'user_age_level'\n",
      " 'user_occupation_id' 'user_occupation_id__2002.0'\n",
      " 'user_occupation_id__2003.0' 'user_occupation_id__2004.0'\n",
      " 'user_occupation_id__2005.0' 'user_occupation_id__nan' 'user_star_level'\n",
      " 'context_id' 'context_timestamp' 'context_timestamp:day'\n",
      " 'context_timestamp:hour' 'context_timestamp:minute'\n",
      " 'context_timestamp:second' 'context_timestamp:dayofweek'\n",
      " 'context_page_id' 'predict_category_property' 'shop_id'\n",
      " 'shop_review_num_level' 'shop_review_positive_rate' 'shop_star_level'\n",
      " 'shop_score_service' 'shop_score_delivery' 'shop_score_description'\n",
      " 'label' 'flag']\n"
     ]
    }
   ],
   "source": [
    "print(data.shape,data.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 转换时间f16为多个特征并删除特征f16后，xgboost的验证集loss较明显的降低\n",
    "- 加入dayofweek特征后，xgboost的验证集loss没变，没用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4保存特征转换、衍生的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle('../2.data/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle('../2.data/data')\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12161692, 140) ['index' 'instance_id' 'item_id' 'item_category_list'\n",
      " 'item_category_list:1' 'item_category_list:2'\n",
      " 'item_category_list_2_3.949153947410967e+17'\n",
      " 'item_category_list_2_4.535254803665509e+17'\n",
      " 'item_category_list_2_5.974242234565864e+17'\n",
      " 'item_category_list_2_7.685797875215753e+17'\n",
      " 'item_category_list_2_9.262054013029023e+17'\n",
      " 'item_category_list_2_1.1470741689685322e+18'\n",
      " 'item_category_list_2_1.3671771540733827e+18'\n",
      " 'item_category_list_2_1.8526005172650624e+18'\n",
      " 'item_category_list_2_1.909641874861641e+18'\n",
      " 'item_category_list_2_1.9163903451332127e+18'\n",
      " 'item_category_list_2_1.9200841681043348e+18'\n",
      " 'item_category_list_2_2.211060154630359e+18'\n",
      " 'item_category_list_2_2.8717293836713016e+18'\n",
      " 'item_category_list_2_3.0892543029476204e+18'\n",
      " 'item_category_list_2_3.348197449185791e+18'\n",
      " 'item_category_list_2_3.434689896486063e+18'\n",
      " 'item_category_list_2_3.6137835631996273e+18'\n",
      " 'item_category_list_2_4.911723539855588e+18'\n",
      " 'item_category_list_2_5.066527928272239e+18'\n",
      " 'item_category_list_2_5.68569013987941e+18'\n",
      " 'item_category_list_2_6.254910033820815e+18'\n",
      " 'item_category_list_2_6.670526099037031e+18'\n",
      " 'item_category_list_2_6.693726201323252e+18'\n",
      " 'item_category_list_2_7.226013370341272e+18'\n",
      " 'item_category_list_2_7.314150500379498e+18'\n",
      " 'item_category_list_2_7.423553047267511e+18'\n",
      " 'item_category_list_2_8.009556227083202e+18'\n",
      " 'item_category_list_2_8.468007938333143e+18'\n",
      " 'item_category_list_2_8.46837010590862e+18'\n",
      " 'item_category_list_2_8.769426218101861e+18'\n",
      " 'item_category_list_2_8.841625760168847e+18' 'item_category_list_2_nan'\n",
      " 'item_category_list:3' 'item_category_list_3_-1.0'\n",
      " 'item_category_list_3_2.4292692654358803e+17'\n",
      " 'item_category_list_3_3.671363205864541e+17'\n",
      " 'item_category_list_3_6.69661864989165e+17'\n",
      " 'item_category_list_3_1.0360823570540439e+18'\n",
      " 'item_category_list_3_1.2112187474183503e+18'\n",
      " 'item_category_list_3_1.4678707293165668e+18'\n",
      " 'item_category_list_3_1.4773376140024678e+18'\n",
      " 'item_category_list_3_1.4953887999089408e+18'\n",
      " 'item_category_list_3_1.5189163023981494e+18'\n",
      " 'item_category_list_3_1.6484457152688369e+18'\n",
      " 'item_category_list_3_1.7737573992370422e+18'\n",
      " 'item_category_list_3_2.0667146032918502e+18'\n",
      " 'item_category_list_3_2.104555450664742e+18'\n",
      " 'item_category_list_3_2.1919081694948444e+18'\n",
      " 'item_category_list_3_2.3942618305577574e+18'\n",
      " 'item_category_list_3_2.633274017530891e+18'\n",
      " 'item_category_list_3_3.0350724682773396e+18'\n",
      " 'item_category_list_3_3.0561722848750684e+18'\n",
      " 'item_category_list_3_3.1239555576744207e+18'\n",
      " 'item_category_list_3_3.124898744105968e+18'\n",
      " 'item_category_list_3_3.492642177859571e+18'\n",
      " 'item_category_list_3_3.8766935579183923e+18'\n",
      " 'item_category_list_3_4.0395627406939643e+18'\n",
      " 'item_category_list_3_4.3324492077594117e+18'\n",
      " 'item_category_list_3_4.4237569224751427e+18'\n",
      " 'item_category_list_3_4.521717672641793e+18'\n",
      " 'item_category_list_3_4.5598378274862894e+18'\n",
      " 'item_category_list_3_4.62004237070379e+18'\n",
      " 'item_category_list_3_4.642777236640082e+18'\n",
      " 'item_category_list_3_4.782940324184009e+18'\n",
      " 'item_category_list_3_4.919080589661355e+18'\n",
      " 'item_category_list_3_4.993377953084092e+18'\n",
      " 'item_category_list_3_5.031957903352581e+18'\n",
      " 'item_category_list_3_5.103300281712297e+18'\n",
      " 'item_category_list_3_5.575767811251205e+18'\n",
      " 'item_category_list_3_5.896127768779494e+18'\n",
      " 'item_category_list_3_5.941301806663871e+18'\n",
      " 'item_category_list_3_6.119230265477212e+18'\n",
      " 'item_category_list_3_6.370392357088947e+18'\n",
      " 'item_category_list_3_6.731593623692005e+18'\n",
      " 'item_category_list_3_6.747145348367564e+18'\n",
      " 'item_category_list_3_6.783634657823732e+18'\n",
      " 'item_category_list_3_6.861763693457621e+18'\n",
      " 'item_category_list_3_7.017624410306476e+18'\n",
      " 'item_category_list_3_7.438434275307066e+18'\n",
      " 'item_category_list_3_7.497531498747093e+18'\n",
      " 'item_category_list_3_7.505733337486864e+18'\n",
      " 'item_category_list_3_7.848078693615387e+18'\n",
      " 'item_category_list_3_7.978017778431587e+18'\n",
      " 'item_category_list_3_8.010602907289638e+18'\n",
      " 'item_category_list_3_8.045794075432612e+18'\n",
      " 'item_category_list_3_8.077497621372666e+18'\n",
      " 'item_category_list_3_8.096917686185033e+18'\n",
      " 'item_category_list_3_8.110966937369051e+18'\n",
      " 'item_category_list_3_8.123435010580049e+18'\n",
      " 'item_category_list_3_8.418308787262291e+18'\n",
      " 'item_category_list_3_8.72739728582695e+18'\n",
      " 'item_category_list_3_8.807331407484389e+18'\n",
      " 'item_category_list_3_8.82646757569844e+18'\n",
      " 'item_category_list_3_9.030622377694665e+18' 'item_category_list_3_nan'\n",
      " 'item_property_list' 'item_brand_id' 'item_city_id' 'item_price_level'\n",
      " 'item_sales_level' 'item_collected_level' 'item_pv_level' 'user_id'\n",
      " 'user_gender_id' 'user_gender_id:_0.0' 'user_gender_id:_1.0'\n",
      " 'user_gender_id:_2.0' 'user_gender_id:_nan' 'user_age_level'\n",
      " 'user_occupation_id' 'user_occupation_id__2002.0'\n",
      " 'user_occupation_id__2003.0' 'user_occupation_id__2004.0'\n",
      " 'user_occupation_id__2005.0' 'user_occupation_id__nan' 'user_star_level'\n",
      " 'context_id' 'context_timestamp' 'context_timestamp:day'\n",
      " 'context_timestamp:hour' 'context_timestamp:minute'\n",
      " 'context_timestamp:second' 'context_timestamp:dayofweek'\n",
      " 'context_page_id' 'predict_category_property' 'shop_id'\n",
      " 'shop_review_num_level' 'shop_review_positive_rate' 'shop_star_level'\n",
      " 'shop_score_service' 'shop_score_delivery' 'shop_score_description'\n",
      " 'label' 'flag']\n"
     ]
    }
   ],
   "source": [
    "print(data.shape,data.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 用了embedding处理f3后，验证集的logloss由0.081953下降到了0.081787"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.特征组合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1高势集数据（High Categorical）\n",
    "### 4.1.1高势集类别进行经验贝叶斯转换成数值feature\n",
    "### 4.1.2平均数编码\n",
    "- 平均数编码（mean encoding），针对高基数类别特征的有监督编码。当一个类别特征列包括了极多不同类别时（如家庭地址，动辄上万）时，可以采用。优点：和独热编码相比，节省内存、减少算法计算时间、有效增强模型表现。\n",
    "\n",
    "\n",
    "- 将f2属性拆分成三个子属性后，xgboost的验证集loss不变，没用\n",
    "\n",
    "### 4.1.3word embedding\n",
    "#### 4.1.3.1抽取f3数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10951924\n",
      "10951924\n"
     ]
    }
   ],
   "source": [
    "# 抽取预测property\n",
    "def predict_func(x):\n",
    "    res = []\n",
    "    if(type(x) == float):\n",
    "        return res\n",
    "\n",
    "    for now in x.split(';'):\n",
    "        if(len(now.split(':')) != 1):\n",
    "            res += now.split(':')[1].split(',')\n",
    "    if('-1' in res):\n",
    "        # print(1)\n",
    "        here = set(res)\n",
    "        here.remove('-1')\n",
    "        return list(here)\n",
    "    else:\n",
    "        return list(set(res))\n",
    "\n",
    "# 抽取真实property\n",
    "def real_func(x):\n",
    "    if(type(x) == float):\n",
    "        return []\n",
    "    return x.split(';')\n",
    "\n",
    "\n",
    "predict_property_list = data['predict_category_property'].apply(predict_func).tolist()\n",
    "real_property_list = data['item_property_list'].apply(real_func).tolist()\n",
    "\n",
    "\n",
    "print(len(predict_property_list))\n",
    "print(len(real_property_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(not os.path.exists('../2.data/predict_and_real_property')):\n",
    "    file = open('../2.data/predict_and_real_property','w')\n",
    "    assert len(predict_property_list) == len(real_property_list)\n",
    "\n",
    "    for i in range(len(predict_property_list)):\n",
    "        if(len(predict_property_list[i]) == 0):\n",
    "            file.write('\\n')\n",
    "        for j in range(len(predict_property_list[i])):\n",
    "            if(j == 0 and j == len(predict_property_list[i]) - 1):\n",
    "                file.write(predict_property_list[i][j] + '\\n')\n",
    "            elif(j == 0):\n",
    "                file.write(predict_property_list[i][j])\n",
    "            elif(j == len(predict_property_list[i]) - 1):\n",
    "                file.write(' ' + predict_property_list[i][j] + '\\n')\n",
    "            else:\n",
    "                file.write(' ' + predict_property_list[i][j])\n",
    "\n",
    "        if(len(real_property_list[i]) == 0):\n",
    "            file.write('\\n')\n",
    "        for j in range(len(real_property_list[i])):\n",
    "            if(j == 0 and j == len(real_property_list[i]) - 1):\n",
    "                file.write(real_property_list[i][j] + '\\n')\n",
    "            elif(j == 0):\n",
    "                file.write(real_property_list[i][j])\n",
    "            elif(j == len(real_property_list[i]) - 1):\n",
    "                file.write(' ' + real_property_list[i][j] + '\\n')\n",
    "            else:\n",
    "                file.write(' ' + real_property_list[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3.2doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunshuchong/anaconda3/lib/python3.6/site-packages/gensim/models/doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "# 模型训练\n",
    "from gensim.models.doc2vec import Doc2Vec,TaggedLineDocument\n",
    "if not os.path.exists('../1.model/item_property_list_doc_model'):\n",
    "    sentences = TaggedLineDocument('../2.data/predict_and_real_property')\n",
    "    model = Doc2Vec(sentences,size=20, window=10, min_count=1, negative=3, hs=0,workers = 22)\n",
    "    model.save('../1.model/item_property_list_doc_model')  \n",
    "else:\n",
    "    model = Doc2Vec.load('../1.model/item_property_list_doc_model')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos(vector1,vector2):  \n",
    "    dot_product = 0.0;  \n",
    "    normA = 0.0;  \n",
    "    normB = 0.0;  \n",
    "    for a,b in zip(vector1,vector2):  \n",
    "        dot_product += a*b  \n",
    "        normA += a**2  \n",
    "        normB += b**2  \n",
    "    if normA == 0.0 or normB==0.0:  \n",
    "        return None  \n",
    "    else:  \n",
    "        return dot_product / ((normA*normB)**0.5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('../2.data/features/embedding-similarity'):\n",
    "    temp = data.reset_index(drop = True).apply(\n",
    "        lambda x:\n",
    "        cos(\n",
    "            model.infer_vector(real_property_list[x.name]),\n",
    "            model.infer_vector(predict_property_list[x.name])\n",
    "           ),axis = 1)\n",
    "    temp.index = data.index\n",
    "    temp = pd.concat([data.loc[:,'index'],temp],axis = 1)\n",
    "    temp.sort_values(['index'],inplace = True)\n",
    "    temp = temp.drop(['index'],axis = 1)[0].rename('embedding-similarity').reset_index(drop = True)\n",
    "    # 保存\n",
    "    temp.to_pickle('../2.data/features/embedding-similarity')\n",
    "else:\n",
    "    temp = pd.read_pickle('../2.data/features/embedding-similarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "del temp\n",
    "del real_property_list\n",
    "del predict_property_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4计算Jaccard相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def func_predict(x):\n",
    "    res = []\n",
    "    if(type(x) != str):\n",
    "        print(x)\n",
    "        return set([])\n",
    "    for cat in x.split(';'):\n",
    "        for pro in cat.split(':')[1].split(','):\n",
    "            if(pro != '-1'):\n",
    "                res.append(pro)\n",
    "    return set(res)\n",
    "\n",
    "def func_real(x):\n",
    "    if(type(x) != str):\n",
    "        return set([])\n",
    "    return set(x.split(';'))\n",
    "\n",
    "def get_jaccard(real,predict):\n",
    "    temp = pd.Series([-1.0] * len(real))\n",
    "    for i in range(len(temp)):\n",
    "        temp[i] = len(real.iloc[i] & predict.iloc[i]) * 1.0 / len(real.iloc[i] | predict.iloc[i])\n",
    "    return temp.rename('property-jaccard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('../2.data/myfeature/property-jaccard'):\n",
    "    predict = data['predict_category_property'].apply(func_predict)\n",
    "    gc.collect()\n",
    "    real = data['item_property_list'].apply(func_real)\n",
    "    gc.collect()\n",
    "    jaccard = get_jaccard(real,predict)\n",
    "    gc.collect()\n",
    "    \n",
    "    jaccard.index = data.index\n",
    "    jaccard.to_pickle('../2.data/myfeature/property-jaccard')\n",
    "else:\n",
    "    jaccard = pd.read_pickle('../2.data/myfeature/property-jaccard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del jaccard\n",
    "del real\n",
    "del predict\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([\n",
    "    'item_category_list',\n",
    "    'item_property_list',\n",
    "    'predict_category_property',\n",
    "],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2当前点击前若干天购买/点击次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = 7 # 滑动窗口大小\n",
    "day_num = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func1(x,temp,name):\n",
    "#     if(x['flag'] == 0):\n",
    "#         return np.nan\n",
    "    return temp[x[name]] if(x[name] in temp) else np.nan\n",
    "\n",
    "def func2(x,temp,name):\n",
    "#     if(x['flag'] == 0):\n",
    "#         return np.nan\n",
    "    return temp[(x[name[0]],x[name[1]])] if((x[name[0]],x[name[1]]) in temp) else np.nan\n",
    "\n",
    "def get_count_temp(data,i,name):\n",
    "    temp = data[\n",
    "        (data['context_timestamp:day'] >= i - days) & \n",
    "        (data['context_timestamp:day'] <= i - 1)\n",
    "    ].groupby(name).apply(lambda x:x['label'].sum()/(1 if i == 0 else i))  # 前n天出现商品的转化率=\n",
    "\n",
    "    \n",
    "    if(len(name) == 1):\n",
    "        name = name[0]\n",
    "        res = data[data['context_timestamp:day'] == i].apply(func1,axis = 1,args = (temp,name))\n",
    "        del temp\n",
    "        gc.collect()\n",
    "        return res\n",
    "    else:\n",
    "        res = data[data['context_timestamp:day'] == i].apply(func2,axis = 1,args = (temp,name))\n",
    "        del temp\n",
    "        gc.collect()\n",
    "        return res\n",
    "    \n",
    "def get_count(data,name):\n",
    "    temp = pd.Series([])\n",
    "    for i in range(day_num):\n",
    "        now = get_count_temp(data,i,name)\n",
    "        temp = temp.append(now)\n",
    "        del now\n",
    "    if(len(name) == 1):\n",
    "        return temp.rename(name[0] + '-buy-count-' + str(days))\n",
    "    else:\n",
    "        return temp.rename(name[0] + '-' + name[1] + '-buy-count-' + str(days))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_browse_count_temp(data,i,name):\n",
    "    temp = data[\n",
    "        (data['context_timestamp:day'] >= i - days) & \n",
    "        (data['context_timestamp:day'] <= i - 1)\n",
    "    ].groupby(name).apply(lambda x:len(x)/(1 if i == 0 else i))  # 前n天出现商品的转化率=\n",
    "\n",
    "    if(len(name) == 1):\n",
    "        name = name[0]\n",
    "        res = data[data['context_timestamp:day'] == i].apply(func1,axis = 1,args = (temp,name))\n",
    "        del temp\n",
    "        gc.collect()\n",
    "        return res\n",
    "    else:\n",
    "        res = data[data['context_timestamp:day'] == i].apply(func2,axis = 1,args = (temp,name))\n",
    "        del temp\n",
    "        gc.collect()\n",
    "        return res\n",
    "    \n",
    "def get_browse_count(data,name):\n",
    "    temp = pd.Series([])\n",
    "    for i in range(day_num):\n",
    "        now = get_browse_count_temp(data,i,name)\n",
    "        temp = temp.append(now)\n",
    "\n",
    "        del now\n",
    "    if(len(name) == 1):\n",
    "        return temp.rename(name[0] + '-browse-count-' + str(days))\n",
    "    else:\n",
    "        return temp.rename(name[0] + '-' + name[1] + '-browse-count-' + str(days))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin concat!\n",
      "2:24:22.456006\n"
     ]
    }
   ],
   "source": [
    "# 2h 35min\n",
    "data_combination = pd.DataFrame()\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "if(not os.path.exists('../2.data/myfeature/buy_and_browse_count_' + str(days))):\n",
    "    features = ['item_id','item_brand_id',\\\n",
    "#                 'item_city_id',\n",
    "                'user_id','shop_id']\n",
    "    res = {}\n",
    "    \n",
    "    # 并发执行\n",
    "    pros = Pool(20)\n",
    "    \n",
    "    # buy-count\n",
    "    for feature in features:\n",
    "        temp = pd.concat([\n",
    "            data.loc[:,feature],\n",
    "            data.loc[:,'context_timestamp:day'],\n",
    "            data.loc[:,'label'],\n",
    "#             data.loc[:,'flag']\n",
    "        ],axis = 1)\n",
    "        res[feature + '-buy'] = pros.apply_async(get_count,(temp,list((feature,)),))\n",
    "            \n",
    "    for i in range(len(features)):\n",
    "        for j in range(i + 1,len(features)):\n",
    "            feature = features[i] + '-' + features[j]\n",
    "            temp = pd.concat([\n",
    "                data.loc[:,features[i]],\n",
    "                data.loc[:,features[j]],\n",
    "                data.loc[:,'context_timestamp:day'],\n",
    "                data.loc[:,'label'],\n",
    "#                 data.loc[:,'flag']\n",
    "            ],axis = 1)\n",
    "            res[feature + '-buy'] = pros.apply_async(get_count,(temp,list((features[i],features[j])),))\n",
    "    \n",
    "    # browse-count\n",
    "    for feature in features:\n",
    "        temp = pd.concat([\n",
    "            data.loc[:,feature],\n",
    "            data.loc[:,'context_timestamp:day'],\n",
    "#             data.loc[:,'flag']\n",
    "        ],axis = 1)\n",
    "        res[feature + '-browse'] = pros.apply_async(get_browse_count,(temp,list((feature,)),))\n",
    "            \n",
    "    for i in range(len(features)):\n",
    "        for j in range(i + 1,len(features)):\n",
    "            feature = features[i] + '-' + features[j]\n",
    "            temp = pd.concat([\n",
    "                data.loc[:,features[i]],\n",
    "                data.loc[:,features[j]],\n",
    "                data.loc[:,'context_timestamp:day'],\n",
    "#                 data.loc[:,'flag']\n",
    "            ],axis = 1)\n",
    "            res[feature + '-browse'] = pros.apply_async(get_browse_count,(temp,list((features[i],features[j])),))\n",
    "\n",
    "    \n",
    "    pros.close()\n",
    "    pros.join()\n",
    "    print('Begin concat!')\n",
    "    \n",
    "    # 组合\n",
    "    for now in res:\n",
    "        data_combination = pd.concat([\n",
    "            data_combination,\n",
    "            res[now].get()\n",
    "        ],axis = 1)\n",
    "\n",
    "        # 保存\n",
    "    data_combination.to_pickle('../2.data/myfeature/buy_and_browse_count_' + str(days))\n",
    "\n",
    "else:\n",
    "    data_combination = pd.read_pickle('../2.data/myfeature/buy_and_browse_count_' + str(days))\n",
    "print(datetime.datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4当前点击前若干天转化率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_combination = pd.DataFrame()\n",
    "if(not os.path.exists('../2.data/myfeature/ratio_' + str(days))):\n",
    "    temp = pd.read_pickle('../2.data/myfeature/buy_and_browse_count_' + str(days))\n",
    "    \n",
    "    data_combination = pd.DataFrame()\n",
    "    \n",
    "    for feature in features:\n",
    "        data_combination = pd.concat([\n",
    "            data_combination,\n",
    "            (temp[feature + '-buy-count-' + str(days)] * 1.0/temp[feature + '-browse-count-' + str(days)]).rename(feature + '-ratio-' + str(days))\n",
    "        ],axis = 1)\n",
    "            \n",
    "    for i in range(len(features)):\n",
    "        for j in range(i + 1,len(features)):\n",
    "            feature = features[i] + '-' + features[j]\n",
    "            data_combination = pd.concat([\n",
    "                data_combination,\n",
    "                (temp[feature + '-buy-count-' + str(days)] * 1.0/temp[feature + '-browse-count-' + str(days)]).rename(feature + '-ratio-' + str(days))\n",
    "            ],axis = 1)\n",
    "    \n",
    "    data_combination.to_pickle('../2.data/myfeature/ratio_' + str(days))\n",
    "\n",
    "else:\n",
    "    data_combination = pd.read_pickle('../2.data/myfeature/ratio_' + str(days))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 加入浏览次数特征后，xgb的logloss由0.08201下降到0.081756\n",
    "- 将f19进行转换为点击概率后，xgboost的验证集loss由0.084273减小到0.080542\n",
    "- 将f1进行转换为点击概率后，xgboost的验证集loss由0.080542减小到0.0775\n",
    "- 将f5进行转换为点击概率后，lgb的验证集loss由0.08287减小到0.08221 \n",
    "- 将f10进行转换为点击概率后，lgb的验证集loss由0.08221减小到0.0821587\n",
    "\n",
    "- 加上f1-f10-label、f1-f4-label、f1-f5-label、f1-f19-label后，lbg的logloss从0.082215下降到0.0820568\n",
    "- 加上f4-f5-label、f4-f10-label、f4-f19-label、f5-f10-label、f5-f19-label、f10-f19后，lbg的logloss从0.0820568下降到0.0819415\n",
    "\n",
    "## 4.5展示、收藏、销量之间的比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not os.path.exists('../2.data/myfeature/proportion')):\n",
    "    data_combination = pd.DataFrame()\n",
    "    \n",
    "    # 展示（f9）-收藏（f8）\n",
    "    data_combination = pd.concat([\n",
    "        data_combination,\n",
    "        (data['item_pv_level'] * 1.0 / data['item_collected_level']).rename('item_collected_level-item_pv_level-proportion')\n",
    "    ],axis = 1)\n",
    "    \n",
    "    # 收藏（f8）-销量（f7）\n",
    "    data_combination = pd.concat([\n",
    "        data_combination,\n",
    "        (data['item_collected_level'] * 1.0 / data['item_sales_level']).rename('item_sales_level-item_collected_level-proportion')\n",
    "    ],axis = 1)\n",
    "\n",
    "    # 展示（f9）-销量（f7）\n",
    "    data_combination = pd.concat([\n",
    "        data_combination,\n",
    "        (data['item_pv_level'] * 1.0 / data['item_sales_level']).rename('item_sales_level-item_pv_level-proportion')\n",
    "    ],axis = 1)\n",
    "\n",
    "    data_combination.to_pickle('../2.data/myfeature/proportion')\n",
    "\n",
    "else:\n",
    "    data_combination = pd.read_pickle('../2.data/myfeature/proportion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6动作前后浏览量（leak）\n",
    "### 4.6.1当日"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:08:09.485089\n"
     ]
    }
   ],
   "source": [
    "# 1小时\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "def func_one(x,dic,feature):\n",
    "    try:\n",
    "        return dic[(x[feature],x['context_timestamp:day'])]\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def func_two(x,dic,feature):\n",
    "    try:\n",
    "        return dic[(x[feature[0]],x[feature[1]],x['context_timestamp:day'])]\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "if(not os.path.exists('../2.data/myfeature/browse_count_today')):\n",
    "    features = ['item_id','item_brand_id',\n",
    "#                 'item_city_id',\n",
    "                'user_id','shop_id']\n",
    "    data_combination = pd.DataFrame()\n",
    "    \n",
    "    for feature in features:\n",
    "        temp = data.groupby([feature,'context_timestamp:day']).apply(lambda x:len(x)).to_dict()\n",
    "        data_combination = pd.concat([\n",
    "            data_combination,\n",
    "            data.apply(func_one,axis = 1,args = (temp,feature)).rename(feature + '-browse-count-today')\n",
    "        ],axis = 1)\n",
    "    \n",
    "    for i in range(len(features)):\n",
    "        for j in range(i + 1,len(features)):\n",
    "            feature = features[i] + '-' + features[j]\n",
    "            temp = data.groupby([features[i],features[j],'context_timestamp:day']).apply(lambda x:len(x)).to_dict()\n",
    "            data_combination = pd.concat([\n",
    "                data_combination,\n",
    "                data.apply(func_two,axis = 1,args = (temp,(features[i],features[j]))).rename(feature + '-browse-count-today')\n",
    "            ],axis = 1)\n",
    "    \n",
    "    # 保存\n",
    "    data_combination.to_pickle('../2.data/myfeature/browse_count_today')\n",
    "else:\n",
    "    data_combination = pd.read_pickle('../2.data/myfeature/browse_count_today')\n",
    "print(datetime.datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.2当小时"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:26:03.785208\n"
     ]
    }
   ],
   "source": [
    "# 1.5h\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "def func_one(x,dic,feature):\n",
    "    try:\n",
    "        return dic[(x[feature],x['context_timestamp:day'],x['context_timestamp:hour'])]\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def func_two(x,dic,feature):\n",
    "    try:\n",
    "        return dic[(x[feature[0]],x[feature[1]],x['context_timestamp:day'],x['context_timestamp:hour'])]\n",
    "    except:\n",
    "        return np.nan\n",
    "if(not os.path.exists('../2.data/myfeature/browse_count_tohour')):\n",
    "    res = []\n",
    "    data_combination = pd.DataFrame()\n",
    "    \n",
    "    for feature in features:\n",
    "        temp = data.groupby([feature,'context_timestamp:day','context_timestamp:hour']).apply(lambda x:len(x)).to_dict()\n",
    "        data_combination = pd.concat([\n",
    "            data_combination,\n",
    "            data.apply(func_one,axis = 1,args = (temp,feature)).rename(feature + '-browse-count-tohour')\n",
    "        ],axis = 1)\n",
    "    \n",
    "    for i in range(len(features)):\n",
    "        for j in range(i + 1,len(features)):\n",
    "            feature = features[i] + '-' + features[j]\n",
    "            temp = data.groupby([features[i],features[j],'context_timestamp:day','context_timestamp:hour']).apply(lambda x:len(x)).to_dict()\n",
    "            data_combination = pd.concat([\n",
    "                data_combination,\n",
    "                data.apply(func_two,axis = 1,args = (temp,(features[i],features[j]))).rename(feature + '-browse-count-tohour')\n",
    "            ],axis = 1)\n",
    "    \n",
    "    # 保存\n",
    "    data_combination.to_pickle('../2.data/myfeature/browse_count_tohour')\n",
    "else:\n",
    "    data_combination = pd.read_pickle('../2.data/myfeature/browse_count_tohour')\n",
    "print(datetime.datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7冷启动特征（leak）\n",
    "### 4.7.1当天"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:42:06.548577\n"
     ]
    }
   ],
   "source": [
    "# 45min\n",
    "start = datetime.datetime.now()\n",
    "def func_one(x):\n",
    "    try:\n",
    "        return x.iloc[0].name\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "def func_two(x,dic):\n",
    "    try:\n",
    "        return 1 if x.name == dic[(x['user_id'],x['context_timestamp:day'])] else 0\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "def func_three(x):\n",
    "    try:\n",
    "        return x.iloc[-1].name\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "def func_four(x,dic):\n",
    "    try:\n",
    "        return 1 if x.name == dic[(x['user_id'],x['context_timestamp:day'])] else 0\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "if(not os.path.exists('../2.data/myfeature/day_browse_is_last')):\n",
    "    res = []\n",
    "    data_combination = pd.DataFrame()\n",
    "    \n",
    "    # 执行\n",
    "    # 是否第一次\n",
    "    dic = data.groupby(['user_id','context_timestamp:day']).apply(func_one).to_dict() # index？\n",
    "    temp = data.apply(func_two,axis = 1,args = (dic,))\n",
    "    data_combination = pd.concat([\n",
    "        data_combination,\n",
    "        temp.rename('user_id-day-browse-is-first')\n",
    "    ],axis = 1)\n",
    "       \n",
    "    # 是否最后一次\n",
    "    dic = data.groupby(['user_id','context_timestamp:day']).apply(func_three).to_dict() # index？\n",
    "    temp = data.apply(func_four,axis = 1,args = (dic,))\n",
    "    data_combination = pd.concat([\n",
    "        data_combination,\n",
    "        temp.rename('user_id-day-browse-is-last')\n",
    "    ],axis = 1)\n",
    "\n",
    "    # 保存\n",
    "    data_combination.to_pickle('../2.data/myfeature/day_browse_is_last')\n",
    "\n",
    "else:\n",
    "    data_combination = pd.read_pickle('../2.data/myfeature/day_browse_is_last')\n",
    "print(datetime.datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.2当小时"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:55:06.649115\n"
     ]
    }
   ],
   "source": [
    "# 1h\n",
    "start = datetime.datetime.now()\n",
    "def func_one(x):\n",
    "    try:\n",
    "        return x.iloc[0].name\n",
    "    except:\n",
    "        return np.nan\n",
    "def func_two(x,dic):\n",
    "    try:\n",
    "        return 1 if x.name == dic[(x['user_id'],x['context_timestamp:day'],x['context_timestamp:hour'])] else 0\n",
    "    except:\n",
    "        return np.nan\n",
    "def func_three(x):\n",
    "    try:\n",
    "        return x.iloc[-1].name\n",
    "    except:\n",
    "        return np.nan\n",
    "def func_four(x,dic):\n",
    "    try:\n",
    "        return 1 if x.name == dic[(x['user_id'],x['context_timestamp:day'],x['context_timestamp:hour'])] else 0\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "if(not os.path.exists('../2.data/myfeature/hour_browse_is_last')):\n",
    "    res = []\n",
    "    data_combination = pd.DataFrame()\n",
    "\n",
    "    # 执行\n",
    "    # 是否第一次\n",
    "    dic = data.groupby(['user_id','context_timestamp:day','context_timestamp:hour']).apply(func_one).to_dict() # index？\n",
    "    temp = data.apply(func_two,axis = 1,args = (dic,))\n",
    "\n",
    "    data_combination = pd.concat([\n",
    "        data_combination,\n",
    "        temp.rename('user_id-hour-browse-is-first')\n",
    "    ],axis = 1)\n",
    "\n",
    "    # 是否最后一次\n",
    "    dic = data.groupby(['user_id','context_timestamp:day','context_timestamp:hour']).apply(func_three).to_dict() # index？\n",
    "    temp = data.apply(func_four,axis = 1,args = (dic,))\n",
    "\n",
    "    data_combination = pd.concat([\n",
    "        data_combination,\n",
    "        temp.rename('user_id-hour-browse-is-last')\n",
    "    ],axis = 1)\n",
    "    \n",
    "    # 保存\n",
    "    data_combination.to_pickle('../2.data/myfeature/hour_browse_is_last')\n",
    "else:\n",
    "    data_combination = pd.read_pickle('../2.data/myfeature/hour_browse_is_last')\n",
    "print(datetime.datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8上次/下次到这次浏览\n",
    "### 4.8.1到这次浏览的时间间隔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_browse_time_interval(data,feature):\n",
    "    now = data.groupby(feature)\n",
    "    \n",
    "    def func(x,feature):\n",
    "#         if(x['flag'] == 0):\n",
    "#             return np.nan\n",
    "        try:\n",
    "            if(len(feature) == 1):\n",
    "                feature = x[feature[0]]\n",
    "            else:\n",
    "                feature = tuple((x[feature[0]],x[feature[1]]))\n",
    "\n",
    "            here = now.get_group(feature).loc[:x.name]\n",
    "            if(len(here) >= 2):\n",
    "                return x['context_timestamp'] - here.iloc[-2]['context_timestamp']\n",
    "            return np.nan\n",
    "        except:\n",
    "            return np.nan\n",
    "    if(len(feature) == 1):\n",
    "        res = data.apply(func,args = (feature,),axis = 1).rename(feature[0] + '-last-browse-time-interval')\n",
    "    else:\n",
    "        res = data.apply(func,args = (feature,),axis = 1).rename(feature[0] + '-' + feature[1] + '-last-browse-time-interval')\n",
    "    del now\n",
    "    res.to_pickle('../2.data/myfeature/temp/' + res.name)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_browse_time_interval(data,feature):\n",
    "    now = data.groupby(feature)\n",
    "\n",
    "    def func(x,feature):\n",
    "#         if(x['flag'] == 0):\n",
    "#             return np.nan\n",
    "        try:\n",
    "            if(len(feature) == 1):\n",
    "                feature = x[feature[0]]\n",
    "            else:\n",
    "                feature = tuple((x[feature[0]],x[feature[1]]))\n",
    "\n",
    "            here = now.get_group(feature).loc[x.name:]\n",
    "            if(len(here) >= 2):\n",
    "                return here.iloc[1]['context_timestamp'] - x['context_timestamp']\n",
    "            return np.nan\n",
    "        except:\n",
    "            return np.nan\n",
    "    if(len(feature) == 1):\n",
    "        res = data.apply(func,args = (feature,),axis = 1).rename(feature[0] + '-next-browse-time-interval')\n",
    "    else:\n",
    "        res = data.apply(func,args = (feature,),axis = 1).rename(feature[0] + '-' + feature[1] + '-next-browse-time-interval')\n",
    "    del now\n",
    "    res.to_pickle('../2.data/myfeature/temp/' + res.name)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3小时\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "if(not os.path.exists('../2.data/myfeature/last_and_next_browse_time_interval')):\n",
    "    res = {}\n",
    "    features = ['item_id','item_brand_id',\n",
    "#                 'item_city_id',\n",
    "                'user_id','shop_id']\n",
    "    data_combination = pd.DataFrame()\n",
    "    \n",
    "    # 并发执行\n",
    "    pros = Pool(20)\n",
    "    # last\n",
    "    for feature in features:\n",
    "        temp = pd.concat([\n",
    "                data[feature],\n",
    "                data['context_timestamp'],\n",
    "#                 data['flag']\n",
    "            ],axis = 1)\n",
    "        \n",
    "        res[feature + '-last'] = pros.apply_async(get_last_browse_time_interval,args = (temp,list((feature,)),))\n",
    "        \n",
    "    for i in range(len(features)):\n",
    "        for j in range(i + 1,len(features)):\n",
    "            feature = features[i] + '-' + features[j]\n",
    "            temp = pd.concat([\n",
    "                data[features[i]],\n",
    "                data[features[j]],\n",
    "                data['context_timestamp'],\n",
    "#                 data['flag']\n",
    "            ],axis = 1)\n",
    "            res[feature + '-last'] = pros.apply_async(get_last_browse_time_interval,args = (temp,list((features[i],features[j])),))\n",
    "    \n",
    "    # next\n",
    "    for feature in features:\n",
    "        temp = pd.concat([\n",
    "                data[feature],\n",
    "                data['context_timestamp'],\n",
    "#                 data['flag']\n",
    "            ],axis = 1)\n",
    "        res[feature + '-next'] = pros.apply_async(get_next_browse_time_interval,args = (temp,list((feature,)),))\n",
    " \n",
    "    for i in range(len(features)):\n",
    "        for j in range(i + 1,len(features)):\n",
    "            feature = features[i] + '-' + features[j]\n",
    "            temp = pd.concat([\n",
    "                data[features[i]],\n",
    "                data[features[j]],\n",
    "                data['context_timestamp'],\n",
    "#                 data['flag']\n",
    "            ],axis = 1)\n",
    "            res[feature + '-next'] = pros.apply_async(get_next_browse_time_interval,args = (temp,list((features[i],features[j])),))\n",
    "\n",
    "    pros.close()\n",
    "    pros.join()\n",
    "    \n",
    "    # 组合\n",
    "    for now in res:\n",
    "        data_combination = pd.concat([\n",
    "            data_combination,\n",
    "            res[now].get()\n",
    "        ],axis = 1)\n",
    "        \n",
    "    # 保存\n",
    "    data_combination.to_pickle('../2.data/myfeature/last_and_next_browse_time_interval')\n",
    "else:\n",
    "    data_combination = pd.read_pickle('../2.data/myfeature/last_and_next_browse_time_interval')\n",
    "\n",
    "print(datetime.datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8.2上次浏览时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if(not os.path.exists('data/myfeature/last_browse_time')):\n",
    "    data_combination = pd.DataFrame()\n",
    "    temp = pd.read_pickle('data/myfeature/last_browse_time_interval')\n",
    "    \n",
    "    for feature in features:\n",
    "        data_combination = pd.concat([\n",
    "            data_combination,\n",
    "            (data['context_timestamp'] - temp[feature + '-last-browse-time-interval']).rename(feature + '-last-browse-time')\n",
    "        ],axis = 1)\n",
    "    \n",
    "    for i in range(len(features)):\n",
    "        for j in range(i + 1,len(features)):\n",
    "            feature = features[i] + '-' + features[j]\n",
    "            data_combination = pd.concat([\n",
    "                data_combination,\n",
    "                (data['context_timestamp'] - temp[feature + '-last-browse-time-interval']).rename(feature + '-last-browse-time')\n",
    "            ],axis = 1)\n",
    "    \n",
    "    data_combination.to_pickle('data/myfeature/last_browse_time')\n",
    "    \n",
    "else:\n",
    "    data_combination = pd.read_pickle('data/myfeature/last_browse_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(not os.path.exists('data/myfeature/next_browse_time')):\n",
    "    data_combination = pd.DataFrame()\n",
    "    temp = pd.read_pickle('data/myfeature/next_browse_time_interval')\n",
    "    \n",
    "    for feature in features:\n",
    "        data_combination = pd.concat([\n",
    "            data_combination,\n",
    "            (data['context_timestamp'] + temp[feature + '-next-browse-time-interval']).rename(feature + '-next-browse-time')\n",
    "        ],axis = 1)\n",
    "    \n",
    "    for i in range(len(features)):\n",
    "        for j in range(i + 1,len(features)):\n",
    "            feature = features[i] + '-' + features[j]\n",
    "            data_combination = pd.concat([\n",
    "                data_combination,\n",
    "                (data['context_timestamp'] + temp[feature + '-next-browse-time-interval']).rename(feature + '-next-browse-time')\n",
    "            ],axis = 1)\n",
    "    \n",
    "    data_combination.to_pickle('data/myfeature/next_browse_time')\n",
    "else:\n",
    "    data_combination = pd.read_pickle('data/myfeature/next_browse_time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.9当前点击前若干小时的浏览次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_browse_count_hour(length,data,name):\n",
    "    now = data.groupby(name)\n",
    "    \n",
    "    def func1(x,name):\n",
    "#         if(x['flag'] == 0):\n",
    "#             return np.nan\n",
    "        try:\n",
    "            if(len(name) == 1):\n",
    "                name = name[0]\n",
    "                return len(now.get_group(x[name]).loc[x.name - length:x.name - 1])\n",
    "            else:\n",
    "                return len(now.get_group((x[name[0]],x[name[1]])).loc[x.name - length:x.name - 1])\n",
    "        except:\n",
    "#             print(x)\n",
    "            return np.nan\n",
    "\n",
    "    res =  data.apply(func1,axis = 1,args = (name,))\n",
    "    \n",
    "    if(len(name) == 1):\n",
    "        res.rename(str(name[0]) + '-browse-count-hour-ago-' + str(length))\\\n",
    "        .to_pickle('../2.data/myfeature/temp/' + str(name[0]) + '_browse_count_hour_ago_' + str(length))\n",
    "    else:\n",
    "        res.rename(str(name[0]) + '-' + str(name[1]) + '-browse-count-hour-ago-' + str(length))\\\n",
    "        .to_pickle('../2.data/myfeature/temp/' + str(name[0]) + '-' + str(name[1]) + '_browse_count_hour_ago_' + str(length))\n",
    "    del now\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3.5小时\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "if(not os.path.exists('../2.data/myfeature/browse_count_hour_ago')):\n",
    "    res = {}\n",
    "    features = ['item_id','item_brand_id',\n",
    "#                 'item_city_id',\n",
    "                'user_id','shop_id']\n",
    "    data_combination = pd.DataFrame()\n",
    "    \n",
    "    # 并发执行\n",
    "    pros = Pool(10)\n",
    "    \n",
    "    length = 1 * 60 * 60 # 取前1个小时\n",
    "    for feature in features:\n",
    "        temp = pd.concat([\n",
    "                data[feature],\n",
    "#                 data['flag'],\n",
    "            ],axis = 1)\n",
    "        res[feature + str(length)] = pros.apply_async(get_browse_count_hour,(length,temp,list((feature,)),))\n",
    "            \n",
    "    for i in range(len(features)):\n",
    "        for j in range(i + 1,len(features)):\n",
    "            feature = features[i] + '-' + features[j]\n",
    "            temp = pd.concat([\n",
    "                data[features[i]],\n",
    "                data[features[j]],\n",
    "#                 data['flag'],\n",
    "            ],axis = 1)\n",
    "            res[feature + str(length)] = pros.apply_async(get_browse_count_hour,(length,temp,list((features[i],features[j])),))\n",
    "    \n",
    "#     length = 1 * 30 * 60\n",
    "#     for feature in features:\n",
    "#         temp = pd.concat([\n",
    "#                 data[feature],\n",
    "# #                 data['flag'],\n",
    "#             ],axis = 1)\n",
    "#         res[feature + str(length)] = pros.apply_async(get_browse_count_hour,(length,temp,list((feature,)),))\n",
    "            \n",
    "#     for i in range(len(features)):\n",
    "#         for j in range(i + 1,len(features)):\n",
    "#             feature = features[i] + '-' + features[j]\n",
    "#             temp = pd.concat([\n",
    "#                 data[features[i]],\n",
    "#                 data[features[j]],\n",
    "# #                 data['flag'],\n",
    "#             ],axis = 1)\n",
    "#             res[feature + str(length)] = pros.apply_async(get_browse_count_hour,(length,temp,list((features[i],features[j])),))\n",
    "\n",
    "    del data\n",
    "    gc.collect()\n",
    "    pros.close()\n",
    "    pros.join()\n",
    "    \n",
    "    # 组合\n",
    "    for now in res:\n",
    "        data_combination = pd.concat([\n",
    "            data_combination,\n",
    "            res[now].get()\n",
    "        ],axis = 1)\n",
    "\n",
    "    del res\n",
    "\n",
    "    # 保存\n",
    "#     data_combination.to_pickle('../2.data/myfeature/browse_count_hour_ago')\n",
    "else:\n",
    "    data_combination = pd.read_pickle('../2.data/myfeature/browse_count_hour_ago')\n",
    "print(datetime.datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.10排序特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_sort(feature_now,temp):\n",
    "#     if(len(feature_now) == 2):\n",
    "#         return data.apply(lambda x:temp[x.loc[feature_now[0]], x.loc[feature_now[1]], x.name]['order'],axis = 1)\n",
    "#     else:\n",
    "#         return data.apply(lambda x:temp[x.loc[feature_now[0]], x.loc[feature_now[1]], x.loc[feature_now[2]], x.name]['order'],axis = 1)\n",
    "\n",
    "# def get_sort_test(feature_now,temp):\n",
    "#     if(len(feature_now) == 2):\n",
    "#         return data_test.apply(lambda x:temp[x.loc[feature_now[0]], x.loc[feature_now[1]], x.name  + length_data]['order'],axis = 1)\n",
    "#     else:\n",
    "#         return data_test.apply(lambda x:temp[x.loc[feature_now[0]], x.loc[feature_now[1]], x.loc[feature_now[2]], x.name + length_data]['order'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start = datetime.datetime.now()\n",
    "# if(not os.path.exists('data/myfeature/order') or not os.path.exists('data/myfeature/order_test')):\n",
    "#     res = {}\n",
    "#     res_test = {}\n",
    "#     features = ['f1','f4','f5','f10','f19']\n",
    "#     length_data = len(data)\n",
    "#     data_test['f5'].fillna(-1,inplace = True)\n",
    "    \n",
    "#     # 执行\n",
    "#     pros = Pool()\n",
    "#     for feature in features:\n",
    "#         feature_now = (feature,'f16:day') # 按天排序\n",
    "#         print(feature_now)\n",
    "#         temp = data.append(data_test).reset_index(drop = True).groupby(feature_now) \\\n",
    "#             .apply(lambda x:pd.concat([pd.Series(range(len(x)),index = x.index).rename('order'),x],axis = 1)).to_dict('index')\n",
    "        \n",
    "#         res[feature] = pros.apply_async(get_sort,args = (feature_now,temp))\n",
    "#         if(flag is False):\n",
    "#             res_test[feature] = pros.apply_async(get_sort_test,args = (feature_now,temp))\n",
    "            \n",
    "#     for i in range(len(features)):\n",
    "#         for j in range(i + 1,len(features)):\n",
    "#             feature_now = (features[i],features[j],'f16:day') # 按天排序\n",
    "#             print(feature_now)\n",
    "#             temp = data.append(data_test).reset_index(drop = True).groupby(feature_now) \\\n",
    "#                 .apply(lambda x:pd.concat([pd.Series(range(len(x)),index = x.index).rename('order'),x],axis = 1)).to_dict('index')\n",
    "                \n",
    "#             res[feature] = pros.apply_async(get_sort,args = (feature_now,temp))\n",
    "#             if(flag is False):\n",
    "#                 res_test[feature] = pros.apply_async(get_sort_test,args = (feature_now,temp))\n",
    "            \n",
    "#     pros.close()\n",
    "#     pros.join()\n",
    "    \n",
    "#     for feature in res:\n",
    "#         data_combination = pd.concat([data_combination,res[feature].get().rename(feature + '-sort-by-day')],axis = 1)\n",
    "#         if(flag is False):\n",
    "#             data_combination_test = pd.concat([data_combination_test,res_test[feature].get().rename(feature + '-sort-by-day')], axis = 1)\n",
    "            \n",
    "#     # 保存\n",
    "#     data_combination.to_pickle('data/myfeature/order')\n",
    "#     if(flag is False):\n",
    "#         data_combination_test.to_pickle('data/myfeature/order_test')\n",
    "# else:\n",
    "#     data_combination = pd.read_pickle('data/myfeature/order')\n",
    "#     data_combination_test = pd.read_pickle('data/myfeature/order_test')\n",
    "# print(datetime.datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.12贝叶斯平滑后的转化率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import numpy\n",
    "# import random\n",
    "# import scipy.special as special\n",
    "\n",
    "\n",
    "# class BayesianSmoothing(object):\n",
    "#     def __init__(self, alpha, beta):\n",
    "#         self.alpha = alpha\n",
    "#         self.beta = beta\n",
    "\n",
    "#     def sample(self, alpha, beta, num, imp_upperbound):\n",
    "#         sample = numpy.random.beta(alpha, beta, num)\n",
    "#         I = []\n",
    "#         C = []\n",
    "#         for clk_rt in sample:\n",
    "#             imp = random.random() * imp_upperbound\n",
    "#             imp = imp_upperbound\n",
    "#             clk = imp * clk_rt\n",
    "#             I.append(imp)\n",
    "#             C.append(clk)\n",
    "#         return I, C\n",
    "\n",
    "#     def update(self, imps, clks, iter_num, epsilon):\n",
    "#         for i in range(iter_num):\n",
    "#             new_alpha, new_beta = self.__fixed_point_iteration(imps, clks, self.alpha, self.beta)\n",
    "#             if abs(new_alpha-self.alpha) < epsilon and abs(new_beta - self.beta) < epsilon:\n",
    "#                 break\n",
    "#             self.alpha = new_alpha\n",
    "#             self.beta = new_beta\n",
    "\n",
    "#     def __fixed_point_iteration(self, imps, clks, alpha, beta):\n",
    "#         numerator_alpha = 0.0\n",
    "#         numerator_beta = 0.0\n",
    "#         denominator = 0.0\n",
    "\n",
    "#         for i in range(len(imps)):\n",
    "#             numerator_alpha += (special.digamma(clks[i]+alpha) - special.digamma(alpha))\n",
    "#             numerator_beta += (special.digamma(imps[i]-clks[i]+beta) - special.digamma(beta))\n",
    "#             denominator += (special.digamma(imps[i]+alpha+beta) - special.digamma(alpha+beta))\n",
    "\n",
    "#         return alpha*(numerator_alpha/denominator), beta*(numerator_beta/denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-\n",
      "f4-\n",
      "f5-\n",
      "f10-\n",
      "f19-\n",
      "f1-f4-\n",
      "f1-f5-\n",
      "f4-f5-\n"
     ]
    }
   ],
   "source": [
    "# def smooth(feature,data,data_type):\n",
    "#     print(feature)\n",
    "#     bs = BayesianSmoothing(1, 1)\n",
    "#     bs.update(data[feature + 'browse-count-5'].values, data[feature + 'count-5'].values, 1000, 0.001)\n",
    "#     print(feature + 'update成功')\n",
    "#     temp = (data[feature + 'count-5'] + bs.alpha) / (data[feature + 'browse-count-5'] + bs.alpha + bs.beta)\n",
    "#     temp.to_pickle('data/myfeature/' + feature + data_type)\n",
    "#     return True\n",
    "\n",
    "# features_to_smooth = [\n",
    "#     'f1-','f4-','f5-','f10-','f19-',\n",
    "#     'f1-f4-','f1-f5-','f4-f5-','f4-f19-','f5-f19-'\n",
    "# ]\n",
    "\n",
    "# for feature in features_to_smooth:\n",
    "#     pros.apply_async(smooth, (feature,data,''))  #增加新的进程\n",
    "    \n",
    "# for feature in features_to_smooth:\n",
    "#     pros.apply_async(smooth, (feature,data_test,'test'))  #增加新的进程\n",
    "    \n",
    "# pros.close() # 禁止在增加新的进程\n",
    "# pros.join()\n",
    "# print(\"pool process done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.特征合并\n",
    "## 5.1读取先前处理的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user_id-day-browse-is-first', 'user_id-day-browse-is-last', 'user_id-hour-browse-is-first', 'user_id-hour-browse-is-last']\n"
     ]
    }
   ],
   "source": [
    "# 读取3.特征变化、衍生产生的data\n",
    "data = pd.read_pickle('data/data')\n",
    "\n",
    "# 读取4.特征组合产生的data_combination\n",
    "data_combination = pd.DataFrame()\n",
    "for file in os.listdir('data/myfeature'):\n",
    "    if('.' not in file):\n",
    "        temp = pd.read_pickle('data/myfeature/' + file)\n",
    "        data_combination = pd.concat([data_combination,temp],axis = 1)\n",
    "        \n",
    "# 删除特征取值少的冗余特征\n",
    "column_to_del = []\n",
    "for column in data_combination.columns:\n",
    "    if(len(data_combination[column].value_counts()) <= 2):\n",
    "        column_to_del.append(column)\n",
    "\n",
    "    # column_to_del += ['f1-f19-browse-count-5','f5-f10-browse-count-5']\n",
    "print(column_to_del)\n",
    "data_combination.drop(column_to_del[:-4],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 拼接data_f3、data_test_f3、data_combination、data_combination_test\n",
    "data = pd.concat([data.loc[:,:'f3-f18-jaccard'],data_combination,data.loc[:,'label']],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.sort_values(['index'],inplace = True)\n",
    "data.set_index('index', inplace=True)# f16转换为索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.特征选择\n",
    "- 除非万不得已，不要用PCA或者LDA降维，直接减原始特征就行了。\n",
    "\n",
    "## 6.1质量不好的特征\n",
    "- 缺失的行特别多，弃用该列，超过15%缺失的特征应该予以删除！\n",
    "- 质量都不错，最多的f12（0.027）\n",
    "\n",
    "## 6.2冗余特征（相关性强的保留一个）\n",
    "- 有些 Feature 之间可能存在线性关系，影响 Model 的性能。\n",
    "- Feature越少，训练越快。\n",
    "\n",
    "## 6.3无关特征\n",
    "- f0样本编号：近似唯一\n",
    "- f1广告商品编号\n",
    "- f10用户编号\n",
    "- f15上下文信息编号：完全唯一\n",
    "- f19店铺编号\n",
    "\n",
    "## 6.4无法直接用的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data.drop('f16',axis = 1)\n",
    "data.drop([\n",
    "    'item_category_list',\n",
    "    'item_category_list:1',\n",
    "    'item_category_list:2',\n",
    "    'item_category_list:3',\n",
    "    'item_property_list',\n",
    "    'user_gender_id',\n",
    "    'user_occupation_id',\n",
    "    'predict_category_property'\n",
    "], axis=1,inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.标签处理\n",
    "- 上采样、下采样、分层采样。\n",
    "\n",
    "# 8.保存结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle('data/round1_train2_5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
