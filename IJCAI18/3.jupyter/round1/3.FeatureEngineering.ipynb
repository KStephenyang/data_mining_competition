{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "# OS\n",
    "import os\n",
    "import datetime\n",
    "import pytz\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "\n",
    "#数据处理\n",
    "import pandas as pd\n",
    "# import ray.dataframe as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from scipy.special import boxcox1p\n",
    "\n",
    "#可视化\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "print(multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.数据载入、对齐、排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle('data/round1_train1')\n",
    "data_test = pd.read_pickle('data/round1_test1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给test数据拼接一个f26特征\n",
    "data_test = pd.concat([\n",
    "    data_test,\n",
    "    pd.Series([0] * len(data_test),name = 'is_trade')\n",
    "], axis = 1)\n",
    "\n",
    "# 拼接    \n",
    "data = data.append(data_test)\n",
    "\n",
    "# 按时间排序，重置索引\n",
    "data.reset_index(drop = True,inplace = True)\n",
    "data.reset_index(inplace = True)\n",
    "data.set_index('context_timestamp', drop = False,inplace=True)# f16转换为索引\n",
    "data.sort_values(['context_timestamp'],inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.特征转换、衍生\n",
    "## 3.1离散数据\n",
    "### 3.1.1将出现次数少的值合并到统一类别中\n",
    "- f6：将10（456）、2（347）、1（85）、11（21）、0（12）、17（1）、16（1）这几个取值单独拉出一个类。\n",
    "- f9：将8（449）、7（245）、0（123）、6（116）、5（63）、4（33）3（11）、2（5）、1（1）这几个取值单独拉出一个类。\n",
    "- f20：将23（353）、4（266）、2（87）、3（80）、1（20）、0（7）、25（4）这几个取值单独拉出一个类。\n",
    "- f22：将5002（477）、5020（357）、5000（81）、5019（70）、5001（60）、4999（7）这几个取值单独拉出一个类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2onehot/dummy-trap\n",
    "- f6广告商品的价格等级、f9广告商品被展示次数的等级、f11用户的预测性别编号、f12用户的预测年龄等级、f13用户的预测职业编号、f14用户的星级编号\n",
    "- f7、f8、f17、f20、f22不确定\n",
    "\n",
    "#### 3.1.2.1f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def split_f2(data):\n",
    "    # 切分\n",
    "    data = pd.concat([\n",
    "        data.loc[:,:'item_category_list'],\n",
    "        data['item_category_list'].astype(np.str).str.split(';', expand=True),\n",
    "        data.loc[:,'item_property_list':]\n",
    "    ], axis = 1)\n",
    "    data.rename({\n",
    "        0:'item_category_list:1',\n",
    "        1:'item_category_list:2',\n",
    "        2:'item_category_list:3'\n",
    "    }, axis='columns',inplace = True)\n",
    "\n",
    "    # 类型转换\n",
    "    data['item_category_list:3'].fillna('-1',inplace = True)\n",
    "    data['item_category_list:1'] = data['item_category_list:1'].astype('int')\n",
    "    data['item_category_list:2'] =data['item_category_list:2'].astype('int')\n",
    "    data['item_category_list:3'] = data['item_category_list:3'].astype('int')\n",
    "\n",
    "    # 对f2:2、f2:3进行onehot编码\n",
    "    temp = pd.get_dummies(data['item_category_list:2'],prefix = 'item_category_list:2')  \n",
    "    data = pd.concat([\n",
    "        data.loc[:,:'item_category_list:2'],\n",
    "        temp,\n",
    "        data.loc[:,'item_category_list:3':]\n",
    "    ], axis = 1)\n",
    "    \n",
    "    temp = pd.get_dummies(data['item_category_list:3'],prefix = 'item_category_list:3')  \n",
    "    return pd.concat([\n",
    "        data.loc[:,:'item_category_list:3'],\n",
    "        temp,\n",
    "        data.loc[:,'item_property_list':]\n",
    "    ], axis = 1)\n",
    "\n",
    "data = split_f2(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2.2f11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def onehot_f11(data):\n",
    "    temp = pd.get_dummies(data['user_gender_id'])  \n",
    "    temp = temp.rename({\n",
    "        0.0:'user_gender_id:woman',\n",
    "        1.0:'user_gender_id:man',\n",
    "        2.0:'user_gender_id:family',\n",
    "        -1.0:'user_gender_id:other'\n",
    "    },axis = 'columns')\n",
    "    return pd.concat([\n",
    "        data.loc[:,:'user_gender_id'],\n",
    "        temp,\n",
    "        data.loc[:,'user_age_level':]\n",
    "    ], axis = 1)\n",
    "\n",
    "data = onehot_f11(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2.3f13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def onehot_f13(data):\n",
    "    temp = pd.get_dummies(data['user_occupation_id']).rename({\n",
    "        2002.0:'user_occupation_id:2002.0',\n",
    "        2003.0:'user_occupation_id:2003.0',\n",
    "        2004.0:'user_occupation_id:2004.0',\n",
    "        2005.0:'user_occupation_id:2005.0'\n",
    "    },axis = 'columns')\n",
    "    return pd.concat([\n",
    "        data.loc[:,:'user_occupation_id'],\n",
    "        temp,\n",
    "        data.loc[:,'user_star_level':]\n",
    "    ], axis = 1)\n",
    "\n",
    "data = onehot_f13(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 将f11、f13进行onehot编码后，xgboost的验证集loss由0.084306减小到0.084273\n",
    "- 将f2;2用onehot表示后，xgboost的验证集loss由0.0775减小到0.07373\n",
    "- 将f5用onehot表示后，xgboost的验证集loss由0.086977提高到0.087148，没用\n",
    "\n",
    "### 3.1.3自然数编码\n",
    "- 消耗内存小，训练时间快，但是相比one-hot特征的质量不高，含了一个假设：不同的类别之间，存在一种顺序关系。\n",
    "\n",
    "### 3.1.4聚类编码\n",
    "- 和独热编码相比，聚类编码试图充分利用每一列0与1的信息表达能力。聚类编码时一般需要特定的专业知识（domain knowledge），例如ZIP码可以根据精确度分层为ZIP3、ZIP4、ZIP5、ZIP6，然后按层次进行编码。\n",
    "\n",
    "## 3.2连续数据\n",
    "### 3.2.1标准化、归一化：分布太宽，做一下scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scaling\n",
    "# def scaling(data):\n",
    "#     for i in range(0,8):\n",
    "#         data['f12'] = data['f12'].replace(1000. + i,0 + i)\n",
    "#     for i in range(0,11):\n",
    "#         data['f14'] = data['f14'].replace(3000. + i,0 + i)\n",
    "#     for i in range(0,20):\n",
    "#         data['f17'] = data['f17'].replace(4001. + i,0 + i)\n",
    "#     for i in range(0,22):\n",
    "#         data['f22'] = data['f22'].replace(4999. + i,0 + i)\n",
    "#         return data\n",
    "\n",
    "# data = scaling(data)\n",
    "# if(flag is False):\n",
    "#     data_test = scaling(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 将f112、f14、f17、f22进行范围缩放后，xgboost的验证集loss由0.0775没变，没用。\n",
    "\n",
    "### 3.2.2正态化：对偏度大于0.75的数值特征（长尾分布）\n",
    "- 用log1p函数进行转化使其更加服从高斯分布\n",
    "np.log1p(train.SalePrice)\n",
    "- Box-Cox变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# f7:2 f8:2.5 f9:5 f20:1.5\n",
    "# def boxcox(data):\n",
    "#     data['f7'] = boxcox1p(data['f7'],2)\n",
    "#     data['f8'] = boxcox1p(data['f8'],2.5)\n",
    "#     data['f9'] = boxcox1p(data['f9'],5)\n",
    "#     data['f20'] = boxcox1p(data['f20'],1.5)\n",
    "#     return data\n",
    "\n",
    "# data = boxcox(data)\n",
    "# if(flag is False):\n",
    "#     data_test = boxcox(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3离散化：Binning\n",
    "- 只有在了解属性的领域知识的基础，确定属性能够划分成简洁的范围时分箱才有意义，即所有的数值落入一个分区时能够呈现出共同的特征。\n",
    "- 当不想让模型总是尝试区分值之间是否太近时，分区可以避免出现过拟合。\n",
    "\n",
    "\n",
    "\n",
    "- 正态化后，xgboost的验证集loss不变，没用\n",
    "\n",
    "### 3.2.4时间数据的转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_f16(data):\n",
    "    now = pd.to_datetime(data['context_timestamp'],unit='s',utc = True)\n",
    "    now = now.apply(lambda x:x.astimezone(pytz.timezone('Asia/Shanghai')))\n",
    "    \n",
    "    # year = pd.Series([-1] * 477303)\n",
    "    # month = pd.Series([-1] * 477303)\n",
    "    day = now.apply(lambda x:x.day).rename('context_timestamp:day')\n",
    "    hour = now.apply(lambda x:x.hour).rename('context_timestamp:hour')\n",
    "    minute = now.apply(lambda x:x.minute).rename('context_timestamp:minute')\n",
    "    second = now.apply(lambda x:x.second).rename('context_timestamp:second')\n",
    "    dayofweek = now.apply(lambda x:x.dayofweek).rename('context_timestamp:dayofweek')\n",
    "\n",
    "    data = pd.concat([\n",
    "        data.loc[:,:'context_timestamp'],\n",
    "        day,\n",
    "        hour,\n",
    "        minute,\n",
    "        second,\n",
    "        dayofweek,\n",
    "        data.loc[:,'context_page_id':]\n",
    "    ], axis = 1)\n",
    "    data.rename({'is_trade':'label'}, axis='columns',inplace = True)\n",
    "    return data\n",
    "\n",
    "data = transform_f16(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 转换时间f16为多个特征并删除特征f16后，xgboost的验证集loss较明显的降低\n",
    "- 加入dayofweek特征后，xgboost的验证集loss没变，没用\n",
    "\n",
    "## 3.3高势集数据（High Categorical）（f1、f3、f4、f10、f18、f19）\n",
    "### 3.3.1高势集类别进行经验贝叶斯转换成数值feature\n",
    "### 3.3.2平均数编码\n",
    "- 平均数编码（mean encoding），针对高基数类别特征的有监督编码。当一个类别特征列包括了极多不同类别时（如家庭地址，动辄上万）时，可以采用。优点：和独热编码相比，节省内存、减少算法计算时间、有效增强模型表现。\n",
    "\n",
    "\n",
    "- 将f2属性拆分成三个子属性后，xgboost的验证集loss不变，没用\n",
    "\n",
    "### 3.3.3word embedding\n",
    "#### 3.3.3.1抽取f3数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    res = []\n",
    "    for now in x.split(';'):\n",
    "        if(len(now.split(':')) != 1):\n",
    "            res += now.split(':')[1].split(',')\n",
    "    if('-1' in res):\n",
    "        # print(1)\n",
    "        here = set(res)\n",
    "        here.remove('-1')\n",
    "        return list(here)\n",
    "    else:\n",
    "        return list(set(res))\n",
    "predict_category_property = data['predict_category_property'].apply(func).tolist()\n",
    "\n",
    "item_property_list = data['item_property_list'].apply(lambda x:x.split(';')).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict_category_property_file = open('data/predict_category_property','w')\n",
    "assert len(predict_category_property) == len(item_property_list)\n",
    "for i in range(len(predict_category_property)):\n",
    "    if(len(predict_category_property[i]) == 0):\n",
    "        predict_category_property_file.write('\\n')\n",
    "    for j in range(len(predict_category_property[i])):\n",
    "        if(j == 0 and j == len(predict_category_property[i]) - 1):\n",
    "            predict_category_property_file.write(predict_category_property[i][j] + '\\n')\n",
    "        elif(j == 0):\n",
    "            predict_category_property_file.write(predict_category_property[i][j])\n",
    "        elif(j == len(predict_category_property[i]) - 1):\n",
    "            predict_category_property_file.write(' ' + predict_category_property[i][j] + '\\n')\n",
    "        else:\n",
    "            predict_category_property_file.write(' ' + predict_category_property[i][j])\n",
    "\n",
    "    if(len(item_property_list[i]) == 0):\n",
    "        predict_category_property_file.write('\\n')\n",
    "    for j in range(len(item_property_list[i])):\n",
    "        if(j == 0 and j == len(item_property_list[i]) - 1):\n",
    "            predict_category_property_file.write(item_property_list[i][j] + '\\n')\n",
    "        elif(j == 0):\n",
    "            predict_category_property_file.write(item_property_list[i][j])\n",
    "        elif(j == len(item_property_list[i]) - 1):\n",
    "            predict_category_property_file.write(' ' + item_property_list[i][j] + '\\n')\n",
    "        else:\n",
    "            predict_category_property_file.write(' ' + item_property_list[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.5.3doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "from gensim.models.doc2vec import Doc2Vec,TaggedLineDocument\n",
    "if not os.path.exists('data/item_property_list_doc_model'):\n",
    "    sentences = TaggedLineDocument('data/predict_category_property')\n",
    "    model = Doc2Vec(sentences,size=10, window=7, min_count=1, negative=3, hs=0)\n",
    "    model.save('data/item_property_list_doc_model')  \n",
    "else:\n",
    "    model = Doc2Vec.load('data/item_property_list_doc_model')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import linalg\n",
    "\n",
    "def cos(vector1,vector2):  \n",
    "    dot_product = 0.0;  \n",
    "    normA = 0.0;  \n",
    "    normB = 0.0;  \n",
    "    for a,b in zip(vector1,vector2):  \n",
    "        dot_product += a*b  \n",
    "        normA += a**2  \n",
    "        normB += b**2  \n",
    "    if normA == 0.0 or normB==0.0:  \n",
    "        return None  \n",
    "    else:  \n",
    "        return dot_product / ((normA*normB)**0.5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp = data.reset_index(drop = True).apply(\n",
    "    lambda x:\n",
    "    cos(\n",
    "        model.infer_vector(item_property_list[x.name]),\n",
    "        model.infer_vector(predict_category_property[x.name])\n",
    "       ),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp.index = data.index\n",
    "data = pd.concat([\n",
    "    data.loc[:,:'shop_score_description'],\n",
    "    temp.rename('embedding-similarity'),\n",
    "    data.loc[:,'label']\n",
    "],axis = 1)\n",
    "del temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4计算Jaccard相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    res = []\n",
    "    for cat in x['predict_category_property'].split(';'):\n",
    "        for pro in cat.split(':')[1].split(','):\n",
    "            if(pro != '-1'):\n",
    "                res.append(pro)\n",
    "    return set(res)\n",
    "\n",
    "def get_jaccard(real,predict):\n",
    "    temp = pd.Series([-1.0] * len(real))\n",
    "    for i in range(len(temp)):\n",
    "        temp[i] = len(real.iloc[i] & predict.iloc[i]) * 1.0 / len(real.iloc[i] | predict.iloc[i])\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict = data.apply(func,axis = 1)\n",
    "real = data.apply(lambda x:set(x['item_property_list'].split(';')),axis = 1)\n",
    "jaccard = get_jaccard(real,predict)\n",
    "\n",
    "jaccard.index = data.index\n",
    "\n",
    "data = pd.concat([\n",
    "    data.loc[:,:'embedding-similarity'],\n",
    "    jaccard.rename('f3-f18-jaccard',inplace = True),\n",
    "    data.loc[:,'label']\n",
    "],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4保存特征转换、衍生的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_pickle('data/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 用了embedding处理f3后，验证集的logloss由0.081953下降到了0.081787"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.特征组合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "days = 5 # 滑动窗口大小\n",
    "day_num = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1当前点击前若干天购买次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_count_temp(i,name):\n",
    "    temp = data[\n",
    "        (data['context_timestamp:day'] >= 18 + i - days) & \n",
    "        (data['context_timestamp:day'] <= 17 + i)]\\\n",
    "    .groupby(name).apply(lambda x:x['label'].sum())  # 前n天出现商品的转化率=\n",
    "    \n",
    "    if(len(name) == 1):\n",
    "        name = name[0]\n",
    "        res = data[data['context_timestamp:day'] == i + 18][name].apply(lambda x:temp[x] if x in temp else -1)\n",
    "        del temp\n",
    "        return res\n",
    "    else:\n",
    "        res = data[data['context_timestamp:day'] == i + 18].apply(lambda x:temp[(x[name[0]],x[name[1]])] if (x[name[0]],x[name[1]]) in temp else -1,axis = 1)\n",
    "        del temp\n",
    "        return res\n",
    "    \n",
    "def get_count(name):\n",
    "    temp = pd.Series([])\n",
    "    for i in range(day_num):\n",
    "        now = get_count_temp(i,name)\n",
    "        temp = temp.append(now)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:05:06.909757\n"
     ]
    }
   ],
   "source": [
    "# 5 min\n",
    "data_combination = pd.DataFrame()\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "if(not os.path.exists('data/myfeature/buy_count_' + str(days))):\n",
    "    features = ['item_id','item_brand_id','item_city_id','user_id','shop_id']\n",
    "    res = {}\n",
    "    \n",
    "    # 并发执行\n",
    "    pros = Pool()\n",
    "    for feature in features:\n",
    "        res[feature] = pros.apply_async(get_count,(list((feature,)),))\n",
    "            \n",
    "    for i in range(len(features)):\n",
    "        for j in range(i + 1,len(features)):\n",
    "            feature = features[i] + '-' + features[j]\n",
    "            res[feature] = pros.apply_async(get_count,(list((features[i],features[j])),))\n",
    "    pros.close()\n",
    "    pros.join()\n",
    "    \n",
    "    # 组合\n",
    "    for now in res:\n",
    "        data_combination = pd.concat([\n",
    "            data_combination,\n",
    "            res[now].get().rename(now + '-buy-count-' + str(days))\n",
    "        ],axis = 1)\n",
    "\n",
    "        # 保存\n",
    "    data_combination.to_pickle('data/myfeature/buy_count_' + str(days))\n",
    "\n",
    "else:\n",
    "    data_combination = pd.read_pickle(\n",
    "        'data/myfeature/buy_count_' + str(days))\n",
    "print(datetime.datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2当前点击前若干天浏览广告次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_browse_count_temp(i,name):\n",
    "    temp = data[\n",
    "        (data['context_timestamp:day'] >= 18 + i - days) & \n",
    "        (data['context_timestamp:day'] <= 17 + i)\n",
    "    ].groupby(name).apply(lambda x:len(x))  # 前n天出现商品的转化率=\n",
    "    \n",
    "    if(len(name) == 1):\n",
    "        name = name[0]\n",
    "        res = data[data['context_timestamp:day'] == i + 18][name]\\\n",
    "        .apply(lambda x:temp[x] if x in temp else -1)\n",
    "        del temp\n",
    "        return res\n",
    "    else:\n",
    "        res = data[data['context_timestamp:day'] == i + 18]\\\n",
    "    .apply(lambda x:temp[(x[name[0]],x[name[1]])] if (x[name[0]],x[name[1]]) in temp else -1,axis = 1)\n",
    "        del temp\n",
    "        return res\n",
    "    \n",
    "def get_browse_count(name):\n",
    "    temp = pd.Series([])\n",
    "    for i in range(day_num):\n",
    "        now = get_browse_count_temp(i,name)\n",
    "        temp = temp.append(now)\n",
    "        del now\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:02:17.534638\n"
     ]
    }
   ],
   "source": [
    "# 2.5min\n",
    "start = datetime.datetime.now()\n",
    "data_combination = pd.DataFrame()\n",
    "\n",
    "if(not os.path.exists('data/myfeature/browse_count_' + str(days))):\n",
    "    # 并发执行\n",
    "    pros = Pool()\n",
    "    res = {}\n",
    "    \n",
    "    for feature in features:\n",
    "        res[feature] = pros.apply_async(get_browse_count,(list((feature,)),))\n",
    "            \n",
    "    for i in range(len(features)):\n",
    "        for j in range(i + 1,len(features)):\n",
    "            feature = features[i] + '-' + features[j]\n",
    "            res[feature] = pros.apply_async(get_browse_count,(list((features[i],features[j])),))\n",
    "    pros.close()\n",
    "    pros.join()\n",
    "    \n",
    "    # 组合\n",
    "    for now in res:\n",
    "        data_combination = pd.concat([\n",
    "            data_combination,\n",
    "            res[now].get().rename(now + '-browse-count-' + str(days))\n",
    "        ],axis = 1)\n",
    "\n",
    "    # 保存\n",
    "    data_combination.to_pickle('data/myfeature/browse_count_' + str(days))\n",
    "else:\n",
    "    data_combination = pd.read_pickle('data/myfeature/browse_count_' + str(days))\n",
    "print(datetime.datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3当前点击前若干天转化率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_combination = pd.DataFrame()\n",
    "if(not os.path.exists('data/myfeature/ratio_' + str(days))):\n",
    "    temp = pd.concat([\n",
    "        pd.read_pickle('data/myfeature/buy_count_' + str(days)),\n",
    "        pd.read_pickle('data/myfeature/browse_count_' + str(days)),\n",
    "    ],axis = 1)\n",
    "    temp[temp == -1] = np.nan\n",
    "    data_combination = pd.DataFrame()\n",
    "    \n",
    "    for feature in features:\n",
    "        data_combination = pd.concat([\n",
    "            data_combination,\n",
    "            (temp[feature + '-buy-count-' + str(days)]/temp[feature + '-browse-count-' + str(days)]).rename(feature + '-ratio-' + str(days))\n",
    "        ],axis = 1)\n",
    "            \n",
    "    for i in range(len(features)):\n",
    "        for j in range(i + 1,len(features)):\n",
    "            feature = features[i] + '-' + features[j]\n",
    "            data_combination = pd.concat([\n",
    "                data_combination,\n",
    "                (temp[feature + '-buy-count-' + str(days)]/temp[feature + '-browse-count-' + str(days)]).rename(feature + '-ratio-' + str(days))\n",
    "            ],axis = 1)\n",
    "    \n",
    "    data_combination.to_pickle('data/myfeature/ratio_' + str(days))\n",
    "\n",
    "else:\n",
    "    data_combination = pd.read_pickle('data/myfeature/ratio_' + str(days))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 加入浏览次数特征后，xgb的logloss由0.08201下降到0.081756\n",
    "- 将f19进行转换为点击概率后，xgboost的验证集loss由0.084273减小到0.080542\n",
    "- 将f1进行转换为点击概率后，xgboost的验证集loss由0.080542减小到0.0775\n",
    "- 将f5进行转换为点击概率后，lgb的验证集loss由0.08287减小到0.08221 \n",
    "- 将f10进行转换为点击概率后，lgb的验证集loss由0.08221减小到0.0821587\n",
    "\n",
    "- 加上f1-f10-label、f1-f4-label、f1-f5-label、f1-f19-label后，lbg的logloss从0.082215下降到0.0820568\n",
    "- 加上f4-f5-label、f4-f10-label、f4-f19-label、f5-f10-label、f5-f19-label、f10-f19后，lbg的logloss从0.0820568下降到0.0819415\n",
    "\n",
    "## 4.4当前点击前若干小时的浏览次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "length = 1 * 60 * 60 # 取前1个小时"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_browse_count_hour(name):\n",
    "    now = data.groupby(name)\n",
    "    \n",
    "    def func1(x,name):\n",
    "        if(len(name) == 1):\n",
    "            name = name[0]\n",
    "            return len(now.get_group(x[name]).loc[x.name - length:x.name - 1])\n",
    "        else:\n",
    "            return len(now.get_group((x[name[0]],x[name[1]])).loc[x.name - length:x.name - 1])\n",
    "    \n",
    "    res =  data.apply(func1,axis = 1,args = (name,))\n",
    "    del now\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3:32:48.059314\n"
     ]
    }
   ],
   "source": [
    "# 3.5小时\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "if(not os.path.exists('data/myfeature/browse_count_hour_ago_' + str(length))):\n",
    "    res = {}\n",
    "    features = ['item_id','item_brand_id','item_city_id','user_id','shop_id']\n",
    "    data_combination = pd.DataFrame()\n",
    "    \n",
    "    # 并发执行\n",
    "    pros = Pool()\n",
    "    for feature in features:\n",
    "        res[feature] = pros.apply_async(get_browse_count_hour,(list((feature,)),))\n",
    "            \n",
    "    for i in range(len(features)):\n",
    "        for j in range(i + 1,len(features)):\n",
    "            feature = features[i] + '-' + features[j]\n",
    "            res[feature] = pros.apply_async(get_browse_count_hour,(list((features[i],features[j])),))\n",
    "\n",
    "    pros.close()\n",
    "    pros.join()\n",
    "    \n",
    "    # 组合\n",
    "    for now in res:\n",
    "        data_combination = pd.concat([\n",
    "            data_combination,\n",
    "            res[now].get().rename(now + '-browse-count-hour-ago-' + str(length))\n",
    "        ],axis = 1)\n",
    "\n",
    "    del res\n",
    "\n",
    "    # 保存\n",
    "    data_combination.to_pickle('data/myfeature/browse_count_hour_ago_' + str(length))\n",
    "else:\n",
    "    data_combination = pd.read_pickle('data/myfeature/browse_count_hour_ago_' + str(length))\n",
    "print(datetime.datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5展示、收藏、销量之间的比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if(not os.path.exists('data/myfeature/proportion')):\n",
    "    data_combination = pd.DataFrame()\n",
    "    \n",
    "    # 展示（f9）-收藏（f8）\n",
    "    data_combination = pd.concat([\n",
    "        data_combination,\n",
    "        (data['item_collected_level']/data['item_pv_level']).rename('item_collected_level-item_pv_level-proportion')\n",
    "    ],axis = 1)\n",
    "    \n",
    "    # 收藏（f8）-销量（f7）\n",
    "    data_combination = pd.concat([\n",
    "        data_combination,\n",
    "        (data['item_sales_level']/data['item_collected_level']).rename('item_sales_level-item_collected_level-proportion')\n",
    "    ],axis = 1)\n",
    "\n",
    "    # 展示（f9）-销量（f7）\n",
    "    data_combination = pd.concat([\n",
    "        data_combination,\n",
    "        (data['item_sales_level']/data['item_pv_level']).rename('item_sales_level-item_pv_level-proportion')\n",
    "    ],axis = 1)\n",
    "\n",
    "    data_combination.to_pickle('data/myfeature/proportion')\n",
    "\n",
    "else:\n",
    "    data_combination = pd.read_pickle('data/myfeature/proportion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6上次到这次浏览\n",
    "### 4.6.1到这次浏览的时间间隔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_last_browse_time_interval(feature):\n",
    "    now = data.groupby(feature)\n",
    "    \n",
    "    def func(x,feature):\n",
    "        if(len(feature) == 1):\n",
    "            feature = x[feature[0]]\n",
    "        else:\n",
    "            feature = tuple((x[feature[0]],x[feature[1]]))\n",
    "            \n",
    "        here = now.get_group(feature).loc[:x.name]\n",
    "        if(len(here) >= 2):\n",
    "            return x['context_timestamp'] - here.iloc[-2]['context_timestamp']\n",
    "        return np.nan\n",
    "\n",
    "    res = data.apply(func,args = (feature,),axis = 1)\n",
    "    del now\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3小时\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "if(not os.path.exists('data/myfeature/last_browse_time_interval')):\n",
    "    res = {}\n",
    "    features = ['item_id','item_brand_id','item_city_id','user_id','shop_id']\n",
    "    data_combination = pd.DataFrame()\n",
    "    \n",
    "    # 并发执行\n",
    "    pros = Pool()\n",
    "    for feature in features:\n",
    "        res[feature] = pros.apply_async(get_last_browse_time_interval,args = (list((feature,)),))\n",
    "        \n",
    "    for i in range(len(features)):\n",
    "        for j in range(i + 1,len(features)):\n",
    "            feature = features[i] + '-' + features[j]\n",
    "            res[feature] = pros.apply_async(get_last_browse_time_interval,args = (list((features[i],features[j])),))\n",
    "    pros.close()\n",
    "    pros.join()\n",
    "    \n",
    "    # 组合\n",
    "    for now in res:\n",
    "        data_combination = pd.concat([\n",
    "            data_combination,\n",
    "            res[now].get().rename(now + '-last-browse-time-interval')\n",
    "        ],axis = 1)\n",
    "        \n",
    "    # 保存\n",
    "    data_combination.to_pickle('data/myfeature/last_browse_time_interval')\n",
    "else:\n",
    "    data_combination = pd.read_pickle('data/myfeature/last_browse_time_interval')\n",
    "\n",
    "print(datetime.datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.2上次浏览时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if(not os.path.exists('data/myfeature/last_browse_time')):\n",
    "    data_combination = pd.DataFrame()\n",
    "    temp = pd.read_pickle('data/myfeature/last_browse_time_interval')\n",
    "    \n",
    "    for feature in features:\n",
    "        data_combination = pd.concat([\n",
    "            data_combination,\n",
    "            (data['context_timestamp'] - temp[feature + '-last-browse-time-interval']).rename(feature + '-last-browse-time')\n",
    "        ],axis = 1)\n",
    "    \n",
    "    for i in range(len(features)):\n",
    "        for j in range(i + 1,len(features)):\n",
    "            feature = features[i] + '-' + features[j]\n",
    "            data_combination = pd.concat([\n",
    "                data_combination,\n",
    "                (data['context_timestamp'] - temp[feature + '-last-browse-time-interval']).rename(feature + '-last-browse-time')\n",
    "            ],axis = 1)\n",
    "    \n",
    "    data_combination.to_pickle('data/myfeature/last_browse_time')\n",
    "    \n",
    "else:\n",
    "    data_combination = pd.read_pickle('data/myfeature/last_browse_time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7这次浏览到下次浏览\n",
    "### 4.7.1到下次浏览时间间隔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_browse_time_interval(feature):\n",
    "    now = data.groupby(feature)\n",
    "\n",
    "    def func(x,feature):\n",
    "        if(len(feature) == 1):\n",
    "            feature = x[feature[0]]\n",
    "        else:\n",
    "            feature = tuple((x[feature[0]],x[feature[1]]))\n",
    "            \n",
    "        here = now.get_group(feature).loc[x.name:]\n",
    "        if(len(here) >= 2):\n",
    "            return here.iloc[1]['context_timestamp'] - x['context_timestamp']\n",
    "        return np.nan\n",
    "\n",
    "    res = data.apply(func,args = (feature,),axis = 1)\n",
    "    del now\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3小时\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "if(not os.path.exists('data/myfeature/next_browse_time_interval')):\n",
    "    data_combination = pd.DataFrame()\n",
    "    res = {}\n",
    "    features = ['item_id','item_brand_id','item_city_id','user_id','shop_id']\n",
    "    \n",
    "    # 并发执行\n",
    "    pros = Pool()\n",
    "    for feature in features:\n",
    "        res[feature] = pros.apply_async(get_next_browse_time_interval,args = (list((feature,)),))\n",
    " \n",
    "    for i in range(len(features)):\n",
    "        for j in range(i + 1,len(features)):\n",
    "            feature = features[i] + '-' + features[j]\n",
    "            res[feature] = pros.apply_async(get_next_browse_time_interval,args = (list((features[i],features[j])),))\n",
    "    pros.close()\n",
    "    pros.join()\n",
    "    \n",
    "    # 组合\n",
    "    for now in res:\n",
    "        data_combination = pd.concat([\n",
    "            data_combination,\n",
    "            res[now].get().rename(now + '-next-browse-time-interval')\n",
    "        ],axis = 1)\n",
    "    \n",
    "    del res\n",
    "    \n",
    "    # 保存\n",
    "    data_combination.to_pickle('data/myfeature/next_browse_time_interval')\n",
    "else:\n",
    "    data_combination = pd.read_pickle('data/myfeature/next_browse_time_interval')\n",
    "\n",
    "print(datetime.datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.2下次浏览时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(not os.path.exists('data/myfeature/next_browse_time')):\n",
    "    data_combination = pd.DataFrame()\n",
    "    temp = pd.read_pickle('data/myfeature/next_browse_time_interval')\n",
    "    \n",
    "    for feature in features:\n",
    "        data_combination = pd.concat([\n",
    "            data_combination,\n",
    "            (data['context_timestamp'] + temp[feature + '-next-browse-time-interval']).rename(feature + '-next-browse-time')\n",
    "        ],axis = 1)\n",
    "    \n",
    "    for i in range(len(features)):\n",
    "        for j in range(i + 1,len(features)):\n",
    "            feature = features[i] + '-' + features[j]\n",
    "            data_combination = pd.concat([\n",
    "                data_combination,\n",
    "                (data['context_timestamp'] + temp[feature + '-next-browse-time-interval']).rename(feature + '-next-browse-time')\n",
    "            ],axis = 1)\n",
    "    \n",
    "    data_combination.to_pickle('data/myfeature/next_browse_time')\n",
    "else:\n",
    "    data_combination = pd.read_pickle('data/myfeature/next_browse_time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8动作前后浏览量（leak）\n",
    "### 4.8.1当日"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()\n",
    "\n",
    "if(not os.path.exists('data/myfeature/browse_count_today')):\n",
    "    res = []\n",
    "    features = ['item_id','item_brand_id','item_city_id','user_id','shop_id']\n",
    "    data_combination = pd.DataFrame()\n",
    "    \n",
    "    for feature in features:\n",
    "        temp = data.groupby([feature,'context_timestamp:day']).apply(lambda x:len(x)).to_dict()\n",
    "        data_combination = pd.concat([\n",
    "            data_combination,\n",
    "            data.apply(lambda x,dic:dic[(x[feature],x['context_timestamp:day'])],axis = 1,args = (temp,)).rename(feature + '-browse-count-today')\n",
    "        ],axis = 1)\n",
    "    \n",
    "    for i in range(len(features)):\n",
    "        for j in range(i + 1,len(features)):\n",
    "            feature = features[i] + '-' + features[j]\n",
    "            temp = data.groupby([features[i],features[j],'context_timestamp:day']).apply(lambda x:len(x)).to_dict()\n",
    "            data_combination = pd.concat([\n",
    "                data_combination,\n",
    "                data.apply(lambda x,dic:dic[(x[features[i]],x[features[j]],x['context_timestamp:day'])],axis = 1,args = (temp,)).rename(feature + '-browse-count-today')\n",
    "            ],axis = 1)\n",
    "    \n",
    "    # 保存\n",
    "    data_combination.to_pickle('data/myfeature/browse_count_today')\n",
    "else:\n",
    "    data_combination = pd.read_pickle('data/myfeature/browse_count_today')\n",
    "print(datetime.datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8.2当小时"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()\n",
    "if(not os.path.exists('data/myfeature/browse_count_tohour')):\n",
    "    res = []\n",
    "    data_combination = pd.DataFrame()\n",
    "    \n",
    "    for feature in features:\n",
    "        temp = data.groupby([feature,'context_timestamp:day','context_timestamp:hour']).apply(lambda x:len(x)).to_dict()\n",
    "        data_combination = pd.concat([\n",
    "            data_combination,\n",
    "            data.apply(lambda x,dic:dic[(x[feature],x['context_timestamp:day'],x['context_timestamp:hour'])],axis = 1,args = (temp,)).rename(feature + '-browse-count-tohour')\n",
    "        ],axis = 1)\n",
    "    \n",
    "    for i in range(len(features)):\n",
    "        for j in range(i + 1,len(features)):\n",
    "            feature = features[i] + '-' + features[j]\n",
    "            temp = data.groupby([features[i],features[j],'context_timestamp:day','context_timestamp:hour']).apply(lambda x:len(x)).to_dict()\n",
    "            data_combination = pd.concat([\n",
    "                data_combination,\n",
    "                data.apply(lambda x,dic:dic[(x[features[i]],x[features[j]],x['context_timestamp:day'],x['context_timestamp:hour'])],axis = 1,args = (temp,)).rename(feature + '-browse-count-tohour')\n",
    "            ],axis = 1)\n",
    "    \n",
    "    # 保存\n",
    "    data_combination.to_pickle('data/myfeature/browse_count_tohour')\n",
    "else:\n",
    "    data_combination = pd.read_pickle('data/myfeature/browse_count_tohour')\n",
    "print(datetime.datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.9冷启动特征（leak）\n",
    "### 4.9.1当天"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2min\n",
    "start = datetime.datetime.now()\n",
    "if(not os.path.exists('data/myfeature/day_browse_is_last')):\n",
    "    res = []\n",
    "    data_combination = pd.DataFrame()\n",
    "    \n",
    "    # 执行\n",
    "    # 是否第一次\n",
    "    dic = data.groupby(['user_id','context_timestamp:day']).apply(lambda x:x.iloc[0].name).to_dict() # index？\n",
    "    temp = data.apply(lambda x,dic:1 if x.name == dic[(x['user_id'],x['context_timestamp:day'])] else 0,axis = 1,args = (dic,))\n",
    "    data_combination = pd.concat([\n",
    "        data_combination,\n",
    "        temp.rename('user_id-day-browse-is-first')\n",
    "    ],axis = 1)\n",
    "       \n",
    "    # 是否最后一次\n",
    "    dic = data.groupby(['user_id','context_timestamp:day']).apply(lambda x:x.iloc[-1].name).to_dict() # index？\n",
    "    temp = data.apply(lambda x,dic:1 if x.name == dic[(x['user_id'],x['context_timestamp:day'])] else 0,axis = 1,args = (dic,))\n",
    "    data_combination = pd.concat([\n",
    "        data_combination,\n",
    "        temp.rename('user_id-day-browse-is-last')\n",
    "    ],axis = 1)\n",
    "\n",
    "    # 保存\n",
    "    data_combination.to_pickle('data/myfeature/day_browse_is_last')\n",
    "\n",
    "else:\n",
    "    data_combination = pd.read_pickle('data/myfeature/day_browse_is_last')\n",
    "print(datetime.datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9.2当小时"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2min\n",
    "start = datetime.datetime.now()\n",
    "if(not os.path.exists('data/myfeature/hour_browse_is_last')):\n",
    "    res = []\n",
    "    data_combination = pd.DataFrame()\n",
    "\n",
    "    # 执行\n",
    "    # 是否第一次\n",
    "    dic = data.groupby(['user_id','context_timestamp:day','context_timestamp:hour']).apply(lambda x:x.iloc[0].name).to_dict() # index？\n",
    "    temp = data.apply(lambda x,dic:1 if x.name == dic[(x['user_id'],x['context_timestamp:day'],x['context_timestamp:hour'])] else 0,axis = 1,args = (dic,))\n",
    "\n",
    "    data_combination = pd.concat([\n",
    "        data_combination,\n",
    "        temp.rename('user_id-hour-browse-is-first')\n",
    "    ],axis = 1)\n",
    "\n",
    "    # 是否最后一次\n",
    "    dic = data.groupby(['user_id','context_timestamp:day','context_timestamp:hour']).apply(lambda x:x.iloc[-1].name).to_dict() # index？\n",
    "    temp = data.apply(lambda x,dic:1 if x.name == dic[(x['user_id'],x['context_timestamp:day'],x['context_timestamp:hour'])] else 0,axis = 1,args = (dic,))\n",
    "\n",
    "    data_combination = pd.concat([\n",
    "        data_combination,\n",
    "        temp.rename('user_id-hour-browse-is-last')\n",
    "    ],axis = 1)\n",
    "    \n",
    "    # 保存\n",
    "    data_combination.to_pickle('data/myfeature/hour_browse_is_last')\n",
    "else:\n",
    "    data_combination = pd.read_pickle('data/myfeature/hour_browse_is_last')\n",
    "print(datetime.datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.10排序特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_sort(feature_now,temp):\n",
    "#     if(len(feature_now) == 2):\n",
    "#         return data.apply(lambda x:temp[x.loc[feature_now[0]], x.loc[feature_now[1]], x.name]['order'],axis = 1)\n",
    "#     else:\n",
    "#         return data.apply(lambda x:temp[x.loc[feature_now[0]], x.loc[feature_now[1]], x.loc[feature_now[2]], x.name]['order'],axis = 1)\n",
    "\n",
    "# def get_sort_test(feature_now,temp):\n",
    "#     if(len(feature_now) == 2):\n",
    "#         return data_test.apply(lambda x:temp[x.loc[feature_now[0]], x.loc[feature_now[1]], x.name  + length_data]['order'],axis = 1)\n",
    "#     else:\n",
    "#         return data_test.apply(lambda x:temp[x.loc[feature_now[0]], x.loc[feature_now[1]], x.loc[feature_now[2]], x.name + length_data]['order'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start = datetime.datetime.now()\n",
    "# if(not os.path.exists('data/myfeature/order') or not os.path.exists('data/myfeature/order_test')):\n",
    "#     res = {}\n",
    "#     res_test = {}\n",
    "#     features = ['f1','f4','f5','f10','f19']\n",
    "#     length_data = len(data)\n",
    "#     data_test['f5'].fillna(-1,inplace = True)\n",
    "    \n",
    "#     # 执行\n",
    "#     pros = Pool()\n",
    "#     for feature in features:\n",
    "#         feature_now = (feature,'f16:day') # 按天排序\n",
    "#         print(feature_now)\n",
    "#         temp = data.append(data_test).reset_index(drop = True).groupby(feature_now) \\\n",
    "#             .apply(lambda x:pd.concat([pd.Series(range(len(x)),index = x.index).rename('order'),x],axis = 1)).to_dict('index')\n",
    "        \n",
    "#         res[feature] = pros.apply_async(get_sort,args = (feature_now,temp))\n",
    "#         if(flag is False):\n",
    "#             res_test[feature] = pros.apply_async(get_sort_test,args = (feature_now,temp))\n",
    "            \n",
    "#     for i in range(len(features)):\n",
    "#         for j in range(i + 1,len(features)):\n",
    "#             feature_now = (features[i],features[j],'f16:day') # 按天排序\n",
    "#             print(feature_now)\n",
    "#             temp = data.append(data_test).reset_index(drop = True).groupby(feature_now) \\\n",
    "#                 .apply(lambda x:pd.concat([pd.Series(range(len(x)),index = x.index).rename('order'),x],axis = 1)).to_dict('index')\n",
    "                \n",
    "#             res[feature] = pros.apply_async(get_sort,args = (feature_now,temp))\n",
    "#             if(flag is False):\n",
    "#                 res_test[feature] = pros.apply_async(get_sort_test,args = (feature_now,temp))\n",
    "            \n",
    "#     pros.close()\n",
    "#     pros.join()\n",
    "    \n",
    "#     for feature in res:\n",
    "#         data_combination = pd.concat([data_combination,res[feature].get().rename(feature + '-sort-by-day')],axis = 1)\n",
    "#         if(flag is False):\n",
    "#             data_combination_test = pd.concat([data_combination_test,res_test[feature].get().rename(feature + '-sort-by-day')], axis = 1)\n",
    "            \n",
    "#     # 保存\n",
    "#     data_combination.to_pickle('data/myfeature/order')\n",
    "#     if(flag is False):\n",
    "#         data_combination_test.to_pickle('data/myfeature/order_test')\n",
    "# else:\n",
    "#     data_combination = pd.read_pickle('data/myfeature/order')\n",
    "#     data_combination_test = pd.read_pickle('data/myfeature/order_test')\n",
    "# print(datetime.datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.11贝叶斯平滑后的转化率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import numpy\n",
    "# import random\n",
    "# import scipy.special as special\n",
    "\n",
    "\n",
    "# class BayesianSmoothing(object):\n",
    "#     def __init__(self, alpha, beta):\n",
    "#         self.alpha = alpha\n",
    "#         self.beta = beta\n",
    "\n",
    "#     def sample(self, alpha, beta, num, imp_upperbound):\n",
    "#         sample = numpy.random.beta(alpha, beta, num)\n",
    "#         I = []\n",
    "#         C = []\n",
    "#         for clk_rt in sample:\n",
    "#             imp = random.random() * imp_upperbound\n",
    "#             imp = imp_upperbound\n",
    "#             clk = imp * clk_rt\n",
    "#             I.append(imp)\n",
    "#             C.append(clk)\n",
    "#         return I, C\n",
    "\n",
    "#     def update(self, imps, clks, iter_num, epsilon):\n",
    "#         for i in range(iter_num):\n",
    "#             new_alpha, new_beta = self.__fixed_point_iteration(imps, clks, self.alpha, self.beta)\n",
    "#             if abs(new_alpha-self.alpha) < epsilon and abs(new_beta - self.beta) < epsilon:\n",
    "#                 break\n",
    "#             self.alpha = new_alpha\n",
    "#             self.beta = new_beta\n",
    "\n",
    "#     def __fixed_point_iteration(self, imps, clks, alpha, beta):\n",
    "#         numerator_alpha = 0.0\n",
    "#         numerator_beta = 0.0\n",
    "#         denominator = 0.0\n",
    "\n",
    "#         for i in range(len(imps)):\n",
    "#             numerator_alpha += (special.digamma(clks[i]+alpha) - special.digamma(alpha))\n",
    "#             numerator_beta += (special.digamma(imps[i]-clks[i]+beta) - special.digamma(beta))\n",
    "#             denominator += (special.digamma(imps[i]+alpha+beta) - special.digamma(alpha+beta))\n",
    "\n",
    "#         return alpha*(numerator_alpha/denominator), beta*(numerator_beta/denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-\n",
      "f4-\n",
      "f5-\n",
      "f10-\n",
      "f19-\n",
      "f1-f4-\n",
      "f1-f5-\n",
      "f4-f5-\n"
     ]
    }
   ],
   "source": [
    "# def smooth(feature,data,data_type):\n",
    "#     print(feature)\n",
    "#     bs = BayesianSmoothing(1, 1)\n",
    "#     bs.update(data[feature + 'browse-count-5'].values, data[feature + 'count-5'].values, 1000, 0.001)\n",
    "#     print(feature + 'update成功')\n",
    "#     temp = (data[feature + 'count-5'] + bs.alpha) / (data[feature + 'browse-count-5'] + bs.alpha + bs.beta)\n",
    "#     temp.to_pickle('data/myfeature/' + feature + data_type)\n",
    "#     return True\n",
    "\n",
    "# features_to_smooth = [\n",
    "#     'f1-','f4-','f5-','f10-','f19-',\n",
    "#     'f1-f4-','f1-f5-','f4-f5-','f4-f19-','f5-f19-'\n",
    "# ]\n",
    "\n",
    "# for feature in features_to_smooth:\n",
    "#     pros.apply_async(smooth, (feature,data,''))  #增加新的进程\n",
    "    \n",
    "# for feature in features_to_smooth:\n",
    "#     pros.apply_async(smooth, (feature,data_test,'test'))  #增加新的进程\n",
    "    \n",
    "# pros.close() # 禁止在增加新的进程\n",
    "# pros.join()\n",
    "# print(\"pool process done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.特征合并\n",
    "## 5.1读取先前处理的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user_id-day-browse-is-first', 'user_id-day-browse-is-last', 'user_id-hour-browse-is-first', 'user_id-hour-browse-is-last']\n"
     ]
    }
   ],
   "source": [
    "# 读取3.特征变化、衍生产生的data\n",
    "data = pd.read_pickle('data/data')\n",
    "\n",
    "# 读取4.特征组合产生的data_combination\n",
    "data_combination = pd.DataFrame()\n",
    "for file in os.listdir('data/myfeature'):\n",
    "    if('.' not in file):\n",
    "        temp = pd.read_pickle('data/myfeature/' + file)\n",
    "        data_combination = pd.concat([data_combination,temp],axis = 1)\n",
    "        \n",
    "# 删除特征取值少的冗余特征\n",
    "column_to_del = []\n",
    "for column in data_combination.columns:\n",
    "    if(len(data_combination[column].value_counts()) <= 2):\n",
    "        column_to_del.append(column)\n",
    "\n",
    "    # column_to_del += ['f1-f19-browse-count-5','f5-f10-browse-count-5']\n",
    "print(column_to_del)\n",
    "data_combination.drop(column_to_del[:-4],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 拼接data_f3、data_test_f3、data_combination、data_combination_test\n",
    "data = pd.concat([data.loc[:,:'f3-f18-jaccard'],data_combination,data.loc[:,'label']],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.sort_values(['index'],inplace = True)\n",
    "data.set_index('index', inplace=True)# f16转换为索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.特征选择\n",
    "- 除非万不得已，不要用PCA或者LDA降维，直接减原始特征就行了。\n",
    "\n",
    "## 6.1质量不好的特征\n",
    "- 缺失的行特别多，弃用该列，超过15%缺失的特征应该予以删除！\n",
    "- 质量都不错，最多的f12（0.027）\n",
    "\n",
    "## 6.2冗余特征（相关性强的保留一个）\n",
    "- 有些 Feature 之间可能存在线性关系，影响 Model 的性能。\n",
    "- Feature越少，训练越快。\n",
    "\n",
    "## 6.3无关特征\n",
    "- f0样本编号：近似唯一\n",
    "- f1广告商品编号\n",
    "- f10用户编号\n",
    "- f15上下文信息编号：完全唯一\n",
    "- f19店铺编号\n",
    "\n",
    "## 6.4无法直接用的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data.drop('f16',axis = 1)\n",
    "data.drop([\n",
    "    'item_category_list',\n",
    "    'item_category_list:1',\n",
    "    'item_category_list:2',\n",
    "    'item_category_list:3',\n",
    "    'item_property_list',\n",
    "    'user_gender_id',\n",
    "    'user_occupation_id',\n",
    "    'predict_category_property'], axis=1,inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.标签处理\n",
    "- 上采样、下采样、分层采样。\n",
    "\n",
    "# 8.保存结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle('data/round1_train2_5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
